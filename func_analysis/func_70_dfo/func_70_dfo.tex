\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Türevsiz Optimizasyon (Derivatýve-free Optimization)

Karesel Yaklaþýksallama (Quadratic Approximation)

Bir nokta etrafýnda, herhangi bir boyutta karesel yaklaþýksallama yapmak
için bir karesel baz fonksiyonu kullanabiliriz, mesela iki boyut için

$$
p(x) =
\left[\begin{array}{ccccc} x_1 & x_2 & x_1^2 & x_1x_2 & x_2^2 \end{array}\right]^T
$$

bir baz olabilir, ki $x=\left[\begin{array}{cc} x_1 & x_2 \end{array}\right]^T$ 
olmak üzere, böylece $f(x) = p(x)^T c$ çarpýmý ile bir özgün fonksiyon 
yaratabiliriz, $a = [a_0, a_1, ...]$ içinde sabitler vardýr bu sabitler 
fonksiyonu özgün olarak belirleyen deðerlerdir. Bir anlamda

$$
f(x) = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2 + a_4 x_2^2
$$

çarpýmýnýn vektörsel halini görmüþ olduk. 

Peki eðer $a$ katsayýlarýný bilmiyorsak, verilen bir deney verisi üzerinden
katsayýlarý nasýl buluruz? Üstteki temeli kullanarak bir veriye en az
kareler baðlamýnda en iyi uyan karesel denklemi uydurabiliriz, bunun için
her veri noktasýný baz fonksiyon üzerinden geniþletmemiz gerekir, yani üç
boyutlu bir fonksiyondan alýnmýþ olacak
$x^1 = (x_1^1,x_2^1), x^2 = (x_1^2,x_2^2), ...,x^n = (x_1^n,x_2^n)$ ve ona
tekabül eden $y^1,y^2,...,y^n$ deðerleri için

$$
\left[\begin{array}{ccccc}
 (x_1^1) & (x_2^1) & (x_1^1)^2 & (x_1^1)(x_2^1) & (x_2^1)^2  \\
\vdots & & & & \vdots \\
 (x_1^n) & (x_2^n) & (x_1^n)^2 & (x_1^n)(x_2^n) & (x_2^n)^2  \\
\end{array}\right] 
\mathbf{x} = 
\left[\begin{array}{c}
y^1 \\ \vdots \\ y^n
\end{array}\right]
$$

Bu problemi en az kareler stili ile çözebiliriz. Fakat bizim ilgilendiðimiz
uydurulan bir karesel fonksiyon üzerinden ayrýca gradyan ve Hessian
bilgisini de alabilmek. Bu formda 

$$
f(x) = x^T A x
$$

bir karesel fonksiyondur, ve gradyan $\nabla f (x) = 2 A x$ ve Hessian
$\nabla^2 f(x) = 2 A$ ($A$ simetrik ise). Ýstediðimiz öyle bir uyum ki elde
edilen katsayýlarý direk $A$ öðeleri olarak alabilelim, ve bu $A$ üzerinden
$\nabla f(x)$ ve $\nabla^2 f(x)$ hesaplamak kolay olsun. 








\begin{minted}[fontsize=\footnotesize]{python}
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
def Rosenbrock(x,y):
    return (1 - x)**2 + 100*(y - x**2)**2

x = np.linspace(-2,2,250)
y = np.linspace(-1,3,250)
X, Y = np.meshgrid(x, y)
Z = Rosenbrock(X, Y)

fig = plt.figure(figsize = (8,4))

ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_surface(X,Y,Z,rstride = 5, cstride = 5, cmap = 'jet', alpha = .4, edgecolor = 'none' )
ax.view_init(0, 100)
ax.set_xlabel('x')
ax.set_ylabel('y')

ax = fig.add_subplot(1, 2, 2, projection='3d')
ax.plot_surface(X,Y,Z,rstride = 5, cstride = 5, cmap = 'jet', alpha = .4, edgecolor = 'none' )
ax.view_init(40, 250)
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.savefig('func_70_dfo_01.png')
\end{minted}

\includegraphics[width=35 em]{func_70_dfo_01.png}

\begin{minted}[fontsize=\footnotesize]{python}
def Peaks(x1,x2):
    return \
    3*(1-x1)**2 * np.exp(-x1**2 - (x2 + 1)**2) - \
    10*(1/5. * x1 - x1**3 - x2**5) * np.exp(-x1**2 - x2**2) - \
    1/3. * np.exp(-(x1+1)**2 - x1**2)

def Himmer(x,y):
    return (x**2 + y - 11)**2 + ( x + y**2 - 7 )**2

x = np.linspace(-3,3,100)
y = np.linspace(-3,3,100)
X, Y = np.meshgrid(x, y)
#Z = Himmer(X, Y)
Z = Peaks(X, Y)

fig = plt.figure()
ax = fig.gca(projection='3d')
ax.plot_surface(X,Y,Z, cmap=cm.jet, rstride=1, cstride=1)
ax.view_init(36, -124)
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.savefig('func_70_dfo_02.png')
\end{minted}

\includegraphics[width=35 em]{func_70_dfo_02.png}


[devam edecek]

\end{document}
