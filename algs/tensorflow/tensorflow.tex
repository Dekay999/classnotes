\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Tensorflow (TF)

Google'ýn yazdýðý ve açýk yazýlým haline getirdiði paket TF çoðunlukla
yapay öðrenim baðlamýnda gündeme geliyor, fakat TF aslýnda genel kullanýmý
olan bir paket. TF bir sayýsal hesap kütüphanesi, daha spesifik olarak, ona
çizit olarak verilen hesaplarý yapabilen bir sayýsal hesap paketi.

TF ile hesap yapmak için hesabý temsil eden bir çizit kurulur, mesela
$f(x,y) = x^2y + y + 2$ için

\includegraphics[width=20em]{tf_01.png}

Bu hesap aðaçýnda görülen sayýlar tek sayý olabilir, çok boyutlu vektör,
matris, ya da çok boyutlu matris olabilir. Matematikte bu objelere genel
olarak ``tensor'' ismi veriliyor, paketin ismi de buradan geliyor,
tensorlar hesap çiziti içinde bir hesaptan diðerine ``akýyorlar''
(flow). Hesabý çizit olarak belirtmenin bazý avantajlarý var, en önemlisi
çizit üzerinde direk otomatik türev alýnabilir, bkz. {\em Otomatik Türev
  Almak} yazýsý, ve bu þekilde gradyan hesaplarý kolay bir þekilde
yapýlabiliyor. Bir diðeri çizitin paralelleþtirme için doðal bir yapý
olmasý; çiziti istediðimiz þekilde bölerek parçalarý farklý mikroiþlemci
(CPU), ya da grafik iþlemci (GPU) üzerinde paralel bir þekilde
iþletebiliriz, mesela üstteki $f$ için $f(3,4)$ hesabý,

\includegraphics[width=20em]{tf_02.png}

þeklinde iki parçaya bölünebilir. 3 girilen soldaki parça kendi baþýna
hesabýný yaparken ayný anda 4 girilen diðer parça iþlemine devam
edebilir. Ayrýca Google TPU adý verilen tensor iþlemci üniteleri üzerinden
artýk CPU, GPU yerine direk TF için optimize edilmiþ yeni iþlemciler
üzerinden paralelizasyon yapýlabiliyor.

TF kodlamasý nasýl olur? Üstteki örnek için

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf

x = tf.Variable(3, name="x")
y = tf.Variable(4, name="y")
f = x*x*y + y + 2
\end{minted}

Ýçinde $x,y,f$ düðümleri (node) olan bir çizit yaratýldý. Bu kadar! Fakat
anlamamýz gereken önemli bir nokta var, üstteki kodu iþletince halen bir
hesap yapmýþ olmuyoruz, sadece üzerinden hesabýn yapýlabileceði çizit
yapýsýný yaratmýþ oluyoruz. Hesabýn kendisi için bir TF oturumu (session)
açmak lazým, bu oturum üzerinden deðiþkenler baþlangýç deðerlerlerine
eþitlenir, ve sonra $f$ hesabý tetiklenir. Esas hesap bu þekilde ortaya
çýkar.

\begin{minted}[fontsize=\footnotesize]{python}
sess = tf.Session()
sess.run(x.initializer)
sess.run(y.initializer)
result = sess.run(f)
print(result)
\end{minted}

\begin{verbatim}
42
\end{verbatim}

Eðer iþimiz bitti ise ve kaynaklarýn (bellek, iþlemci gibi) geri
dönüþümünü, serbest býrakýlmasýný istiyorsak oturumu kapatýrýz,

\begin{minted}[fontsize=\footnotesize]{python}
sess.close()
\end{minted}

Kodlama açýsýndan biraz daha temiz bir yol, 

\begin{minted}[fontsize=\footnotesize]{python}
with tf.Session() as sess:
    x.initializer.run()
    y.initializer.run()
    result = f.eval()
print result
\end{minted}

\begin{verbatim}
42
\end{verbatim}

\verb!with! kullanýmý ile blok dýþýna çýkýlýnca kapatma iþlemi otomatik
olarak yapýlýyor. Nihai hesap için \verb!eval! çaðrýsý yapýldýðýný gördük,
bu çaðrý aslýnda herhangi bir düðümün hesaplanmasýný tetikleyebilir. Bu
tetikleme sonrasýnda TF bir düðümün hangi diðer düðümlere baðlý olduðuna
bakarak çizitte önce o düðümlerin hesabýný yapacaktýr, ve o çýktýlarý
çizite göre birleþtirerek nihai sonucu bulacaktýr. Mesela

\begin{minted}[fontsize=\footnotesize]{python}
w = tf.constant(3)
x = w + 2
y = x + 5
z = x * 3

with tf.Session() as sess:
    print 'y =', y.eval()
    print 'z =', z.eval()
\end{minted}

\begin{verbatim}
y = 10
z = 15
\end{verbatim} 

TF otomatik olarak $y$'nin $w$'ye, onun da $x$'e baðlý olduðunu gördü, önce
$w$'yi iþletti, sonra $y$'yi, ve onu da $z$ hesabý için kullandý. Dikkat,
TF önbellekleme yapmaz, yani üstteki kod $w,x$ hesabýný iki kere
yapar. Hesap çaðrýsý sonrasý deðiþken deðerleri muhafaza edilir (çünkü
onlar çizitin parçasý) fakat düðüm deðerleri yokolur.

TF bir anlamda numpy kütüphanesinin çizitli, çok iþlemcili versiyonu olarak
görülebilir. Bir numpy matrisi üzerinde yapýlan pek çok iþlem TF ile de
yapýlabilir. Mesela bir matrisin tümü, herhangi bir ekseni bazýndaki toplam
alttaki gibi alýnabiliyor,

\begin{minted}[fontsize=\footnotesize]{python}
x = tf.constant([[1., 1., 1.], [1., 1.,1.]])
c1 = tf.reduce_sum(x)
print tf.Session().run(c1)
c1 = tf.reduce_sum(x, 0) # y ekseni uzerinden toplam
print tf.Session().run(c1)
c2 = tf.reduce_sum(x, 1) # x ekseni uzerinden toplam
print tf.Session().run(c2)
\end{minted}

\begin{verbatim}
6.0
[ 2.  2.  2.]
[ 3.  3.]
\end{verbatim}

Lineer Regresyon

Þimdi bir toptan basit lineer regresyon hesabýný TF ile yapalým. Regresyonu
California emlak veri seti üzerinde iþleteceðiz, bu veride bölge bazlý
olarak ev sahiplerinin ortalama yaþý, geliri, gibi deðiþkenler ile hedef
deðiþkeni olan ev fiyatý kayýtlý. Hedef ve kaynak deðiþkenler arasýndaki
lineer iliþki lineer regresyon ile hesaplanabilir, tanýdýk formül,

$\hat{\theta} = (X^TX )^{-1} X^T y $

Veriye bir yanlýlýk (sadece 1 deðeri içeren yeni bir kolon) ekleyip üstteki
hesabý yapalým.

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

housing = fetch_california_housing(data_home="/home/burak/Downloads/scikit-data")
print housing['data'].shape
print housing['target'][:5]

m, n = housing.data.shape

housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]
X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")
XT = tf.transpose(X)
theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)

with tf.Session() as sess:
    theta_value = theta.eval()
print 'theta'
print theta_value
\end{minted}

\begin{verbatim}
(20640, 8)
[ 4.526  3.585  3.521  3.413  3.422]
theta
[[ -3.68059006e+01]
 [  4.36796039e-01]
 [  9.45724174e-03]
 [ -1.07348330e-01]
 [  6.44418657e-01]
 [ -3.95741154e-06]
 [ -3.78908939e-03]
 [ -4.20193195e-01]
 [ -4.33070064e-01]]
\end{verbatim}

TF'in matris çarpýmý için \verb!matmul!, tersini alma için
\verb!matrix_inverse! çaðrýlarýný görüyoruz. Üstteki kodu olduðu gibi alýp
pek çok iþlemci üzerinde direk paralel þekilde iþletebiliriz, ayný iþi pür
numpy bazlý kodla yapmak daha külfetli olurdu.

Gradyan Ýniþi

Klasik toptan usül ile hesabý gördük. Peki gradyan iniþi ile lineer
regresyon nasýl yaparýz? Burada iki yaklaþýmý göstereceðiz, biri daha zor,
diðeri daha kolay. Zor olan matematiksel olarak elle kendimizin gradyan
türevini almasý, daha kolay olaný türevi TF içindeki otomatik türev alma
mekanizmasýný kullanarak o iþi de TF'e yaptýrmak.

Veriyi hazýrlayalým ve çiziti oluþturalým; baþlangýç $\theta$ deðeri
rasgele atansýn, $X,y$ deðerleri verinin kendisi olacak, tahmin ve hata
için fonksiyonlar olsun.

\begin{minted}[fontsize=\footnotesize]{python}
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]

reset_graph()

X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")
error = y_pred - y
mse = tf.reduce_mean(tf.square(error), name="mse")
\end{minted}

Elle türevi alýnmýþ gradyan nedir? [1, sf. 261]'e göre formül

$$ 
\nabla_\theta MSE(\theta) = \frac{2}{m} X^T (X \cdot \theta - y)
$$  

ve gradyan güncellemesi

$$ 
\theta^{t+1} = \theta - \eta \nabla_\theta MSE(\theta)
$$

Alttaki TF kodu bir döngü içinde gradyan güncellemesi yapacak ve hata
karelerinin ortalamasý (mean square error -MSE-) hesaplayýp ekrana
basacak. MSE'in gittikçe aþaðý inmesi lazým, çünkü gradyanýn tersi yönünde
hatayý azaltacak þekilde hareket ediyoruz. 

\begin{minted}[fontsize=\footnotesize]{python}
n_epochs = 1000
learning_rate = 0.01

gradients = 2/np.float(m) * tf.matmul(tf.transpose(X), error)
training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)    
    best_theta = theta.eval()
    
print 'theta'
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450073)
('Epoch', 200, 'MSE =', 0.56670469)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881161)
('Epoch', 500, 'MSE =', 0.54363626)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214705)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393812]
 [-0.04269557]
 [-0.66145277]
 [-0.63752776]]
\end{verbatim}

Otomatik Türev ile Gradyan

Sembolik türev yerine TF içindeki \verb!autodiff! paketine türevi aldýralým
ve gradyan iniþini böyle yapalým,

\begin{minted}[fontsize=\footnotesize]{python}
gradients = tf.gradients(mse, [theta])[0] # otomatik turev

training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)
    
    best_theta = theta.eval()
    
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450061)
('Epoch', 200, 'MSE =', 0.56670463)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881167)
('Epoch', 500, 'MSE =', 0.5436362)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214717)
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]
\end{verbatim}

Ayný sonuca eriþtik. 

Daha da basitleþtirebiliriz, üstteki kodda \verb!assign! ile gradyan iniþi
için gereken çýkartma iþlemi elle yapýldý. TF paketi içinde bu çýkartmayý
yapacak optimizasyon rutinleri de var, mesela
\verb!GradientDescentOptimizer!.

\begin{minted}[fontsize=\footnotesize]{python}
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(mse)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
        sess.run(training_op)    
    best_theta = theta.eval()

print('theta')
print(best_theta)
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450061)
('Epoch', 200, 'MSE =', 0.56670463)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881167)
('Epoch', 500, 'MSE =', 0.5436362)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214717)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]
\end{verbatim}

Görüldüðü gibi matris iþlemi içeren her türlü hesap TF ile kodlanabilir, bu
yapýldýðýnda kodlar rahat bir þekilde paralelize edilebilir. Yapay
öðrenimde ne kadar çok lineer cebir kullanýmý olduðunu biliyoruz, ayrýca
türev almak otomatikleþtirildiði için akla gelebilecek her türlü lineer
cebir, optimizasyon iþlemi TF üzerinden kodlanabilir.






























TensorFlow ile Cok Katmanli Yapay Sinir Aglari 

\begin{minted}[fontsize=\footnotesize]{python}
from tensorflow.python.framework import ops
import pandas as pd
import tensorflow as tf

cols_of_interest = ['AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI', 'FTV']
df = pd.read_csv('lowbwt.dat',sep='\s*',engine='python')
x_vals = np.array(df[cols_of_interest])
y_vals = np.array(df['BWT'])

ops.reset_default_graph()
tf.set_random_seed(3)
np.random.seed(3)

batch_size = 10

sess = tf.Session()

tmp = np.random.choice(range(len(x_vals)), size=len(x_vals), replace=False)
first = int(len(x_vals)*0.80)
train_indices = tmp[:first]
test_indices = tmp[first:]

x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]

def normalize_cols(m):
    col_max = m.max(axis=0)
    col_min = m.min(axis=0)
    return (m-col_min) / (col_max - col_min)
    
x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))
x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
def init_weight(shape, st_dev):
    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))
    return(weight)
    
def init_bias(shape, st_dev):
    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))
    return(bias)
    
x_data = tf.placeholder(shape=[None, 8], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)

def fully_connected(input_layer, weights, biases):
    layer = tf.add(tf.matmul(input_layer, weights), biases)
    return(tf.nn.relu(layer))

weight_1 = init_weight(shape=[8, 25], st_dev=10.0)
bias_1 = init_bias(shape=[25], st_dev=10.0)
layer_1 = fully_connected(x_data, weight_1, bias_1)

weight_2 = init_weight(shape=[25, 10], st_dev=10.0)
bias_2 = init_bias(shape=[10], st_dev=10.0)
layer_2 = fully_connected(layer_1, weight_2, bias_2)

weight_3 = init_weight(shape=[10, 3], st_dev=10.0)
bias_3 = init_bias(shape=[3], st_dev=10.0)
layer_3 = fully_connected(layer_2, weight_3, bias_3)

weight_4 = init_weight(shape=[3, 1], st_dev=10.0)
bias_4 = init_bias(shape=[1], st_dev=10.0)
final_output = fully_connected(layer_3, weight_4, bias_4)

loss = tf.reduce_mean(tf.abs(y_target - final_output))

my_opt = tf.train.AdamOptimizer(0.05)
train_step = my_opt.minimize(loss)

init = tf.initialize_all_variables()
sess.run(init)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
loss_vec = []; test_loss = []

for i in range(200):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    rand_x = x_vals_train[rand_index]
    rand_y = np.transpose([y_vals_train[rand_index]])
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})

    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})
    loss_vec.append(temp_loss)
    
    test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})
    test_loss.append(test_temp_loss)
    if (i+1)%25==0:
        print('Epoch: ' + str(i+1) + '. Kayip = ' + str(temp_loss))


# Plot loss over time
plt.plot(loss_vec, 'k-', label=u'Egitim Kaybý')
plt.plot(test_loss, 'r--', label=u'Test Kaybý')
plt.title(u'Her Epoch Ýçinde Kayýp')
plt.xlabel('Epoch')
plt.ylabel(u'Kayýp')
plt.legend(loc="upper right")
plt.savefig('tf_03.png')
\end{minted}

\begin{verbatim}
Epoch: 25. Kayip = 6739.92
Epoch: 50. Kayip = 2719.33
Epoch: 75. Kayip = 1346.03
Epoch: 100. Kayip = 1823.08
Epoch: 125. Kayip = 951.273
Epoch: 150. Kayip = 1440.24
Epoch: 175. Kayip = 2061.46
Epoch: 200. Kayip = 1526.82
\end{verbatim}

\includegraphics[width=30em]{tf_03.png}
























\begin{minted}[fontsize=\footnotesize]{python}
from tensorflow.python.framework import ops
import pandas as pd
import tensorflow as tf

cols_of_interest = ['AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI', 'FTV']
df = pd.read_csv('lowbwt.dat',sep='\s*',engine='python')
x_vals = np.array(df[cols_of_interest])
y_vals = np.array(df['LOW'])

seed = 1
ops.reset_default_graph()
tf.set_random_seed(seed)
np.random.seed(seed)
sess = tf.Session()

tmp = np.random.choice(range(len(x_vals)), size=len(x_vals), replace=False)
first = int(len(x_vals)*0.80)
train_indices = tmp[:first]
test_indices = tmp[first:]

x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]

def normalize_cols(m):
    col_max = m.max(axis=0)
    col_min = m.min(axis=0)
    return (m-col_min) / (col_max - col_min)
    
x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))
x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))

batch_size = 130

x_data = tf.placeholder(shape=[None, 8], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)

def init_variable(shape):
    return(tf.Variable(tf.random_normal(shape=shape)))

def logistic(input_layer, multiplication_weight, bias_weight, activation = True):
    linear_layer = tf.add(tf.matmul(input_layer, multiplication_weight), bias_weight)
    if activation:
        return(tf.nn.sigmoid(linear_layer))
    else:
        return(linear_layer)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
A1 = init_variable(shape=[8,20])
b1 = init_variable(shape=[20])
logistic_layer1 = logistic(x_data, A1, b1)

A2 = init_variable(shape=[20,10])
b2 = init_variable(shape=[10])
logistic_layer2 = logistic(logistic_layer1, A2, b2)

A3 = init_variable(shape=[10,1])
b3 = init_variable(shape=[1])
final_output = logistic(logistic_layer2, A3, b3, activation=False)

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=final_output, labels=y_target))
     
my_opt = tf.train.AdamOptimizer(learning_rate = 0.002)
train_step = my_opt.minimize(loss)

init = tf.global_variables_initializer()
sess.run(init)

prediction = tf.round(tf.nn.sigmoid(final_output))
predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)
accuracy = tf.reduce_mean(predictions_correct)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
loss_vec = []; train_acc = []; test_acc = []

for i in range(1500):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    rand_x = x_vals_train[rand_index]
    rand_y = np.transpose([y_vals_train[rand_index]])
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})
    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})
    loss_vec.append(temp_loss)
    temp_acc_train = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])})
    train_acc.append(temp_acc_train)
    temp_acc_test = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})
    test_acc.append(temp_acc_test)
    if (i+1)%150==0:
        print('Loss = ' + str(temp_loss))

plt.plot(loss_vec, 'k-')
plt.title(u"Her Epoch Ýçin Çapraz Entropi Kaybý")
plt.xlabel(u"Epoch")
plt.ylabel(u"Çapraz Entropi Kaybý")
plt.ylim(0.0,2.0)
plt.savefig('tf_04.png')        

plt.figure()
plt.plot(train_acc, 'k-', label=u"Eðitim Seti Doðrulugu")
plt.plot(test_acc, 'r--', label=u"Test Set Doðrulugu")
plt.title(u"Eðitim ve Test Doðruluðu")
plt.xlabel("Epoch")
plt.ylabel(u"Doðruluk")
plt.ylim(0.0,0.8)
plt.legend(loc='lower right')
plt.savefig('tf_05.png')        
\end{minted}

\begin{verbatim}
Loss = 0.627154
Loss = 0.545542
Loss = 0.608412
Loss = 0.618306
Loss = 0.578082
Loss = 0.498052
Loss = 0.583924
Loss = 0.509966
Loss = 0.545845
Loss = 0.582545
\end{verbatim}

\includegraphics[width=30em]{tf_04.png}

\includegraphics[width=30em]{tf_05.png}


[devam edecek]

Kaynaklar 

[1] Géron, {\em Hands-On Machine Learning with Scikit-Learn and TensorFlow}

\end{document}
