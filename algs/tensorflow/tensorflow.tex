\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Tensorflow (TF)

Google'ýn yazdýðý ve açýk yazýlým haline getirdiði paket TF çoðunlukla
yapay öðrenim baðlamýnda gündeme geliyor, fakat TF aslýnda genel kullanýmý
olan bir paket. TF bir sayýsal hesap kütüphanesi, daha spesifik olarak, ona
çizit olarak verilen hesaplarý yapabilen bir sayýsal hesap paketi.

TF ile hesap yapmak için hesabý temsil eden bir çizit kurulur, mesela
$f(x,y) = x^2y + y + 2$ için

\includegraphics[width=20em]{tf_01.png}

Bu hesap aðaçýnda görülen sayýlar tek sayý olabilir, çok boyutlu vektör,
matris, ya da çok boyutlu matris olabilir. Matematikte bu objelere genel
olarak ``tensor'' ismi veriliyor, paketin ismi de buradan geliyor,
tensorlar hesap çiziti içinde bir hesaptan diðerine ``akýyorlar''
(flow). Hesabý çizit olarak belirtmenin bazý avantajlarý var, en önemlisi
çizit üzerinde direk otomatik türev alýnabilir, bkz. {\em Otomatik Türev
  Almak} yazýsý, ve bu þekilde gradyan hesaplarý kolay bir þekilde
yapýlabiliyor. Bir diðeri çizitin paralelleþtirme için doðal bir yapý
olmasý; çiziti istediðimiz þekilde bölerek parçalarý farklý mikroiþlemci
(CPU), ya da grafik iþlemci (GPU) üzerinde paralel bir þekilde
iþletebiliriz, mesela üstteki $f$ için $f(3,4)$ hesabý,

\includegraphics[width=20em]{tf_02.png}

þeklinde iki parçaya bölünebilir. 3 girilen soldaki parça kendi baþýna
hesabýný yaparken ayný anda 4 girilen diðer parça iþlemine devam
edebilir. Ayrýca Google TPU adý verilen tensor iþlemci üniteleri üzerinden
artýk CPU, GPU yerine direk TF için optimize edilmiþ yeni iþlemciler
üzerinden paralelizasyon yapýlabiliyor.

TF kodlamasý nasýl olur? Üstteki örnek için

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf

x = tf.Variable(3, name="x")
y = tf.Variable(4, name="y")
f = x*x*y + y + 2
\end{minted}

Ýçinde $x,y,f$ düðümleri (node) olan bir çizit yaratýldý. Bu kadar! Fakat
anlamamýz gereken önemli bir nokta var, üstteki kodu iþletince halen bir
hesap yapmýþ olmuyoruz, sadece üzerinden hesabýn yapýlabileceði çizit
yapýsýný yaratmýþ oluyoruz. Hesabýn kendisi için bir TF oturumu (session)
açmak lazým, bu oturum üzerinden deðiþkenler baþlangýç deðerlerlerine
eþitlenir, ve sonra $f$ hesabý tetiklenir. Esas hesap bu þekilde ortaya
çýkar.

\begin{minted}[fontsize=\footnotesize]{python}
sess = tf.Session()
sess.run(x.initializer)
sess.run(y.initializer)
result = sess.run(f)
print(result)
\end{minted}

\begin{verbatim}
42
\end{verbatim}

Eðer iþimiz bitti ise ve kaynaklarýn (bellek, iþlemci gibi) geri
dönüþümünü, serbest býrakýlmasýný istiyorsak oturumu kapatýrýz,

\begin{minted}[fontsize=\footnotesize]{python}
sess.close()
\end{minted}

Kodlama açýsýndan biraz daha temiz bir yol, 

\begin{minted}[fontsize=\footnotesize]{python}
with tf.Session() as sess:
    x.initializer.run()
    y.initializer.run()
    result = f.eval()
print result
\end{minted}

\begin{verbatim}
42
\end{verbatim}

\verb!with! kullanýmý ile blok dýþýna çýkýlýnca kapatma iþlemi otomatik
olarak yapýlýyor. Nihai hesap için \verb!eval! çaðrýsý yapýldýðýný gördük,
bu çaðrý aslýnda herhangi bir düðümün hesaplanmasýný tetikleyebilir. Bu
tetikleme sonrasýnda TF bir düðümün hangi diðer düðümlere baðlý olduðuna
bakarak çizitte önce o düðümlerin hesabýný yapacaktýr, ve o çýktýlarý
çizite göre birleþtirerek nihai sonucu bulacaktýr. Mesela

\begin{minted}[fontsize=\footnotesize]{python}
w = tf.constant(3)
x = w + 2
y = x + 5
z = x * 3

with tf.Session() as sess:
    print 'y =', y.eval()
    print 'z =', z.eval()
\end{minted}

\begin{verbatim}
y = 10
z = 15
\end{verbatim} 

TF otomatik olarak $y$'nin $w$'ye, onun da $x$'e baðlý olduðunu gördü, önce
$w$'yi iþletti, sonra $y$'yi, ve onu da $z$ hesabý için kullandý. Dikkat,
TF önbellekleme yapmaz, yani üstteki kod $w,x$ hesabýný iki kere
yapar. Hesap çaðrýsý sonrasý deðiþken deðerleri muhafaza edilir (çünkü
onlar çizitin parçasý) fakat düðüm deðerleri yokolur.

TF bir anlamda numpy kütüphanesinin çizitli, çok iþlemcili versiyonu olarak
görülebilir. Bir numpy matrisi üzerinde yapýlan pek çok iþlem TF ile de
yapýlabilir. Mesela bir matrisin tümü, herhangi bir ekseni bazýndaki toplam
alttaki gibi alýnabiliyor,

\begin{minted}[fontsize=\footnotesize]{python}
x = tf.constant([[1., 1., 1.], [1., 1.,1.]])
c1 = tf.reduce_sum(x)
print tf.Session().run(c1)
c1 = tf.reduce_sum(x, 0) # y ekseni uzerinden toplam
print tf.Session().run(c1)
c2 = tf.reduce_sum(x, 1) # x ekseni uzerinden toplam
print tf.Session().run(c2)
\end{minted}

\begin{verbatim}
6.0
[ 2.  2.  2.]
[ 3.  3.]
\end{verbatim}

Lineer Regresyon

Þimdi bir toptan basit lineer regresyon hesabýný TF ile yapalým. Regresyonu
California emlak veri seti üzerinde iþleteceðiz, bu veride bölge bazlý
olarak ev sahiplerinin ortalama yaþý, geliri, gibi deðiþkenler ile hedef
deðiþkeni olan ev fiyatý kayýtlý. Hedef ve kaynak deðiþkenler arasýndaki
lineer iliþkiyi lineer regresyon ile hesaplanabilir, tanýdýk formül,

$\hat{\theta} = (X^TX )^{-1} X^T y $

Veriye bir yanlýlýk (sadece 1 deðeri içeren yeni bir kolon) ekleyip üstteki
hesabý yapalým.

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

housing = fetch_california_housing(data_home="/home/burak/Downloads/scikit-data")
print housing['data'].shape
print housing['target'][:5]

m, n = housing.data.shape

housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]
X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")
XT = tf.transpose(X)
theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)

with tf.Session() as sess:
    theta_value = theta.eval()
print 'theta'
print theta_value
\end{minted}

\begin{verbatim}
(20640, 8)
[ 4.526  3.585  3.521  3.413  3.422]
theta
[[ -3.68059006e+01]
 [  4.36796039e-01]
 [  9.45724174e-03]
 [ -1.07348330e-01]
 [  6.44418657e-01]
 [ -3.95741154e-06]
 [ -3.78908939e-03]
 [ -4.20193195e-01]
 [ -4.33070064e-01]]
\end{verbatim}

TF'in matris çarpýmý için \verb!matmul!, tersini alma için
\verb!matrix_inverse! çaðrýlarýný görüyoruz. Üstteki kodu olduðu gibi alýp
pek çok iþlemci üzerinde direk paralel þekilde iþletebiliriz, ayný iþi pür
numpy bazlý kodla yapmak daha külfetli olurdu.

Gradyan Ýniþi

Klasik toptan usül ile hesabý gördük. Peki gradyan iniþi ile lineer
regresyon nasýl yaparýz? Burada iki yaklaþýmý göstereceðiz, biri daha zor,
diðeri daha kolay. Zor olan matematiksel olarak elle kendimizin gradyan
türevini almasý, daha kolay olaný türevi TF içindeki otomatik türev alma
mekanizmasýný kullanarak o iþi de TF'e yaptýrmak.

Çiziti oluþturalým; baþlangýç $\theta$ deðeri rasgele atansýn, $X,y$
deðerleri verinin kendisi olacak, tahmin ve hata için fonksiyonlar olsun.

\begin{minted}[fontsize=\footnotesize]{python}
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]

reset_graph()

X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")
error = y_pred - y
mse = tf.reduce_mean(tf.square(error), name="mse")
\end{minted}

Elle türevi alýnmýþ gradyan nedir? [1, sf. 261]'e göre formül

$$ 
\nabla_\theta MSE(\theta) = \frac{2}{m} X^T (X \cdot \theta - y)
$$  

ve gradyan güncellemesi

$$ 
\theta^{t+1} = \theta - \eta \nabla_\theta MSE(\theta)
$$

Alttaki TF kodu bir döngü içinde gradyan güncellemesi yapacak ve hata
karelerinin ortalamasý (mean square error -MSE-) hesaplayýp ekrana
basacak. MSE'in gittikçe aþaðý inmesi lazým, çünkü gradyanýn tersi yönünde
hatayý azaltacak þekilde hareket ediyoruz. 

\begin{minted}[fontsize=\footnotesize]{python}
n_epochs = 1000
learning_rate = 0.01

gradients = 2/np.float(m) * tf.matmul(tf.transpose(X), error)
training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)    
    best_theta = theta.eval()
    
print 'theta'
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450073)
('Epoch', 200, 'MSE =', 0.56670469)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881161)
('Epoch', 500, 'MSE =', 0.54363626)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214705)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393812]
 [-0.04269557]
 [-0.66145277]
 [-0.63752776]]
\end{verbatim}

Otomatik Türev ile Gradyan

Sembolik türev yerine TF içindeki \verb!autodiff! paketine türevi aldýralým
ve gradyan iniþini böyle yapalým,

\begin{minted}[fontsize=\footnotesize]{python}
gradients = tf.gradients(mse, [theta])[0] # otomatik turev

training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)
    
    best_theta = theta.eval()
    
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450061)
('Epoch', 200, 'MSE =', 0.56670463)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881167)
('Epoch', 500, 'MSE =', 0.5436362)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214717)
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]
\end{verbatim}

Ayný sonuca eriþtik. 

Daha da basitleþtirebiliriz, üstteki kodda \verb!assign! ile gradyan iniþi
için gereken çýkartma iþlemi elle yapýldý. TF paketi içinde bu çýkartmayý
yapacak optimizasyon rutinleri de var, mesela
\verb!GradientDescentOptimizer!.

\begin{minted}[fontsize=\footnotesize]{python}
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(mse)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
        sess.run(training_op)    
    best_theta = theta.eval()

print('theta')
print(best_theta)
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450061)
('Epoch', 200, 'MSE =', 0.56670463)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881167)
('Epoch', 500, 'MSE =', 0.5436362)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214717)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]
\end{verbatim}

Görüldüðü gibi matris iþlemi içeren her türlü hesap TF ile kodlanabilir, bu
yapýldýðýnda kodlar rahat bir þekilde paralelize edilebilir. Yapay
öðrenimde ne kadar çok lýneer cebir kullanýmý olduðunu biliyoruz, ayrýca
türev almak otomatikleþtirildiði için akla gelebilecek her türlü lineer
cebir, optimizasyon iþlemi TF üzerinden kodlanabilir.


Kaynaklar 

[1] Géron, {\em Hands-On Machine Learning with Scikit-Learn and TensorFlow}

\end{document}
