\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Tensorflow (TF)

Google'ýn yazdýðý ve açýk yazýlým haline getirdiði paket TF çoðunlukla
yapay öðrenim baðlamýnda gündeme geliyor, fakat TF aslýnda genel kullanýmý
olan bir paket. TF bir sayýsal hesap kütüphanesi, daha spesifik olarak, ona
çizit olarak verilen hesaplarý yapabilen bir sayýsal hesap paketi.

TF ile hesap yapmak için hesabý temsil eden bir çizit kurulur, mesela
$f(x,y) = x^2y + y + 2$ için

\includegraphics[width=20em]{tf_01.png}

Bu hesap aðaçýnda görülen sayýlar tek sayý olabilir, çok boyutlu vektör,
matris, ya da çok boyutlu matris olabilir. Matematikte bu objelere genel
olarak ``tensor'' ismi veriliyor, paketin ismi de buradan geliyor,
tensorlar hesap çiziti içinde bir hesaptan diðerine ``akýyorlar''
(flow). Hesabý çizit olarak belirtmenin bazý avantajlarý var, en önemlisi
çizit üzerinde direk otomatik türev alýnabilir, bkz. {\em Otomatik Türev
  Almak} yazýsý, ve bu þekilde gradyan hesaplarý kolay bir þekilde
yapýlabiliyor. Bir diðeri çizitin paralelleþtirme için doðal bir yapý
olmasý; çiziti istediðimiz þekilde bölerek parçalarý farklý mikroiþlemci
(CPU), ya da grafik iþlemci (GPU) üzerinde paralel bir þekilde
iþletebiliriz, mesela üstteki $f$ için $f(3,4)$ hesabý,

\includegraphics[width=20em]{tf_02.png}

þeklinde iki parçaya bölünebilir. 3 verilen soldaki parça kendi baþýna
hesabýný yaparken ayný anda 4 verilen diðer parça iþlemine devam
eder mesela. 

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
a = tf.constant([[1.], [2.]])
b = tf.constant([1., 2.])
c = tf.reduce_sum(a + b)
print tf.Session().run(c)
\end{minted}

\begin{verbatim}
12.0
\end{verbatim}

Lineer Regresyon

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

housing = fetch_california_housing(data_home="/home/burak/Downloads/scikit-data")
scaler = StandardScaler()
m, n = housing.data.shape
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
reset_graph()

n_epochs = 1000
learning_rate = 0.01

X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")
error = y_pred - y
mse = tf.reduce_mean(tf.square(error), name="mse")
\end{minted}

Elle Hesaplanan Gradyan

\begin{minted}[fontsize=\footnotesize]{python}
gradients = 2/np.float(m) * tf.matmul(tf.transpose(X), error)
training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)
    
    best_theta = theta.eval()
    
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450073)
('Epoch', 200, 'MSE =', 0.56670469)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881161)
('Epoch', 500, 'MSE =', 0.54363626)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214705)
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393812]
 [-0.04269557]
 [-0.66145277]
 [-0.63752776]]
\end{verbatim}

Otomatik Hesap ile Gradyan

\begin{minted}[fontsize=\footnotesize]{python}
gradients = tf.gradients(mse, [theta])[0]
training_op = tf.assign(theta, theta-(learning_rate*gradients))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
        if epoch % 100 == 0: print("Epoch", epoch, "MSE =", mse.eval())
	sess.run(training_op)
    
    best_theta = theta.eval()
    
print best_theta
\end{minted}

\begin{verbatim}
('Epoch', 0, 'MSE =', 9.1615429)
('Epoch', 100, 'MSE =', 0.71450061)
('Epoch', 200, 'MSE =', 0.56670463)
('Epoch', 300, 'MSE =', 0.55557162)
('Epoch', 400, 'MSE =', 0.54881167)
('Epoch', 500, 'MSE =', 0.5436362)
('Epoch', 600, 'MSE =', 0.53962916)
('Epoch', 700, 'MSE =', 0.53650916)
('Epoch', 800, 'MSE =', 0.53406781)
('Epoch', 900, 'MSE =', 0.53214717)
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]
\end{verbatim}








\end{document}
