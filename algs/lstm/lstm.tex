\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Uzun Kýsa-Vade Hafýza Aðlarý (Long Short-Term Memory Networks, LSTM)

Kendini tekrarlayan YSA (RNN) yapýlarýnýn içindeki gizli konum $s_t$ bir
zaman diliminden bir diðerine aktarýlabiliyordu, ve bu sýrada bir matris
çarpýmý üzerinden deðiþime uðrayabiliyordu. Böylece her zaman diliminde
yeni görülen verinin ``hafýza'' olarak ta görülebilecek $s_t$'ye etkisi
olabiliyordu. RNN dýþ dünya hakkýndaki iç modelini böyle güncelliyordu.

Fakat RNN ile tarif edilen bu güncellemeye hiç bir sýnýr getirmedik. Biraz
düþünürsek bu güncellemenin biraz kaotik bir hal alabileceðini görebiliriz
[1]. Mesela bir filmi kare kare izleyerek filmde neler olduðunu tarif
etmeye uðraþan bir RNN düþünelim. Bir karede bir karakterin ABD'de olduðunu
düþünebilir, ama sonraki karede karakterin suþi yediðini görüyor ve
Japonya'da olduðuna karar verebilir, sonra Panda ayýsý görüyor ve karakteri
kuzey kutbunda zannediyor olabilir.

Bu tarif edilen kaos enformasyonun çok hýzlý etki ettiðini ve ayný hýzda
yokolduðuna iþaret. Bu tür bir yapýda modelin uzun vadeli hafýza tutmasý
oldukça zor. Bize gereken modelin sadece güncelleme yapmasý deðil,
güncelleme yapmayý da öðrenmesi. Ali adlý bir karakter film karesinde yoksa
o kareler Ali hakkýndaki bilgiyi güncellemek için kullanýlmamalý, ayný
þekilde Ayþe'nin içinde olmadýðý kareler onun hakkýndaki bilgiyi
güncellemek için kullanýlmamalý. 


Zaman Serisi Sýnýflandýrmak

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd, zipfile
import tensorflow as tf
from tensorflow.contrib import rnn

learning_rate = 0.001
training_iters = 100000
batch_size = 25
display_step = 10

n_input = 1 
n_steps = 152 
n_hidden = 128 
n_classes = 2

with zipfile.ZipFile('wafer.zip', 'r') as z:
      df_train =  pd.read_csv(z.open('Wafer/wafer_TRAIN.txt'),header=None)
      df_test =  pd.read_csv(z.open('Wafer/wafer_TEST.txt'),header=None)

def minibatches(batch_size,input="train"):
      df = None
      if input=="train": df=df_train
      if input=="test": df=df_test
      df = np.array(df)
      for i in range(len(df)):
            batch_x = []; batch_y = []
            for j in range(batch_size):
                  batch_x.append(list(df[i,1:]))
                  batch_y.append([int(df[i,0]==-1), int(df[i,0]==1) ])
            batch_x = np.array(batch_x).reshape(batch_size,n_steps,1)
            batch_y = np.array(batch_y).reshape(batch_size,2)
            yield batch_x, batch_y                  

\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

reset_graph()

x = tf.placeholder("float", [None, n_steps, n_input])
y = tf.placeholder("float", [None, n_classes])

weights = {
    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
    'out': tf.Variable(tf.random_normal([n_classes]))
}

def LSTM(x, weights, biases):
    x = tf.unstack(x, n_steps, 1)
    lstm_cell = rnn.BasicLSTMCell(n_hidden)
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)
    return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = LSTM(x, weights, biases)

correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
new_pred = tf.argmax(y,1)

print 'cost'
scf = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)
cost = tf.reduce_mean(scf)
print 'optimizer'
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

mfile = "/tmp/time_series_classif"

init = tf.global_variables_initializer()
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    b_it = minibatches(batch_size)
    while step < int(1000 / batch_size):
          batch_x, batch_y = next(b_it)
          sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
          if step % display_step == 0:
                # Calculate batch accuracy
                acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})
                # Calculate batch loss
                loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})
                print("Iter " + str(step) + ", Minibatch Loss= " + \
                      "{:.6f}".format(loss) + ", Training Accuracy= " + \
                      "{:.5f}".format(acc))
          step += 1

    print("Optimization Finished!")
    saver.save(sess, mfile) # not shown in the book
\end{minted}

\begin{verbatim}
cost
optimizer
Iter 10, Minibatch Loss= 1.847300, Training Accuracy= 0.00000
Iter 20, Minibatch Loss= 0.049264, Training Accuracy= 1.00000
Iter 30, Minibatch Loss= 0.176535, Training Accuracy= 1.00000
Optimization Finished!
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
saver = tf.train.Saver()
from sklearn import metrics
real = []
pred = []
with tf.Session() as sess:
    saver.restore(sess, mfile)
    for batch_x, batch_y in minibatches(1,input="test"):
      res = sess.run(new_pred, feed_dict={x: batch_x, y: batch_y})
      pred.append(res[0])
      real.append(np.argmax(batch_y[0]))
    fpr, tpr, thresholds = metrics.roc_curve(np.array(real), np.array(pred))
    print 'AUC', metrics.auc(fpr, tpr)      
\end{minted}                    

\begin{verbatim}
AUC 1.0
\end{verbatim}





Kaynaklar

[1] Chen, {\em Exploring LSTMs: Understanding Basics (Part One)}, \url{https://www.topbots.com/exploring-lstm-tutorial-part-1-recurrent-neural-network-deep-learning/}

\end{document}

