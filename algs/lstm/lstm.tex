\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Uzun Kýsa-Vade Hafýza Aðlarý (Long Short-Term Memory Networks, LSTM)

Kendini tekrarlayan YSA (RNN) yapýlarýnýn içindeki gizli konum $s_t$ bir
zaman diliminden bir diðerine aktarýlabiliyordu, ve bu sýrada bir matris
çarpýmý üzerinden deðiþime uðrayabiliyordu. Böylece her zaman diliminde
yeni görülen verinin ``hafýza'' olarak ta görülebilecek $s_t$'ye etkisi
olabiliyordu. RNN dýþ dünya hakkýndaki iç modelini böyle güncelliyordu.

Fakat RNN ile tarif edilen bu güncellemeye hiç bir sýnýr getirmedik. Biraz
düþünürsek bu güncellemenin biraz kaotik bir hal alabileceðini görebiliriz
[1]. Mesela bir filmi kare kare izleyerek filmde neler olduðunu tarif
etmeye uðraþan bir RNN düþünelim. Bir karede bir karakterin ABD'de olduðunu
düþünebilir, ama sonraki karede karakterin suþi yediðini görüyor ve
Japonya'da olduðuna karar verebilir, sonra Panda ayýsý görüyor ve karakteri
kuzey kutbunda zannediyor olabilir.

Bu tarif edilen kaos enformasyonun çok hýzlý etki ettiðini ve ayný hýzda
yokolduðuna iþaret. Bu tür bir yapýda modelin uzun vadeli hafýza tutmasý
oldukca zor. Bize gereken modelin sadece güncelleme yapmasý deðil,
güncelleme yapmayý da öðrenmesi. Ali adlý bir karakter film karesinde yoksa
o kareler Ali hakkýndaki bilgiyi güncellemek için kullanýlmamalý, ayný
þekilde Ayþe'nin içinde olmadýðý kareler onun hakkýndaki bilgiyi
güncellemek için kullanýlmamalý. 

Çözüm için þöyle bir yaklaþým kullanabiliriz. 

1) Bir ``unutma'' mekanizmasý ekle. Film seyrediyoruz, bir sahne bitiyor, o
sahnenin hangi gün, saat kaçte, nerede olduðunu unutuyoruz. Fakat bir
karakter o sahnede ölmüþse, bunu hatýrlýyoruz. Modelin bu þekilde ne zaman
hatýrlayacaðýný, ne zaman unutacaðýný öðrenmesini istiyoruz.

2) Belleðe yazma (zulaya atma?) mekanizmasý. Model yeni bir kare gördüðünde
o karedeki bilginin kaydetmeye deðer olup olmadýðýna karar vermesi lazým,
ve bunu öðrenmesini istiyoruz. 

3) Yani yeni bir girdi gelince model ihtiyacý olmadýðý bilgiyi
unutacak. Sonra girdinin hangi kýsmýnýn faydalý olduðuna karar verecek ve o
kýsmý uzun-vadeli hafýzasýna kaydedecek.

4) Uzun-vadeli hafýzanýn hangi kýsmý sýk kullaným gerekiriyor (bir
odaklanma mekanizmasý), yani iþlem hafýzasý (working memory) hangisi, buna
karar vermek faydalý.

Yani bize gereken bir uzun kýsa-vade hafýza aðýdýr. RNN her zaman adýmýnda
hafýzasýný kontrolsüz bir þekilde güncelleyebiliyorken, bir LSTM hafýzasýný
çok daha seçici, kararlý bir þekilde günceller, bunu yaparken spesifik
öðrenme mekanizmalarý kullanýr ki bu mekanizma ona görülen bilginin hangi
kýsminin hatýrlanmaya deðer, hangisinin güncellenmesi, ve hangisinin daha
fazla odaklanýlma gerektiðini belirler. 

Matematiksel olarak $t$ anýnda bir $x_t$ girdisi alýyoruz, uzun-vadeli ve
iþlem hafýzasý $c_{t-1}$ ve $h_{t-1}$ bir önceki zaman diliminden bir
önceki bu zamana aktarýlýyor ve onlarý bir þekilde güncellemek
istiyoruz. Bize gereken bir tür hatýrlama geçidi (remember gate), bu
elektronik devrelerdeki gibi bir geçit, 0 ile 1 arasýnda olacak $n$ tane
sayý, bu sayý $n$ hafýza ögesinin ne kadar hatýrlanacaðýný, yani ne kadar
uzun-vadeli olup olmayacaðýný belirleyecek. 1 tut, 0 unut demek olacak.

Doðal olarak ufak bir YSA kullanarak bir geçidi öðrenebiliriz, 

$$ f_t = \sigma (W_r x_t + U_r h_{t-1}) $$

Bu basit, sýð (derin olmayan) bir YSA, $\sigma$ sigmoid aktivasyonu. Sigmoid
kullandýk çünkü 0 ile 1 arasýnda çýktýya ihtiyaçýmiz var. Þimdi girdiden
öðreneceðimiz bilgiyi hesaplamamýz lazým, bu bilgi uzun-vadeli hafýzamýz
için bir aday olacak. 

$$ c_t' = \phi(W_l x_t + U_l h_{t-1})$$












[devam edecek]



Zaman Serisi Sýnýflandýrmak

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd, zipfile
import tensorflow as tf
from tensorflow.contrib import rnn

learning_rate = 0.001
training_iters = 100000
batch_size = 25
display_step = 10

n_input = 1 
n_steps = 152 
n_hidden = 128 
n_classes = 2

with zipfile.ZipFile('wafer.zip', 'r') as z:
      df_train =  pd.read_csv(z.open('Wafer/wafer_TRAIN.txt'),header=None)
      df_test =  pd.read_csv(z.open('Wafer/wafer_TEST.txt'),header=None)

def minibatches(batch_size,input="train"):
      df = None
      if input=="train": df=df_train
      if input=="test": df=df_test
      df = np.array(df)
      for i in range(len(df)):
            batch_x = []; batch_y = []
            for j in range(batch_size):
                  batch_x.append(list(df[i,1:]))
                  batch_y.append([int(df[i,0]==-1), int(df[i,0]==1) ])
            batch_x = np.array(batch_x).reshape(batch_size,n_steps,1)
            batch_y = np.array(batch_y).reshape(batch_size,2)
            yield batch_x, batch_y                  

\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

reset_graph()

x = tf.placeholder("float", [None, n_steps, n_input])
y = tf.placeholder("float", [None, n_classes])

weights = {
    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
    'out': tf.Variable(tf.random_normal([n_classes]))
}

def LSTM(x, weights, biases):
    x = tf.unstack(x, n_steps, 1)
    lstm_cell = rnn.BasicLSTMCell(n_hidden)
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)
    return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = LSTM(x, weights, biases)

correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
new_pred = tf.argmax(y,1)

print 'cost'
scf = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)
cost = tf.reduce_mean(scf)
print 'optimizer'
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

mfile = "/tmp/time_series_classif"

init = tf.global_variables_initializer()
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    b_it = minibatches(batch_size)
    while step < int(1000 / batch_size):
          batch_x, batch_y = next(b_it)
          sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
          if step % display_step == 0:
                # Calculate batch accuracy
                acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})
                # Calculate batch loss
                loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})
                print("Iter " + str(step) + ", Minibatch Loss= " + \
                      "{:.6f}".format(loss) + ", Training Accuracy= " + \
                      "{:.5f}".format(acc))
          step += 1

    print("Optimization Finished!")
    saver.save(sess, mfile) # not shown in the book
\end{minted}

\begin{verbatim}
cost
optimizer
Iter 10, Minibatch Loss= 1.847300, Training Accuracy= 0.00000
Iter 20, Minibatch Loss= 0.049264, Training Accuracy= 1.00000
Iter 30, Minibatch Loss= 0.176535, Training Accuracy= 1.00000
Optimization Finished!
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
saver = tf.train.Saver()
from sklearn import metrics
real = []
pred = []
with tf.Session() as sess:
    saver.restore(sess, mfile)
    for batch_x, batch_y in minibatches(1,input="test"):
      res = sess.run(new_pred, feed_dict={x: batch_x, y: batch_y})
      pred.append(res[0])
      real.append(np.argmax(batch_y[0]))
    fpr, tpr, thresholds = metrics.roc_curve(np.array(real), np.array(pred))
    print 'AUC', metrics.auc(fpr, tpr)      
\end{minted}                    

\begin{verbatim}
AUC 1.0
\end{verbatim}

























\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
import numpy as np
from pprint import pprint
import datetime

np.random.seed(1)

t_min, t_max = 0, 30
resolution = 0.1

def f(t):
    return t * np.sin(t) / 3 + 2 * np.sin(t*5)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = f(Ts)
    X = ys[:, :-1].reshape(-1, n_steps, 1)
    y = ys[:, 1:].reshape(-1, n_steps, 1)
    y = y[:,-1,:]
    y = y.flatten()
    print y.shape    
    return list(X),y

X,y = next_batch(300, 6)
print X[0]
print y[0]

sequence_length = 6
instruction_count = 1

reference_input_data,reference_output_data = next_batch(300, 6)
   
NUM_EXAMPLES = len(reference_input_data) / 4 # we use 1/4 of the data for the training

test_input = reference_input_data[NUM_EXAMPLES:]
test_output = reference_output_data[NUM_EXAMPLES:] # everything beyond NUM_EXAMPLES

train_input = reference_input_data[:NUM_EXAMPLES]
train_output = reference_output_data[:NUM_EXAMPLES]

data = tf.placeholder(tf.float32, [None, sequence_length, instruction_count], name='data')
target = tf.transpose(tf.placeholder(tf.float32, [None], name='target'))

LSTM_SIZE = 40
FEATURE_SIZE = 1

def default_weights_and_bias():
    Weights = tf.Variable(tf.truncated_normal([LSTM_SIZE, LSTM_SIZE + FEATURE_SIZE], -0.2, 0.1))
    bias = tf.Variable(tf.constant(0.0, shape = [LSTM_SIZE, 1]))
    
    return Weights, bias

W_f, _ = default_weights_and_bias()

b_f = tf.Variable(tf.constant(1.0, shape = [LSTM_SIZE, 1]))

# The forget layer
#
# Shapes:
#   - W_f: 24x27
#   - ht_minus_1_and_xt: 27x?
#   - b_f: 24x1
#   - f_t: 24x?
def f_t(ht_minus_1_and_xt):
    return tf.sigmoid(tf.matmul(W_f, ht_minus_1_and_xt) + b_f)

W_i, b_i = default_weights_and_bias()

# Input Gate Layer
#
# Shapes:
#   - W_i: 24x27
#   - ht_minus_1_and_xt: 27x?
#   - b_i: 24x1
#   - i_t: 24x?
def i_t(ht_minus_1_and_xt):
    return tf.sigmoid(tf.matmul(W_i, ht_minus_1_and_xt) + b_i)

W_C, b_c = default_weights_and_bias()

# New Candidates for the Conveyor
#
# Shapes:
#   - W_C: 24x27
#   - ht_minus_1_and_xt: 27x?
#   - b_c: 24x1
#   - candidate_C_t: 24x?
def candidate_C_t(ht_minus_1_and_xt):
    return tf.tanh(tf.matmul(W_C, ht_minus_1_and_xt) + b_c)

# Updated Conveyor
#
# Shapes:
#   - f_t: 24x?
#   - Conveyor: 24x?
#   - i_t: 24x?
#   - CandidateConveyor: 24x?
def C_t(ht_minus_1_and_xt, Conveyor, CandidateConveyor):
    return f_t(ht_minus_1_and_xt) * Conveyor + i_t(ht_minus_1_and_xt) * CandidateConveyor

W_o, b_o = default_weights_and_bias()

# Updated Conveyor
#
# Shapes:
#   - W_o: 24x27
#   - b_o: 24x1
#   - ht_minus_1_and_xt: 27x?
#   - FinalConveyor: 24x?
#   - o_t: 24x?
#   - h_t: 24x?
def h_t(ht_minus_1_and_xt, FinalConveyor):
    o_t = tf.sigmoid(tf.matmul(W_o, ht_minus_1_and_xt) + b_o)
    
    return o_t * tf.tanh(FinalConveyor)

def lstm_cell(ht_minus_1_and_Conveyor, xt):
    ht_minus_1, Conveyor = ht_minus_1_and_Conveyor
    
    ht_minus_1_and_xt = tf.transpose(tf.concat([ht_minus_1, xt], 1))
    
    CandidateConveyor = candidate_C_t(ht_minus_1_and_xt)
    
    FinalConveyor = C_t(ht_minus_1_and_xt, Conveyor, CandidateConveyor)
    
    lstm_prediction = tf.transpose(h_t(ht_minus_1_and_xt, FinalConveyor))
    
    return(lstm_prediction, FinalConveyor)

data_length = tf.shape(data)[0]

def lstm_loop(last_lstm_prediction, last_state, step):
    lstm_prediction, state = lstm_cell((last_lstm_prediction, last_state),
                                       data[:, step, :])
    return lstm_prediction, state, tf.add(step, 1)

initial_Conveyor = tf.zeros([LSTM_SIZE, data_length])

initial_prediction = tf.zeros([data_length, LSTM_SIZE])

timesteps = sequence_length

for_each_time_step = lambda a, b, step: tf.less(step, timesteps)

lstm_prediction, lstm_state, _ = tf.while_loop(for_each_time_step,
                                               lstm_loop,
                                               (initial_prediction, initial_Conveyor, 0),
                                               parallel_iterations=32)

weight = tf.Variable(tf.truncated_normal([LSTM_SIZE, 1]))

bias = tf.Variable(tf.constant(0.0, shape=[1]))

prediction = tf.matmul(lstm_prediction, weight) + bias

with tf.name_scope('mean_square_error'):
    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(target, tf.unstack(prediction, axis = 1))))
    
tf.summary.scalar('mean_square_error', mean_square_error)

optimizer = tf.train.AdamOptimizer()

minimize = optimizer.minimize(mean_square_error)

with tf.name_scope('error'):
    with tf.name_scope('mistakes'):
        mistakes = tf.not_equal(target, tf.round(tf.unstack(prediction, axis = 1)))

sess = tf.InteractiveSession()

merged = tf.summary.merge_all()

date = str(datetime.datetime.now())

init_op = tf.global_variables_initializer()

saver = tf.train.Saver() 

sess.run(init_op)

epoch = 700

for i in range(epoch):
    if (i + 1) % 20 == 0:
        summary, mean_squ_err = sess.run([merged, mean_square_error], {data: test_input, target: test_output})        
        print('Epoch {:4d} | mean squ error {: 3.1f}'.format(i + 1, mean_squ_err))
    
    sess.run(minimize,{data: train_input, target: train_output})

saver.save(sess, "/tmp/lstm-time-")
    
sess.close()
\end{minted}

\begin{verbatim}
(300,)
[[-3.22914761]
 [-2.55665759]
 [-1.44953796]
 [-0.0840293 ]
 [ 1.30288372]
 [ 2.47053056]]
3.23248438178
(300,)
Epoch   20 | mean squ error  2586.9
Epoch   40 | mean squ error  1204.9
Epoch   60 | mean squ error  640.6
Epoch   80 | mean squ error  384.2
Epoch  100 | mean squ error  294.2
Epoch  120 | mean squ error  239.5
Epoch  140 | mean squ error  202.4
Epoch  160 | mean squ error  173.7
Epoch  180 | mean squ error  151.0
Epoch  200 | mean squ error  132.4
Epoch  220 | mean squ error  116.7
Epoch  240 | mean squ error  103.1
Epoch  260 | mean squ error  91.3
Epoch  280 | mean squ error  80.9
Epoch  300 | mean squ error  71.7
Epoch  320 | mean squ error  63.7
Epoch  340 | mean squ error  56.7
Epoch  360 | mean squ error  50.7
Epoch  380 | mean squ error  45.7
Epoch  400 | mean squ error  41.4
Epoch  420 | mean squ error  37.9
Epoch  440 | mean squ error  35.0
Epoch  460 | mean squ error  32.5
Epoch  480 | mean squ error  30.4
Epoch  500 | mean squ error  28.6
Epoch  520 | mean squ error  27.0
Epoch  540 | mean squ error  25.7
Epoch  560 | mean squ error  24.5
Epoch  580 | mean squ error  23.5
Epoch  600 | mean squ error  22.6
Epoch  620 | mean squ error  21.8
Epoch  640 | mean squ error  21.2
Epoch  660 | mean squ error  20.6
Epoch  680 | mean squ error  20.1
Epoch  700 | mean squ error  19.7
\end{verbatim}

















\begin{minted}[fontsize=\footnotesize]{python}
saver = tf.train.Saver() 

sess = tf.InteractiveSession()

tst_input = [[-3.22914761],
             [-2.55665759],
             [-1.44953796],
             [-0.0840293 ],
             [ 1.30288372],
             [ 2.47053056]]

tst_input = [ np.array(tst_input) ]
    
with tf.Session() as sess:
    saver.restore(sess, "/tmp/lstm-time-")
    print sess.run(prediction, { data: tst_input  } )

\end{minted}

\begin{verbatim}
[[ 3.26369405]]
\end{verbatim}




Kaynaklar

[1] Chen, {\em Exploring LSTMs: Understanding Basics (Part One)}, \url{https://www.topbots.com/exploring-lstm-tutorial-part-1-recurrent-neural-network-deep-learning/}

\end{document}

