\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Takviyeli Öðrenme (Reinforcement Learning)

Bilgisayar otomatik olarak oyun oynamayý öðrenebilir mi? Diyelim herhangi
bir bilgisayar oyunu, dama, satranç, ya da eðlence oyunlarýndan Pong.
Takviyeli öðrenme teknikleri ile bu sorunun cevabý evet.

Daha önce {\em Yapay Zeka ve Müsabaka} yazýsýnda farklý bir yaklaþým
gördük, bir deðer fonksiyonu vardý, bu fonksiyonlara tahtanýn durumunu
veriyorduk, deðer fonksiyonu da bize o pozisyonun taraflar için ne kadar
avantajlý olduðunu bir sayý ile raporluyordu. Bu fonksiyon bir kez, ve
önceden kodlanmaktaydý, ve oyun oynayan yapay zeka altüst (minimax)
algoritmasi ile kendisi için en avantajlý karþý taraf için en avantajsýz
pozisyonlarý bu fonksiyon ile deðerlendirerek bir arama algoritmasý
üzerinden buluyordu.

Fakat deðer fonksiyonu yaklaþýmýnýn bazý dezavantajlarý var, birincisi
fonksiyon deterministik olmasý. Oyun sýrasýnda deðiþmiyor, ve önceden
kodlanmýþ. Daha iyi bir yaklaþým olasýlýksal bir ilke (policy)
$\pi_\theta(a,s)$ kodlamak, ve bu ilkeyi her oyun sonunda
güncellemek. Böylece oyun sýrasýnda hem oyuncu yeni þeyler denemeye (açýk
fikirli!) hazýr oluyor, takýlýp kalmýyor, kazandýran ilkeler daha yoðun
olasýlýklara tekabül ettiði için yine iyi bir oyun oynama becerisine
kavuþuyor, ayrýca kendini sürekli güncelliyor.

Ýlke $\pi_\theta(a,s)$'de oyun konumu (state) $s$ ile, yapýlacak hareket
ise $a$ ile belirtilir. Pong örneðinde konum bir görüntü olarak bize
bildiriliyor olabilir, hareket ise raketin yukarý mý aþaðý mý gideceði;
verili konum $s$ için $\pi_\theta(a|s)$ bize, kazanmak için optimallik
baðlamýnda, mümkün tüm davranýþlarýn daðýlýmýný verecek.

$$ 
\nabla_\theta E_{x \sim \pi_\theta(x)} [f(x)] = 
\nabla_\theta \sum_x \pi f(x)
$$


$h(s,a,\theta) = \phi(s,a)^T\theta$

$$ 
\pi_\theta(s,a) = \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
$$

Usttekinin log'unun gradyani


$$ 
\nabla_\theta \log \pi_\theta = 
\nabla_\theta \log \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
$$

$$ = \nabla_\theta \big[ \log e^{h(s,a,\theta)} - \log \sum_b e^{h(s,b,\theta)}\big]$$

cunku 

$$ \log(\frac{x}{y}) = \log x - \log y $$

Devam edelim

$$ = \nabla_\theta \big[ h(s,a,\theta) - \log \sum_b e^{h(s,b,\theta)} \big]$$
Gradyan her iki terime de uygulanir,

$$ 
= \phi(s,a) - \sum_b h(s,b,\theta)\frac{e^{h(s,b,\theta)}}{\sum_b e^{h(s,b,\theta)}} 
$$

$$ 
= \phi(s,a) - \sum_b h(s,b,\theta) \pi_\theta(b,s)
$$








Kaynaklar

[1] Géron, {\em Hands-On Machine Learning with Scikit-Learn and TensorFlow}

[2] Bayramli, {\em OpenAI Gym, Pong, Derin Takviyeli Ogrenme},
\url{http://sayilarvekuramlar.blogspot.de/2017/09/openai-gym-pong-derin-takviyeli-ogrenme.html}

\end{document}
