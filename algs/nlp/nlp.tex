\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Derin Öðrenme ile Doðal Dil Ýþlemek (Natural Language Processing)

Doküman sýnýflamak, bir film için yazýlmýþ yorumu beðendi / beðenmedi
þeklinde irdelemek; tüm bu iþlemler doðal dil iþlemek kategorisine girer,
ve derin yapay sinir aðlarý (DYSA) ile halledilebilirler. 

Doküman nasýl temsil edilir? 

Doküman kelimelerden oluþur, fakat bir kelime temel olarak sayýsal deðil
kategoriktir, DYSA kullanmak için kelimelerin sayýsallaþtýrýlmasý
lazým. Bir çözüm öne-hot kodlamasý, tüm dokümanlardaki tüm kelimeler ön bin
kelimelik bir ``sözlükten'' geliyorsa, her kelime için ön bin boyutunda bir
vektör yaratýrýz, bu vektörde kelimelerin yerleri önceden bellidir, ``cat
(kedi)'' kelimesi mesela 300. indis, o zaman ``cat'' kelimesini temsil için
10,000 büyüklüðündeki bir vektörün 300. öðesi 1 diðer 9999 ögesi 0 olur.

Bu temsil þekli biraz israflý degil mi? Ayrýca kelimeler arasýnda benzerlik
için bize hiçbir fayda getirmiyor.

Daha önce {\em Lineer Cebir, SVD ile Kümeleme, Benzerlik} yazýsýnda boyut
azaltma iþleminden bahsettik. Bir kelimeyi, ya da dökümaný her ikisinin
iliþkisini içeren bir matris üzerinde SVD iþlettikten sonra daha ufak bir
boyutta temsil edebiliyorduk. Bu azaltýlmýþ boyutta kelimeler pür sayýsal
hale geliyordu ve kelimelere tekabül eden vektörlerin, anlamsal baðlamda,
birbirine yakýnlýk ya da uzaklýklarý bu sayýlar üzerinden ölçülebiliyordu.

\begin{verbatim}
cat:  (0.01359, ..., -0.2524, 1.0048, 0.06259)
mat:  (0.01396, ..., 0.033483, -0.10007, 0.1158)
chills: (-0.24776, ..., 0.079717, 0.23865, -0.014213)
sat:  (-0.35609, ..., -0.35413, 0.38511, -0.070976)
\end{verbatim}

DYSA ile ayný sonuç kelime gömme (word embedding) mekanizmasý ile elde
ediliyor. Fikir aslýnda gayet basit ve dahiyane. Tüm dokümanlar üzerinden
sözlüðü oluþtururuz, ama kelimeleri one-hot vektörü deðil, tek bir indis
haline getiririz, üstteki ``cat' sadece 300 sayýsýna dönüþür. Bunlardan
sadece sýnýrlý sayýda kelimeyi alýrýz, mesela ilk 5'i, her dokümanýn ilk 50
kelimesi tutulur, gerisi atýlýr, eðer eksik varsa dolgulama (padding) ile
sýfýrlar eklenip 5'e getirilir.

Tabii bu indis deðerleri YSA için direk kullanýlamaz, bir sonraki aþama,
YSA'ya bir gömme tabakasý eklemek, YSA'nýn eðitimde kullanacaðý esas
deðerler bunlar. Her kelimenin gömme boyutu $n$ önceden kararlaþtýrýlýr,
mesela $n=4$ diyelim, eðer sözlük büyüklüðü $|V|$ ise, $n \times |V|$
boyutunda bir büyük gömme referans matrisi elde edilir. YSA icinde bulunan
gömme girdi katmaný ise 5 x 4 = 20 olacaktýr.

\includegraphics[width=20em]{nlp_02.png}

Altta ``cat chills on a mat (kedi paspas üzerinde takýlýyor)'' cümlesini
görüyoruz,

\hspace{1.2cm}
\includegraphics[width=15em]{nlp_03.png}

Aslýnda girdi katmaný 5 x 4 boyutundaki bir ``tensor'' da olabilirdi
(modern YSA araçlarý çok boyutlu tensorlar ile rahatça çalýþýrlar), biz
basitleþtirme amaçýyla vektörün düzleþtirildiðini düþünelim,

\includegraphics[width=25em]{nlp_04.png}

Ama burada ilginç bir durum var, bu durum alýþýlagelen YSA kodlamasýndan
farklý; $x$ vektörüne bir ``girdi'' dendi, fakat $x$'i bir tamamen
baðlanmýþ aðýrlýk tabakasý olarak görmek daha doðru. Fakat bu aðýrlýk
tabakasý diðer aðýrlýk tabakalarý gibi deðil; Her eðitim veri noktasý için
indis bazlý esas girdilere göre, {\em referans gömme matrisindeki} uygun
satýrlar çekilip bir $x$ haline getiriliyor. Ardýndan geriye yayýlma ile
hata düzeltme yapýlacaðý zaman gömme referans matrisindeki vektörler
güncelleniyor. 

Peki eðitim hedef deðeri nedir? Burada farklý yaklaþýmlar var, üstteki
Lineer Cebir yazýsýnda komþu kelimeler tahmin edilmeye uðraþýlýr dedik. Bir
baþka ufak numara bir cümleyi alýp içindeki tek bir kelimeyi ``bozmak'',
yani oraya anlamsýz bir kelime getirmek, ve bu yeni cümleyi yanlýþ
bozulmamýþ olanýný doðru etiketi ile eðitim verisine koymak. Cümleler nasýl
olsa hazýr var, onlarý bozmak kolay, bu þekilde iki kategorili bir
sýnýflama problemi elde ediyoruz. 

0/1 hedefli ufak YSA'miz þöyle olabilir,

$x \in \mathbb{R}^{20 \times 1}, 
W \in \mathbb{R}^{8 \times 20}, 
U \in \mathbb{R}^{8 \times 1}$ 

Örnek girdi,

$$ x = \left[\begin{array}{rrrrr}
x_{cat} & x_{chills} & x_{on} & x_{a} & x_{mat}
\end{array}\right]$$

Katmanlar,

$$ s = U^T a$$

$$ a = f(z)$$

$$ z = Wx + b$$


\includegraphics[width=15em]{nlp_05.png}


\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
tf.reset_default_graph()

params = tf.constant([10,20,30,40])
ids = tf.constant([0,1,2,3])

with tf.Session() as sess:
     print tf.nn.embedding_lookup(params,ids).eval()
\end{minted}

\begin{verbatim}
[10 20 30 40]
\end{verbatim}



\includegraphics[width=35em]{nlp_01.png}




\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
import numpy as np
import data_helpers
from tensorflow.contrib import learn

dev_sample_percentage = .1
positive_data_file = "./data/rt-polarity.pos"
negative_data_file = "./data/rt-polarity.neg"
embedding_dim = 120
batch_size = 40
num_epochs = 200

x_text, y = data_helpers.load_data_and_labels(positive_data_file, negative_data_file)

\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
print y[3], x_text[3]
print y[4], x_text[4]
print y[10000], x_text[10000]
\end{minted}

\begin{verbatim}
[0 1] if you sometimes like to go to the movies to have fun , wasabi is a good place to start
[0 1] emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one
[1 0] like mike is a slight and uninventive movie like the exalted michael jordan referred to in the title , many can aspire but none can equal
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
max_document_length = max([len(x.split(" ")) for x in x_text])
vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
x = np.array(list(vocab_processor.fit_transform(x_text)))
print x
\end{minted}

\begin{verbatim}
[[    1     2     3 ...,     0     0     0]
 [    1    31    32 ...,     0     0     0]
 [   57    58    59 ...,     0     0     0]
 ..., 
 [   75    84  1949 ...,     0     0     0]
 [    1  2191  2690 ...,     0     0     0]
 [11512     3   147 ...,     0     0     0]]
\end{verbatim}




















\inputminted[fontsize=\footnotesize]{python}{nlp1.py}

\inputminted[fontsize=\footnotesize]{python}{nlp2.py}







Kaynaklar

[1] Britz, {\em Implementing a CNN for Text Classification in TensorFlow}, \url{http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/}

\end{document}
