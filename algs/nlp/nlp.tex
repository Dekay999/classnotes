\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Derin Ogrenme ile Dogal Dil Islemek (Natural Language Processing)

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np
import data_helpers

x_text, y = data_helpers.load_data_and_labels("./data/rt-polaritydata/rt-polarity.pos", "./data/rt-polaritydata/rt-polarity.neg")

max_document_length = max([len(x.split(" ")) for x in x_text])
print max_document_length
print x_text[1]
print x_text[2]
print x_text[3]
vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
x = np.array(list(vocab_processor.fit_transform(x_text)))

np.random.seed(10)
shuffle_indices = np.random.permutation(np.arange(len(y)))
x_shuffled = x[shuffle_indices]
y_shuffled = y[shuffle_indices]

dev_sample_index = -1 * int(0.80 * float(len(y)))
x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
print("Vocabulary Size: {:d}".format(len(vocab_processor.vocabulary_)))
print("Train/Dev split: {:d}/{:d}".format(len(y_train), len(y_dev)))
vocabulary_size = len(vocab_processor.vocabulary_)
embedding_size = 128

print('x_train', x_train.shape)
print x_train[19]
\end{minted}

\begin{verbatim}
56
the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth
effective but too tepid biopic
if you sometimes like to go to the movies to have fun , wasabi is a good place to start
Vocabulary Size: 18758
Train/Dev split: 2133/8529
('x_train', (2133, 56))
[   84  2733   249    38   182     1   995    35     9 18287  6323    58
 16066     9     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
\end{verbatim}


\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
from tensorflow.contrib import learn
from tensorflow.python.framework import ops

ops.reset_default_graph()
sess = tf.Session()

print('Creating Model')
embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))
b = tf.Variable(tf.random_normal(shape=[1,1]))

x_data = tf.placeholder(shape=[None, x_train.shape[1]], dtype=tf.int32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)

embed = tf.nn.embedding_lookup(embeddings, x_data)
embed_avg = tf.reduce_mean(embed, 1)
model_output = tf.add(tf.matmul(embed_avg, A), b)

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))
prediction = tf.round(tf.sigmoid(model_output))
predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)
accuracy = tf.reduce_mean(predictions_correct)

my_opt = tf.train.AdagradOptimizer(0.005)
train_step = my_opt.minimize(loss)

init = tf.initialize_all_variables()
sess.run(init)

for i in range(2):
    d = feed_dict={x_data: x_train[i].reshape(1,56), y_target: y_train[i]}
    print y_train[i]
    #train_loss_temp = sess.run(loss, d)
\end{minted}

\begin{verbatim}
Creating Model
[1 0]
[1 0]
\end{verbatim}














\end{document}
