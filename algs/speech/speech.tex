\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Konuþma Tanýma (Speech Recognition)

Frekans Üzerinden Özellik Çýkartýmý, RNN, LSTM, GRU

1 saniyelik ses dosyalarýmýz olsun, bu dosyalardaki ses kayýtlarý dört
farklý komutu olabilsin, Ýngilizce up, down, yes, no (yukarý, aþaðý, evet,
hayýr) komutlarý. Ses kayýtlarý aslýnda zaman serileridir, tek boyutlu bir
veri, mesela 1 saniyelik 16,000 sayý içeren bir vektör. Örnek bir 'down'
kaydýnýn neye benzediðini görelim,

\begin{minted}[fontsize=\footnotesize]{python}
import util
import scipy.io.wavfile, zipfile
import io, time, os, random, re

f = util.train_dir + '/down/004ae714_nohash_0.wav'
wav = io.BytesIO(open(f).read())
v = scipy.io.wavfile.read(wav)
print v[1]
plt.plot(v[1])
plt.savefig('speech_01.png')
\end{minted}

\begin{verbatim}
[-130 -135 -131 ..., -154 -190 -224]
\end{verbatim}

\includegraphics[width=20em]{speech_01.png}

Yapay öðrenme baðlamýnda zaman serileri için daha önce {\em Uzun Kýsa-Vade
  Hafýza Aðlarý} yazýsýnda LSTM yapýsýný görmüþtük. Örnek olarak zaman
serilerini sýnýflamýþtýk, zaman serisindeki tüm verileri LSTM'e
vermiþtik. Yani bir þeride 150 kusur veri noktasý varsa, o kadar LSTM
hücresi yaratýlacaktý. Fakat içinde binlerce öðe olan seriler için bu iyi
olmayabilir. Çözüm seriyi bir þekilde özetleyerek bu daha az olan veriyi
LSTM'e vermek.

Ses verilerini frekans üzerinden özetlemek iyi bilinen bir teknik, ses
verisi ufak pencerelere bölünür, bu pencereler üzerinde Fourier transformu
iþletilir, ve oradaki frekans bilgileri, hangi frekansýn ne kadar önemli
olduðu elde edilir. Spektogram bu bilgiyi renkli olarak göstermenin bir
yolu, üstteki ses için,

\begin{minted}[fontsize=\footnotesize]{python}
plt.specgram(v[1], Fs=util.fs, NFFT=1024)
plt.savefig('speech_02.png')
\end{minted}

\includegraphics[width=20em]{speech_02.png}

Spektogramýn örüntü tanýma için kullanýlabileceðini anlamak için bir tane
daha farklý 'down' sesi, bir de 'no' sesinin spektogramýna bakalým,

\begin{minted}[fontsize=\footnotesize]{python}
f1 = util.train_dir + '/down/0f3f64d5_nohash_2.wav'
wav1 = io.BytesIO(open(f1).read())
v1 = scipy.io.wavfile.read(wav1)
plt.specgram(v1[1], Fs=util.fs, NFFT=1024)
plt.savefig('speech_03.png')

f2 = util.train_dir + '/no/01bb6a2a_nohash_0.wav'
wav2 = io.BytesIO(open(f2).read())
v2 = scipy.io.wavfile.read(wav2)
plt.specgram(v2[1], Fs=util.fs, NFFT=1024)
plt.savefig('speech_04.png')
\end{minted}

\includegraphics[width=20em]{speech_03.png}
\includegraphics[width=20em]{speech_04.png}

Görüyoruz ki 'down' seslerinin spektogramlarý birbirine benziyor. Demek ki
öðrenme için bu yapýyý kullanabiliriz.

Kademeli LSTM

[..]

\includegraphics[width=20em]{stacked-rnn.png}

\inputminted[fontsize=\footnotesize]{python}{model1.py}

Modelin girdi tensor'unun boyutlarýný iþleye iþleye nasýl deðiþtirdiðini görelim,

\begin{minted}[fontsize=\footnotesize]{python}
import model1
m = model1.Model()
\end{minted}

\begin{verbatim}
Tensor("Placeholder_1:0", shape=(?, 16000), dtype=float32)
Tensor("stft/rfft:0", shape=(?, 124, 129), dtype=complex64)
Tensor("Abs:0", shape=(?, 124, 129), dtype=float32)
Tensor("rnn/transpose:0", shape=(?, 124, 200), dtype=float32)
LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 200) dtype=float32>)
LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 200) dtype=float32>)
LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_6:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_7:0' shape=(?, 200) dtype=float32>)
LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_8:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_9:0' shape=(?, 200) dtype=float32>)
Tensor("rnn/while/Exit_8:0", shape=(?, 200), dtype=float32)
Tensor("fully_connected/BiasAdd:0", shape=(?, 4), dtype=float32)
\end{verbatim}

\inputminted[fontsize=\footnotesize]{python}{train_rnn.py}

Ýþletmek için mesela \verb!python train_rnn.py model1! iþletince
\verb!model1.py! içindeki model yüklenecek ve eðitilmeye baþlanacak. Eðitim
sonrasý modelin baþarýsý eðitim verisi üzerinde yüzde 91, doðrulama
verisinde yüzde 92. 

[..]

Mikrofondan 1 saniyelik ses parçalarýný alýp parçayý model üzerinde iþletip
dört komuttan birini seçen örnek kod \verb!mic.py!'da bulunabilir.


CTC

\inputminted[fontsize=\footnotesize]{python}{train_ctc.py}


[devam edecek]

Kaynaklar

[1] Bayramli, {\em VCTK Ses Tanima Verisi, Konusmaci 225}, \url{https://www.dropbox.com/s/xecprghgwbbuk3m/vctk-pc225.tar.gz?dl=1}

[2] Remy, {\em Application of Connectionist Temporal Classification (CTC) for Speech Recognition},\url{https://github.com/philipperemy/tensorflow-ctc-speech-recognition}

[3] Graves, {\em Supervised Sequence Labelling with Recurrent Neural Networks}, \url{https://www.cs.toronto.edu/~graves/preprint.pdf}

[4] Graves, {\em How to build a recognition system (Part 2): CTC Loss}, \url{https://docs.google.com/presentation/d/12gYcPft9_4cxk2AD6Z6ZlJNa3wvZCW1ms31nhq51vMk}

[5] Graves, {\em How to build a recognition system (Part 1): CTC Loss}, \url{https://docs.google.com/presentation/d/1AyLOecmW1k9cIbfexOT3dwoUU-Uu5UqlJZ0w3cxilkI}

[6] Bayramli, {\em Voice Files}, \url{https://www.dropbox.com/s/qznhytp3iqaq6gk/voice_cmd_medium.zip?dl=1}

\end{document}
