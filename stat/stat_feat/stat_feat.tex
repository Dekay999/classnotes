\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Özellik İşleme, Seçme, Veri İncelemesi

Medyan ve Yüzdelikler (Percentile)

Üstteki hesapların çoğu sayıları toplayıp, bölmek üzerinden yapıldı. Medyan
ve diğer yüzdeliklerin hesabı (ki medyan 50. yüzdeliğe tekabül eder) için
eldeki tüm değerleri "sıraya dizmemiz" ve sonra 50. yüzdelik için
ortadakine bakmamız gerekiyor. Mesela eğer ilk 5. yüzdeliği arıyorsak ve
elimizde 80 tane değer var ise, baştan 4. sayıya / vektör hücresine / öğeye
bakmamız gerekiyor. Eğer 100 eleman var ise, 5. sayıya bakmamız gerekiyor,
vs.

Bu sıraya dizme işlemi kritik. Kıyasla ortalama hesabı hangi sırada olursa
olsun, sayıları birbirine topluyor ve sonra bölüyor. Zaten ortalama ve
sapmanın istatistikte daha çok kullanılmasının tarihi sebebi de aslında bu;
bilgisayar öncesi çağda sayıları sıralamak (sorting) zor bir işti. Bu
sebeple hangi sırada olursa olsun, toplayıp, bölerek hesaplanabilecek
özetler daha makbuldü. Fakat artık sıralama işlemi kolay, ve veri setleri
her zaman tek tepeli, simetrik olmayabiliyor. Örnek veri seti olarak ünlü
\verb!dellstore2! tabanındaki satış miktarları kullanırsak,

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)
\end{minted}

\begin{verbatim}
213.948899167
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.median(data)
\end{minted}

\begin{verbatim}
214.06
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.std(data)
\end{minted}

\begin{verbatim}
125.118481954
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)+2*np.std(data)
\end{minted}

\begin{verbatim}
464.185863074
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.percentile(data, 95)
\end{minted}

\begin{verbatim}
410.4115
\end{verbatim}

Görüldüğü gibi üç nokta hesabı için ortalamadan iki sapma ötesini
kullanırsak, 464.18, fakat 95. yüzdeliği kullanırsak 410.41 elde
ediyoruz. Niye? Sebep ortalamanın kendisi hesaplanırken çok üç
değerlerin toplama dahil edilmiş olması ve bu durum, ortalamanın
kendisini daha büyük seviyeye doğru itiyor. Yüzdelik hesabı ise sadece
sayıları sıralayıp belli bazı elemanları otomatik olarak üç nokta
olarak addediyor.

Box Whisker Grafikleri

Tek boyutlu bir verinin dağılımını görmek için Box ve Whisker grafikleri
faydalı araçlardır; medyan (median), dağılımın genişliğini ve sıradışı
noktaları (outliers) açık şekilde gösterirler. İsim nereden geliyor? Box
yani kutu, dağılımın ağırlığının nerede olduğunu gösterir, medyanın
sağındada ve solunda olmak üzere iki çeyreğin arasındaki kısımdır, kutu
olarak resmedilir. Whiskers kedilerin bıyıklarına verilen isimdir, zaten
grafikte birazcık bıyık gibi duruyorlar. Bu uzantılar medyan noktasından
her iki yana kutunun iki katı kadar uzatılır sonra verideki "ondan az olan
en büyük" noktaya kadar geri çekilir. Tüm bunların dışında kalan veri ise
teker teker nokta olarak grafikte basılır. Bunlar sıradışı (outlier)
oldukları için daha az olacakları tahmin edilir.

BW grafikleri iki veriyi dağılımsal olarak karşılaştırmak için
birebirdir. Mesela Larsen and Marx adlı araştırmacılar çok az veri
içeren Quintus Curtius Snodgrass veri setinin değişik olduğunu
ispatlamak için bir sürü hesap yapmışlardır, bir sürü matematiksel
işleme girmişlerdir, fakat basit bir BW grafiği iki setin farklılığını
hemen gösterir.

BW grafikleri iki veriyi dağılımsal olarak karşılaştırmak için
birebirdir. Mesela Larsen and Marx adlı araştırmacılar çok az veri
içeren Quintus Curtius Snodgrass veri setinin değişik olduğunu
ispatlamak için bir sürü hesap yapmışlardır, bir sürü matematiksel
işleme girmişlerdir, fakat basit bir BW grafiği iki setin farklılığını
hemen gösterir.

Python üzerinde basit bir BW grafiği 

\begin{minted}[fontsize=\footnotesize]{python}
spread= rand(50) * 100
center = ones(25) * 50
flier_high = rand(10) * 100 + 100
flier_low = rand(10) * -100
data =concatenate((spread, center, flier_high, flier_low), 0)
plt.boxplot(data)
plt.savefig('stat_feat_01.png')
\end{minted}

\includegraphics[height=6cm]{stat_feat_01.png}

Bir diğer örnek Glass veri seti üzerinde

\begin{minted}[fontsize=\footnotesize]{python}
data = loadtxt("glass.data",delimiter=",")
head = data[data[:,10]==7]
tableware = data[data[:,10]==6]
containers = data[data[:,10]==5]

print head[:,1]

data =(containers[:,1], tableware[:,1], head[:,1])

plt.yticks([1, 2, 3], ['containers', 'tableware', 'head'])

plt.boxplot(data,0,'rs',0,0.75)
plt.savefig('stat_feat_02.png')
\end{minted}

\begin{verbatim}
[ 1.51131  1.51838  1.52315  1.52247  1.52365  1.51613  1.51602  1.51623
  1.51719  1.51683  1.51545  1.51556  1.51727  1.51531  1.51609  1.51508
  1.51653  1.51514  1.51658  1.51617  1.51732  1.51645  1.51831  1.5164
  1.51623  1.51685  1.52065  1.51651  1.51711]
\end{verbatim}

\includegraphics[height=6cm]{stat_feat_02.png}

Zaman Kolonlarını Zenginleştirmek

Veri madenciliğinde "veriden veri yaratma" tekniği çok kullanılıyor; mesela
bir sipariş veri satırında o siparişin hangi zamanda (timestamp) olduğunu
belirten bir kolon varsa (ki çoğu zaman vardır), bu kolonu "parçalayarak"
ek, daha genel, özetsel bilgi kolonları yaratılabilir. Zaman kolonları çoğu
zaman saniyeye kadar kaydedilir, bu bilgiyi alıp mesela ay, mevsim,
haftanın günü, saat, iş saati mi (9-5 arası), akşam mı, sabah mı, öğlen mi,
vs. gibi ek bilgiler çıkartılabilir. Tüm kolonlar veri madenciliği
algoritmasına verilir, ve algoritma belki öğlen saati ile sipariş verilmiş
olması arasında genel bir bağlantı bulacaktır.

Python + Pandas ile bir zaman kolonu şöyle parçalanabilir, örnek veri
üzerinde görelim, sadece iki kolon var, müşteri no, ve sipariş zamanı,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
from StringIO import StringIO
s = """customer_id;order_date
299;2012-07-20 19:44:55.661000+01:00
421;2012-02-17 21:54:15.013000+01:00
437;2012-02-20 22:18:12.021000+01:00
463;2012-02-20 23:46:21.587000+01:00
482;2012-05-21 09:50:02.739000+01:00
607;2012-02-21 11:57:12.462000+01:00
641;2012-02-21 13:40:28.088000+01:00
674;2012-08-21 14:53:15.851000+01:00
780;2012-02-23 10:31:05.571000+01:00
"""
df = pd.read_csv(StringIO(s),sep=';', parse_dates=True)

def f(x):
   tmp = pd.to_datetime(x['order_date'])
   tpl = tmp.timetuple(); yymm = int(tmp.strftime('%m%d'))
   spring = int(yymm >= 321 and yymm < 621)
   summer = int(yymm >= 621 and yymm < 921)
   fall = int(yymm >= 921 and yymm < 1221)
   winter = int( spring==0 and summer==0 and fall==0 )
   warm_season = float(tpl.tm_mon >= 4 and tpl.tm_mon <= 9)
   work_hours = float(tpl.tm_hour > 9 and tpl.tm_hour < 17)
   morning = float(tpl.tm_hour >= 7 and tpl.tm_hour <= 11)
   noon = float(tpl.tm_hour >= 12 and tpl.tm_hour <= 14)
   afternoon = float(tpl.tm_hour >= 15 and tpl.tm_hour <= 19)
   night = int (morning==0 and noon==0 and afternoon==0)

   return pd.Series([tpl.tm_hour, tpl.tm_mon,
                     tpl.tm_wday, warm_season,
                     work_hours, morning, noon, afternoon, night,
                     spring, summer, fall, winter])
cols = ['ts_hour','ts_mon','ts_wday','ts_warm_season',
        'ts_work_hours','ts_morning','ts_noon','ts_afternoon',
        'ts_night', 'ts_spring', 'ts_summer', 'ts_fall', 'ts_winter']
df[cols] = df.apply(f, axis=1)
print df[cols]
\end{minted}

\begin{verbatim}
   ts_hour  ts_mon  ts_wday  ts_warm_season  ts_work_hours  ts_morning  \
0       18       7        4               1              0           0   
1       20       2        4               0              0           0   
2       21       2        0               0              0           0   
3       22       2        0               0              0           0   
4        8       5        0               1              0           1   
5       10       2        1               0              1           1   
6       12       2        1               0              1           0   
7       13       8        1               1              1           0   
8        9       2        3               0              0           1   

   ts_noon  ts_afternoon  ts_night  ts_spring  ts_summer  ts_fall  ts_winter  
0        0             1         0          0          1        0          0  
1        0             0         1          0          0        0          1  
2        0             0         1          0          0        0          1  
3        0             0         1          0          0        0          1  
4        0             0         0          1          0        0          0  
5        0             0         0          0          0        0          1  
6        1             0         0          0          0        0          1  
7        1             0         0          0          1        0          0  
8        0             0         0          0          0        0          1  
\end{verbatim}

Sıcak mevsim (warm season) Mart-Eylül aylarını kapsar, bu ikisel bir
değişken hale getirildi. Belki siparişin, ya da diğer başka bir verinin
bununla bir alakası vardır. Genel 4 sezon tek başına yeterli değil midir?
Olabilir, fakat bazı kalıplar / örüntüler (patterns) belki sıcak / soğuk
mevsim bilgisiyle daha çok bağlantılıdır. 

Aynı şekilde saat 1-24 arasında bir sayı olarak var, fakat "iş saatini"
ayrı bir ikisel değişken olarak kodlamak yine bir "kalıp yakalama"
şansımızı arttırabilir. Bu kolonun ayrı bir şekilde kodlanmış olması veri
tasarımı açısından ona önem verildiğini gösterir, ve madencilik
algoritmaları bu kolonu, eğer ona bağlı bir kalıp var ise,
yakalayabilirler.

Not: Burada ufak bir pürüz sabah, öğlen, akşamüstü gibi zamanları kodlarken
çıktı. Gece 19'dan sonra ve 7'den önce bir sayı olacaktı, fakat bu durumda
$x>19$ ve $x<7$ hiçbir sonuç getirmeyecekti. Burada saatlerin 24 sonrası başa
dönmesi durumu problem çıkartıyordu, tabii ki karşılaştırma ifadelerini
çetrefilleştirerek bu iş çözülebilir, ama o zaman kod temiz olmaz (mesela
($x>19$ ve $x<24$) ya da ($x>0$ ve $x<7$) yapabilirdik). Temiz kod için gece
haricinde diğer tüm seçenekleri kontrol ediyoruz, ve gece "sabah, öğlen,
akşamüstü olmayan şey" haline geliyor. Aynı durum mevsimler için de
geçerli. Onun için 

\begin{minted}[fontsize=\footnotesize]{python}
night = int (morning==0 and noon==0 and afternoon==0)
\end{minted}

kullanıldı.

Kategörleri İkileştirme, Anahtarlama Numarası (Öne-Hot Encoding, Hashing Trick)

Üstteki kodda bir problem var, dokümanı temsil eden ve içinde 1 ya da
0 hücreli özellik vektörünü (feature vector) oluşturmak için tüm
kelimelerin ne olduğunu bilmeliyiz. Yani veriyi bir kere baştan sonra
tarayarak bir sözlük oluşturmalıyız (ki öyle yapmaya mecbur kaldık) ve
ancak ondan sonra her doküman için hangi kelimenin olup olmadığını
saptamaya ve onu kodlamaya başlayabiliriz. Halbuki belgelere bakar
bakmaz, teker teker giderken bile hemen bir özellik vektörü
oluşturabilseydik daha iyi olmaz mıydı?

Bunu başarmak için anahtarlama numarasını kullanmamız lazım. Bilindiği
gibi temel yazılım bilime göre bir kelimeyi temsil eden bir anahtar
(hash) üretebiliriz, ki bu hash değeri bir sayıdır. Elimizde bir
"sayı" olması bize faydalı olur yarar, bu sayının en fazla kaç
olabileceğinden hareketle (hatta bu sayıya bir limit koyarak) özellik
vektörümüzün boyutunu önceden saptamış oluruz.  Sonra kelimeye
bakarız, hash üretiriz, sonuç mesela 230 geldi, o zaman özellik
vektöründeki 230'uncu kolonun değerini 1 yaparız. 

\begin{minted}[fontsize=\footnotesize]{python}
d_input = dict()

def add_word(word):
    hashed_token = hash(word) % 127
    d_input[hashed_token] = d_input.setdefault(hashed_token, 0) + 1

add_word("obama")
print d_input
\end{minted}

\begin{verbatim}
{48: 1}
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
add_word("politics")
print d_input
\end{minted}

\begin{verbatim}
{48: 1, 91: 1}
\end{verbatim}

Üstteki kodda bunun örneğini görüyoruz. Hash sonrası mod uyguladık
(yüzde işareti ile) ve hash sonucunu en fazla 127 olacak şekilde
sınırladık. Sözlük (dictionary) yavaş yavaş büyüyebiliyor.
Potansiyel problemler ne olabilir? Hashing mükemmel değildir, çarpışma
(collision) olması mümkündür yani nadiren farklı kelimelerin aynı
numaraya eşlenebilmesi durumu. Bu problemleri iyi bir anahtarlama
algoritması kullanarak, mod edilen sayıyı büyük tutarak çözmek
mümkündür, ya da bu tür nadir çarpışmalar "kabul edilir hata" olarak
addedilebilir.

Pandas kullanarak bir Dataframe'i otomatik olarak anahtarlamak istersek,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

data = pd.DataFrame(data)
print data
\end{minted}

\begin{verbatim}
   pop   state  year
0  1.5    Ohio  2000
1  1.7    Ohio  2001
2  3.6    Ohio  2002
3  2.4  Nevada  2001
4  2.9  Nevada  2002
\end{verbatim}

Şimdi bu veri üzerinde sadece eyalet (state) için bir anahtarlama numarası
yapalım

\begin{minted}[fontsize=\footnotesize]{python}
def hash_col(df,col,N):
    for i in range(N): df[col + '_' + str(i)] = 0.0
    df[col + '_hash'] = df.apply(lambda x: hash(x[col]) % N,axis=1)    
    for i in range(N):
        idx = df[df[col + '_hash'] == i].index
        df.ix[idx,'%s_%d' % (col,i)] = 1.0
    df = df.drop([col, col + '_hash'], axis=1)
    return df

print hash_col(data,'state',4)
\end{minted}

\begin{verbatim}
   pop  year  state_0  state_1  state_2  state_3
0  1.5  2000        0        1        0        0
1  1.7  2001        0        1        0        0
2  3.6  2002        0        1        0        0
3  2.4  2001        0        0        0        1
4  2.9  2002        0        0        0        1
\end{verbatim}

Azar Azar İşlemek (Incremental, Minibatch Processing)

Üstteki yöntemler eğer tüm veri hafızada ise iyi. Fakat çoğu zaman onlarca
kategori, birkaç milyonluk satır içeren bir veriye bakmamız gerekiyor;
biliyoruz ki bu kadar veri için Büyük Veri teknolojilerine (mesela Spark,
Hadoop gibi) geçmek gereğinden fazla külfet getirecek, elimizdeki dizüstü,
masaüstü bilgisayarı bu işlemler için yeterli olmalı, fakat çoğu kütüphane
tek makinada azar azar işlem yapmak için yazılmamış.

Bu durumda kendimiz çok basit Python kavramlarını, iyi bir anahtarlama
kodunu, ve lineer cebir hesaplarında seyreklik (sparsıty) tekniklerini
kullanarak ufak veri parçaları işleyen bir ortamı yaratabiliriz.

Örnek veri olarak {\em Lineer Regresyon} yazısında görülen oy kalıpları
verisini biraz değiştirerek yeni bir analiz için kullanalım. Veri oy
verenlerin ırk, cinsiyet, meslek, hangi partiye oy verdikleri ve
kazançlarını kaydetmiş, biz analizimizde bahsedilen kategorilerin bu
kişilerin kazancıyla bağlantılı olup olmadığına bakacağız. Veriyi oluşturalım,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('../stat_logit/nes.dat',sep=r'\s+')
df = df[['presvote','year','gender','income','race','occup1']]
df = df.dropna()
df.to_csv('nes2.csv',index=None)
\end{minted}

Önce kategorilerden ne kadar var, sayalım. Basit toplam yani,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('nes2.csv')
print u'tüm veri', len(df)
print 'cinsiyet', np.array(df['gender'].value_counts())
print u'ırk', np.array(df['race'].value_counts())
print 'parti', np.array(df['presvote'].value_counts())
\end{minted}

\begin{verbatim}
tüm veri 13804
cinsiyet [7461 6343]
ırk [12075  1148   299   180    85    17]
parti [6998 6535  271]
\end{verbatim}

Mesela son sonuçtaki her hücre belli bir partiye verilen oyların sayısı;
veriye göre üç farklı kategori varmış demek ki, veri ABD için olduğuna göre
bunlardan ilk ikisi bilinen iki büyük parti, üçüncü hücre de herhalde
bağımsız adaylar.

Kategorik verileri ikileştirmeye gelelim. Burada üç nokta önemli, azar azar
işleyeceğiz, sonucu seyrek matris almak istiyoruz, ve hangi kategorik
değerin hangi kolona eşleneceğini elle tanımlamak istemiyoruz (eşleme
otomatik olmalı). Seyreklik önemli çünkü eğer 1000 farklı kategorik değere
sahip olan 10 tane kolon varsa, bu 10000 tane yeni kolon yaratılması
demektir - her farklı kategori için o değere tekabül eden kolon 1 olacak
gerisi 0 olacak. Bu rakamlar orta ölçekte bile rahatlıkla milyonlara
ulaşabilir. Eğer ikileştirme için seyrek matris kullanırsak çoğu sıfır olan
değerler hafızada bile tutulmaz. Eşleme otomatik olmalı, zaten onun için
anahtarlama yapacağız. 

Anahtarlama icin \verb!sklearn.feature_extraction.text.HashingVectorizer!
var,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
vect = HashingVectorizer(n_features=20)
a = ['aa','bb','cc']
res = vect.transform(a)
print res
\end{minted}

\begin{verbatim}
  (0, 5)	1.0
  (1, 19)	1.0
  (2, 18)	-1.0
\end{verbatim}

Sonuçlar seyrek matris olarak, ve üç değer için üç ayrı satır olarak
geldi. Anahtarlama niye-1 ya da +1 veriyor? Aslında bu bizim için çok faydalı,
çünkü birazdan PCA işleteceğiz mesela ve bu yöntem her veri kolonunun
sıfırda ortalanmış olmasını ister. Üstteki teknikte bizim tahminimiz -1,+1
arasında rasgele seçim yapılıyor böylece üretilen anahtar kolonlarında
-1 / +1 doğal olarak dengelenmiş olacaktır, o zaman otomatik olarak
ortalamaları sıfıra iner. Akıllıca bir teknik.

Devam edelim, sonucu tek satır olacak şekilde kendimiz tekrar
düzenleyebiliriz. O zaman Python \verb!yield! kavramını [3] kullanarak
(azar azar satır okumak için), ve anahtarlama, seyrek matrisler ile şu
şekilde bir kod olabilir,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
import pandas as pd, csv
import scipy.sparse as sps

HASH = 30
vect = HashingVectorizer(decode_error='ignore',n_features=HASH)

def get_row(cols):
    with open("nes2.csv", 'r') as csvfile:
        rd = csv.reader(csvfile)
        headers = {k: v for v, k in enumerate(next(rd))}
        for row in rd:
            label = float(row[headers['income']])
            rrow = [x + str(row[headers[x]]) for x in headers if x in cols]
            X_train = vect.transform(rrow)
            yield X_train.tocoo(), label            

def get_minibatch(row_getter,size=10):
    X_train = sps.lil_matrix((size,HASH))
    y_train = []
    for i in range(size):
        cx,y = row_getter.next()
        for dummy,j,val in zip(cx.row, cx.col, cx.data): X_train[i,j] = val
        y_train.append(y)
    return X_train, y_train
    
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
X,y = get_minibatch(row_getter,size=1)
print y, X.todense()
\end{minted}

\begin{verbatim}
[4.0] [[ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]]
\end{verbatim}

İlgilendiğimiz kolon listesini \verb!get_row!'a verip bir gezici fonksiyon
yarattık. Bu geziciyi \verb!get_minibatch!'e verdik, kaç tane satır
istediğimizi ona söylüyoruz, o bize istenen kadar satırı seyrek matris
olarak veriyor. 10 tane daha isteyelim,

\begin{minted}[fontsize=\footnotesize]{python}
X,y = get_minibatch(row_getter,size=10)
print len(y), X.shape, type(X)
\end{minted}

\begin{verbatim}
10 (10, 30) <class 'scipy.sparse.lil.lil_matrix'>
\end{verbatim}

PCA

Lineer Cebir'in temel bileşen analizi (PCA) tekniğini kullanarak boyut
azaltması yapabiliriz. Bu yazıda veriyi satır satır işleyerek PCA hesabı
yapan bir kod var, ayrıca bu kod satırları seyrek formatta alabiliyor. Bu
iyi haber, çünkü bu ufak örnekte onlarca kolon var fakat milyonlarca kolon
olsa bu kod yine işleyecektir. 

\begin{minted}[fontsize=\footnotesize]{python}
import sys; sys.path.append('../../linear/linear_app05pca')
import ccipca
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
pca = ccipca.CCIPCA(n_components=10,n_features=30)
for i in range(10000): 
    X,y = get_minibatch(row_getter,size=1)
    pca.partial_fit(X)
pca.post_process()

print 'varyans orani'
print pca.explained_variance_ratio_
\end{minted}

\begin{verbatim}
varyans orani
[ 0.36086926  0.16186391  0.13377998  0.09440711  0.0702763   0.05113956
  0.04768294  0.0343724   0.02336052  0.02224802]
\end{verbatim}

Her bileşenin verideki varyansın ne kadarını açıkladığı görülüyor. 30
kolonu 10 kolona indirdik, acaba veri temsilinde ilerleme elde ettik mi?
Veriyi PCA'nın bulduğu uzaya yansıtıp bu boyutu azaltılmış veriyi
regresyonda kullansak ne olur acaba? Yansıtma ve regresyon,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)
P = pca.components_.T
for i in range(10000):
    X_train, y_train = get_minibatch(row_getter,1)
    Xp = np.dot((X_train-pca.mean_),P)
    clf.partial_fit(Xp, y_train)
\end{minted}

Şimdi sonraki 1000 satırı test için kullanalım,

\begin{minted}[fontsize=\footnotesize]{python}
y_predict = []
y_real = []
for i in range(1000):
    X_test,y_test = get_minibatch(row_getter,1)
    Xp = np.dot((X_test-pca.mean_),P)
    y_predict.append(clf.predict(Xp)[0])
    y_real.append(y_test[0])
y_predict = np.array(y_predict)
y_real = np.array(y_real)
print 'ortalama tahmin hatasi', np.sqrt(((y_predict-y_real)**2).sum()) / len(y_predict)
print 'maksimum deger', np.max(y_real)
\end{minted}

\begin{verbatim}
ortalama tahmin hatasi 0.0105872845541
maksimum deger 5.0
\end{verbatim}

1 ile 5 arasinda gidip gelen degerlerin tahmininde 0.01 civari ortalama
hata var. Fena degil. Peki verinin kendisini oldugu gibi alip regresyonda
kullansaydik? Regresyonun hedef verisi kazanç, kaynak verisi geri kalan
kategoriler. Her parça 1000 satır, 10 parça alacağız, 10,000 satır modeli
eğitmek için kullanılacak. Geri kalanlar test verisi
olacak. \verb!sklearn.linear_model.SGDRegressor! ufak seyrek matris
parçaları ile eğitilebiliyor,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)

y_predict = []; y_real = []

for i in range(10):
    X_train, y_train = get_minibatch(row_getter,1000)
    clf.partial_fit(X_train, y_train)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
X_test,y_test = get_minibatch(row_getter,1000)
print 'Skor', clf.score(X_test, y_test)    
y_predict = clf.predict(X_test) 
print 'ortalama tahmin hatasi', np.sqrt(((y_predict-y_test)**2).sum()) / len(y_predict)
\end{minted}

\begin{verbatim}
Skor 0.609560912327
ortalama tahmin hatasi 0.0208096951078
\end{verbatim}


Bu sonuç ta hiç fena değil.  Sonuç olarak veri içinde bazı kalıplar
olduğunu gördük, tahmin yapabiliyoruz. Hangi kolonların daha önemli
olduğunu bulmak için her kolonu teker teker atıp skorun yukarı mı aşağı mı
indiğine bakabilirdik.

Kaynaklar 

[1] Teetor, {\em R Cookbook}

[2] Scikit-Learn Documentation, {\em 4.2. Feature extraction}, \url{http://scikit-learn.org/dev/modules/feature_extraction.html}

[3] Bayramli, {\em Fonksiyon Gezmek ve Yield}, \url{http://sayilarvekuramlar.blogspot.com/2011/02/fonksiyon-gezmek-ve-yield.html}

\end{document}
	

