\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}

Ekler (Appendix) 

Binom ve $p$ Ýçin Maksimum Olurluk Tahmini [1]

$$ L(p;x) = \prod_{i=1}^n f(x_i;p) = \prod_{i=1}^n {n \choose x} p^x(1-p)^{1-x} $$

Log alalým

$$ \log L(p;x) = 
\sum_{i=1}^n \log {n \choose x} + x \log p + (1-x) \log (1-p) $$

$p$'ye göre türevi alalým, bu sýrada kombinasyon ifadesi ${n \choose x}$
içinde $p$ olmadýðý için o yokolacaktýr,

$$ \frac{\partial \log L(p)}{\partial p} =
\frac{x}{p} - \frac{n-x}{1-p}
$$

Maksimum deðeri bulmak için sýfýra eþitleyelim ve $p$ için çözelim,

$$ 0 = \frac{x}{p} - \frac{n-x}{1-p} $$

$$  \frac{x}{p} = \frac{n-x}{1-p}  $$

$$ p(n-x)  = x(1-p) $$

$$ pn - px = x-px $$

$$ pn = x $$

$$ p = \frac{x}{n} $$

Yani $p$ için maksimum olurluk tahmini $x/n$. 

Bernoulli daðýlýmý Binom daðýlýmýna çok benzer, sadece onun baþ kýsmýnda
kombinasyon ifadesi yoktur. Fakat o ifade $p$'ye göre türevde nasýl olsa
yokolacaðýna göre Bernoulli daðýlýmý için de tahmin edici aynýdýr.

\newpage

Bayes Usulü Güven Aralýðý (Confidence Intervals) 

Bayes ile bu hesabý yapmak için bir daðýlýmý baz almak lazým. Eðer sonuç
olarak bir tek sayý deðil, bir daðýlým elde edersek bu daðýlým üzerinde
güvenlik hesaplarýný yaparýz. Mesela sonuç, sonsal daðýlým (posterior) bir
Gaussian daðýlým ise, bu daðýlýmýn yüzde 95 aðýrlýðýnýn nerede olduðu, ve
nasýl hesaplandýðý bellidir.

Bayes Teorisi

$$ P(A \mid B)  = \frac{P(B \mid A)P(A)}{P(B)} $$

Veri analizi baðlamýnda diyelim ki deneyler yaparak tahmini olarak
hesaplamak (estimate) istediðimiz bir parametre var, bu bir protonun
kütlesi ya da bir ameliyat sonrasý hayatta kalma oraný olabilir. Bu
durumlarda iki ayrý "olaydan" bahsetmemiz gerekir, B olayý spesifik bazý
ölçümlerin elde edilmesi "olayýdýr", mesela ölçüm üç sayýdan oluþuyorsa,
biz bir ölçümde spesifik olarak $\{0.2,4,5.4\}$ deðerlerini elde
etmiþiz. Ýkinci olay bilmediðimiz parametrenin belli bir deðere sahip
olmasý olacak. O zaman Bayes Teorisinin þu þekilde tekrar yazabiliriz, 

$$ P(parametre \mid veri ) \propto P(veri \mid parametre)P(parametre) $$

$\propto$ iþareti orantýlý olmak (proportional to) anlamýna geliyor. Böleni
attýk çünkü o bir sabit (tamamen veriye baðlý, tahmini hesaplamak
istediðimiz parametreye baðlý deðil). Tabii bu durumda sol ve sað taraf
birbirine eþit olmaz, o yüzden eþitlik yerine orantýlý olmak iþaretini
kullandýk. Bu çerçevede "belli bir sayýsal sabit çerçevesinde birbirine
eþit (equal within a numeric constant)" gibi cümleler de görülebilir. 

Örnek

Diyelim ki bir bozuk para ile 10 kere yazý-tura attýk, ve sonuç altta

T H H H H T T H H H

Bu veriye bakarak paranýn hileli olup olmadýðýný anlamaya
çalýþacaðýz. Bayes ifadesini bu veriye göre yazalým,

$$ P(p | \{ \textrm{T H H H H T T H H H} \} \propto 
P(\{ \textrm{T H H H H T T H H H} | p) P(p) \}
$$

$P(p)$ ifadesi ne anlama gelir? Aslýnda bu ifadeyi $P([Dagilim] = p)$ olarak
görmek daha iyi, artýk $p$ parametresini bir daðýlýmdan gelen bir özgün deðer
olarak gördüðümüze göre, o daðýlýmýn belli bir $p$'ye eþit olduðu zamaný
modelliyoruz burada. Her halükarda $P(p)$ daðýlýmýný, yani onsel (prior)
olasýlýðý bilmiyoruz, hesaptan önce her deðerin mümkün olduðunu biliyoruz, o
zaman bu onsel daðýlýmý düz (flat) olarak alýrýz, yani $P(p) = 1$.

$P(\{\textrm{T H H H H T T H H H} | p)$ ifadesi göz korkutucu olabilir, ama
buradaki her öðenin baðýmsýz özdeþçe daðýlmýþ (independent identically
distributed) olduðunu görürsek, ama bu ifadeyi ayrý ayrý
$P(\{\textrm{T}|p)$ ve $P(\{\textrm{H}|p)$ çarpýmlarý olarak
görebiliriz. $P(\{\textrm{T}|p) = p$ ve $P(\{\textrm{H}|p)=1-p$ olduðunu
biliyoruz. O zaman

$$ P(p | \{ \textrm{7 Tura, 3 Yazý} \} \propto
p^7(1-p)^3
$$

Grafiklersek, 

\includegraphics[height=6cm]{stat_appendix_01.png}

Böylece $p$ için bir sonsal daðýlým elde ettik. Artýk bu daðýlýmýn yüzde 95
aðýrlýðýnýn nerede olduðunu rahatça görebiliriz /
hesaplayabiliriz. Daðýlýmýn tepe noktasýnýn $p=0.7$ civarýnda olduðu
görülüyor. Bir daðýlýmla daha fazlasýný yapmak ta mümkün, mesela bu
fonksiyonu $p$'ye baðlý baþka bir fonksiyona karþý entegre etmek mümkün,
mesela beklentiyi bu þekilde hesaplayabiliriz.

Onsel daðýlýmýn her noktaya eþit aðýrlýk veren birörnek (uniform) seçilmiþ
olmasý, yani problemi çözmeye sýfýr bilgiden baþlamýþ olmamýz, yöntemin bir
zayýflýðý olarak görülmemeli. Yöntemin kuvveti elimizdeki bilgiyle baþlayýp
onu net bir þekilde veri ve olurluk üzerinden sonsal tek daðýlýma
götürebilmesi. Baþlangýç ve sonuç arasýndaki baðlantý gayet net. Fazlasý da
var; ilgilendiðimiz alaný (domain) öðrendikçe, baþta hiç bilmediðimiz onsel
daðýlýmý daha net, bilgili bir þekilde seçebiliriz ve bu sonsal daðýlýmý da
daha olmasý gereken modele daha yaklaþtýrabilir. 

\newpage

Çok Boyutlu Gaussian'ý Parçalamak (Partitioning)

Diyelim ki Normal bir vektör $X$'i $X = (X_1,X_2)$ olarak parçaladýk. Bunu
Gaussian'a etkileri ne olur? Ayný þekilde $\mu = (\mu_1,\mu_2)$ olarak
parçalayabiliriz. $\Sigma$ ise

$$ \Sigma = 
\left[\begin{array}{rr}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]
 $$

olarak parçalanabilir. $a,b$'nin parçalarýnýn boyutlarý $p,q$ olsun, $n = p+q$.

Þimdi birleþik Gaussian'ý 

$$ f(x;\mu,\Sigma) = 
\frac{ 1}{(2\pi)^{(p+q)/2} \det(\Sigma)^{1/2}} 
\exp 
\bigg\{ 
-\frac{ 1}{2}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]^T
\left[\begin{array}{rr}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]^{-1}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]
\bigg\}
 $$

Birleþik yoðunluðu parçalar üzerinden belirtirsek, bu yoðunluðu $X_2$ için
bileþen yoðunluða ve $X_1$ için bir koþullu yoðunluða ayýrabiliriz. Yani 

$$ f(x_1,x_2) = f(x_1|x_2) f(x_2) $$

tanýmýndaki parçalarý elde etmeye çalýþacaðýz.  Ama bundan önce
bölüntülenmiþ matrislere yakýndan bakalým. 

Bir bölüntülenmiþ (partitioned) matrisin tersini almak için, o matrisin
parçalarýnýn tersini almak doðru deðildir, yani

$$ 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] ^{-1} \ne
\left[\begin{array}{rr}
E^{-1} & F ^{-1}\\
G^{-1} & H^{-1}
\end{array}\right]  
 $$

Tersini alma iþlemi için bazý numaralar lazým. Ana numara bölüntülenmiþ matrisi 
köþegen bir matris haline getirmek, çünkü köþegen matrislerin tersi,
köþegendeki elemanlarýn tersidir, yani ters alma operasyonu bu tür
matrislerin ``içine iþler'', o yüzden bir þekilde bir köþegen matris
elde etmeye uðraþacaðýz. Bunun için bölüntülenmiþ matrisimizi saðdan ve
soldan bazý matrislerle çarpacaðýz. Ayrýca þunu da bilelim, 

$$ XYZ = W $$

durumunda $Y$'nin tersini almak istersek, sað ve soldaki $X,Z$
matrislerinin tersini almak gerekmez, niye?

$$ X^{-1}XYZ = X^{-1}W $$

$$ YZZ^{-1} = X^{-1}WZ^{-1} $$

$$ Y = X^{-1}WZ^{-1} $$

Þimdi iki tarafýn da tersini alalým, 

$$ Y^{-1} = ZW^{-1}X $$

Tamam, baþlayalým. 

$$ M = 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] 
 $$

matrisini köþegen yapacaðýz. Eðer sadece alt sol köþeyi sýfýrlayasaydýk, 
bunu yapacak özel bir matrisle soldan çarpardýk,

$$ 
\left[\begin{array}{rr}
I & -FH^{-1} \\
0 & I
\end{array}\right] 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] = 
\left[\begin{array}{rr}
E & F \\
0 & H
\end{array}\right] 
 $$

Sadece üst sað köþeyi sýfýrlamak isteseydik, saðdan çarpardýk

$$ 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] 
\left[\begin{array}{rr}
I & 0 \\
-H^{-1}G & I
\end{array}\right] 
=
\left[\begin{array}{rr}
E & 0 \\
G & H
\end{array}\right] 
 $$

Hepsini biraraya koyalým, 

$$ 
\left[\begin{array}{rr}
I & -FH^{-1} \\
0 & I
\end{array}\right] 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] 
\left[\begin{array}{rr}
I & 0 \\
-H^{-1}G & I
\end{array}\right] 
= 
\left[\begin{array}{rr}
E-FH^{-1}G & 0 \\
0 & H
\end{array}\right] 
\mlabel{2}
 $$

Bu çarpýmýn doðruluðu çarpým elle yapýlarak kontrol edilebilir.

Üstte gördüðümüz gibi 

$$ XYZ = W $$

ifadesindeki $Y$'nin tersi 

$$ Y^{-1} = ZW^{-1}X $$

ile olur. 

$$ 
\underbrace{
\left[\begin{array}{rr}
I & -FH^{-1} \\
0 & I
\end{array}\right] 
}_{X}
\underbrace{
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right] 
}_{Y}
\underbrace{
\left[\begin{array}{rr}
I & 0 \\
-H^{-1}G & I
\end{array}\right] 
}_{Z}
= 
\underbrace{
\left[\begin{array}{rr}
E-FH^{-1}G & 0 \\
0 & H
\end{array}\right] 
}_{W}
 $$

O zaman 

$$ 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right]^{-1}
=
\left[\begin{array}{rr}
I & 0 \\
-H^{-1}G & I
\end{array}\right] 
\left[\begin{array}{rr}
E-FH^{-1}G & 0 \\
0 & H
\end{array}\right]^{-1}
\left[\begin{array}{rr}
I & -FH^{-1} \\
0 & I
\end{array}\right] 
 $$

Daha kýsa olmasý eþitliðin sað tarafýnda, ortadaki matris için
$E-FH^{-1}G$ yerine $M/H$ kullanalým (bu arada $M/H$ lineer cebirde ``$M$'in
$H$'e göre Schur tamamlayýcýsý (complement)'' olarak bilinir),

$$ 
\left[\begin{array}{rr}
E & F \\
G & H
\end{array}\right]^{-1}
=
\left[\begin{array}{rr}
I & 0 \\
-H^{-1}G & I
\end{array}\right] 
\left[\begin{array}{rr}
(M/H)^{-1} & 0 \\
0 & H^{-1}
\end{array}\right]
\left[\begin{array}{rr}
I & -FH^{-1} \\
0 & I
\end{array}\right] 
\mlabel{3}
 $$

Eþitliðin sað tarafýndaki çarpýmý gerçekleþtirirsek, 

$$ =
\left[\begin{array}{rr}
(M/H)^{-1} & -(M/H)^{-1}FH^{-1} \\
-H^{-1}G(M/H)^{-1} & H^{-1}+H^{-1}G(M/H)^{-1}FH^{-1} 
\end{array}\right]
 $$

Bu final ifade bölüntülenmiþ bir matrisin tersini o matrisin içindeki parçalar
üzerinden temsil eden bir ifadedir. 

Ýçinde bir köþesi sýfýr olan bölüntülenmiþ matrislerde determinantlar þöyle
iþler,

$$ 
\det \bigg(
\left[\begin{array}{rr}
E & 0 \\
G & H
\end{array}\right]
\bigg) 
= 
\det \bigg(
\left[\begin{array}{rr}
E & F \\
0 & H
\end{array}\right] 
\bigg) =
\det(E)\det(H)
 $$

Ayrýca 

$$ \det(AB) = \det(A)\det(B) $$

O zaman (2)'nin determinantýný alýrsak, $\det$ yerine $||$ kullandýk, 

$$ |M| = |M/H||H| 
\mlabel{4}
$$

Bu ifade gayet doðal duruyor (bir raslantý herhalde, ya da Schur tamamlayýcýsý 
iþareti özellikle böyle seçilmiþ),

Bölüntülenmiþ bir matrisin devriðini almak için her bloðunun ayrý ayrý devriði
alýnýr, ve tüm bloklarýn yaný bölüntülenmiþ tamamýnýn bir daha devriði alýnýr,
yani

$$ 
\left[\begin{array}{rr}
A & B \\ C & D 
\end{array}\right]^T = 
\left[\begin{array}{rr}
A^T & C^T \\ B^T & D^T
\end{array}\right]
 $$

Þimdi çok deðiþkenli Normal için bileþen ve koþullu yoðunluk hesaplarýna
gelelim. Gaussian formülünün $\exp$ kýsmýný alýrsak, 

$$ \exp 
\bigg\{ 
-\frac{ 1}{2}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]^T
\left[\begin{array}{rr}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]^{-1}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]
\bigg\}
 $$

(3)'teki açýlýmý kullanýrsak, ve $E = \Sigma_{11},F=\Sigma_{12},..$ olacak þekilde,

$$ \exp 
\bigg\{ 
-\frac{ 1}{2}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]^T
\left[\begin{array}{rr}
I & 0 \\ 
-\Sigma_{22}^{-1}\Sigma_{21} & I
\end{array}\right]
\left[\begin{array}{rr}
(\Sigma/\Sigma_{22}) & 0 \\ 
0 & \Sigma_{22}^{-1} 
\end{array}\right]
\left[\begin{array}{rr}
I & -\Sigma_{12}\Sigma_{22}^{-1}  \\ 
0 & I
\end{array}\right]
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]
\bigg\}
 $$

Açýlýmý tamamen yaparsak, 

$$ 
 \begin{array}{lll}
= && \exp \bigg\{
-\frac{1 }{2} 
(x_1 - \mu_1 - \Sigma_{12}\Sigma_{22}^{-1} (x_2 - \mu_2))^T 
(\Sigma/\Sigma_{22})^{-1} 
(x_1 - \mu_1 - \Sigma_{12}\Sigma_{22}^{-1} (x_2 - \mu_2))
\bigg\} \cdot \\
&& \exp \bigg\{
1\frac{ 1}{2}(x_2-\mu_2)^T\Sigma_{22}^{-1} (x_2-\mu_2)
 \bigg\}
\end{array}
 $$

Not: $\Sigma_{12}^T = \Sigma_{21}$. Üstte birinci $\exp$ içinde sol bölümde devriðin içindeki ifadelerden,
mesela $x_1^T,\mu_1^T$'den ve $\Sigma_{21}$'li ifadeden devrik iþlemini çekip, büyük paranteze 
alýnýnca bu deðiþim oldu. 

Þimdi mesela 1. $\exp$'ye dikkat edersek, ortada $(\Sigma/\Sigma_{22})^{-1} $ var, ve bu ifadenin solunda ve saðýnda 
birbirinin devriði olan ayný terimler duruyor. Ýfadenin tamamý bir Normal
daðýlým. Ayný þey 2. $\exp$ için geçerli. 

Ýþin $\exp$ tarafýný halletik. Þimdi $\exp$ öncesindeki kesiri (4) kullanarak
parçalayalým, 

$$ 
\frac{ 1}{(2\pi)^{(p+q)/2} \det(\Sigma)^{1/2}}  = 
\frac{ 1}{(2\pi)^{(p+q)/2} \bigg(\det(\Sigma/\Sigma_{22})\det(\Sigma_{22})\bigg)^{1/2}} 
 $$

$$ =
\bigg( \frac{ 1}{(2\pi)^{p/2} \det(\Sigma/\Sigma_{22})^{1/2}} \bigg)
\bigg( \frac{ 1}{(2\pi)^{q/2} \det(\Sigma_{22})^{1/2}} \bigg)
 $$

Bu parçalarýn her birini ayrý bir $\exp$ önünde kullanabiliriz, ve ikinci $\exp$
ifadesinin 

$$ 
\frac{ 1}{(2\pi)^{q/2} \det(\Sigma_{22})^{1/2}}
\exp \bigg\{
\frac{ 1}{2}(x_2-\mu_2)^T\Sigma_{22}^{-1} (x_2-\mu_2)
 \bigg\}
 $$


olduðunu görüyoruz. Bu ifade $f(x_2)$ bileþen yoðunluðudur! O zaman geri
kalanlar, yani diðer kesir ve birinci $\exp$ hep beraber $f(x_1|x_2)$
yoðunluðu olmalýdýr. Yani,

$$ 
\frac{ 1}{(2\pi)^{p/2} \det(\Sigma/\Sigma_{22})^{1/2}} \cdot
 $$
$$ 
\exp \bigg\{
-\frac{1 }{2} 
(x_1 - \mu_1 - \Sigma_{12}\Sigma_{22}^{-1} (x_2 - \mu_2))^T 
(\Sigma/\Sigma_{22})^{-1} 
(x_1 - \mu_1 - \Sigma_{12}\Sigma_{22}^{-1} (x_2 - \mu_2))
\bigg\}
 $$

Buradan genel bir kural çýkartabiliriz, 

1) $X_2$'nin bileþen yoðunluðu $X_2 \sim N(\mu_2, \Sigma_{22})$

2) $X_2 = x_2$ olmak üzere $X_1$'in koþullu daðýlýmý 

$$ X_1 | X_2 = x_2 \sim 
N\bigg(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1} (x_2 -\mu_2) \ , \
\Sigma/\Sigma_{22} \bigg)
 $$

$\Sigma/\Sigma_{22}$ nedir? Hatýrlarsak, $M/H = E-FH^{-1}G$, ve 
$E = \Sigma_{11},F=\Sigma_{12},..$ o zaman 

$$ \Sigma/\Sigma_{22} = \Sigma_{11}-\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} $$

Yani

$$ X_1 | X_2 = x_2 \sim 
N\bigg(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1} (x_2 -\mu_2) \ , \
\Sigma_{11}-\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\bigg)
 $$ 

\newpage

Moment 

Olasýlýk matematiðinde "moment üreten iþlevler" olarak adlandýrýlan,
baþlangýçta pek yararlý gibi gözükmesede bir takým matematiksel
özellikleri olduðu için, ispatlarda oldukça iþe yarayan bir kavram
vardýr.

Her rasgele deðiþkenin bir daðýlýmý olduðunu biliyoruz. Her rasgele
deðiþkenin de ayrýca bir moment üreten fonksiyonu da vardýr. Ayrýca,
moment üreten fonksiyon ile rasgele deðiþken arasýnda bire-bir olarak
bir iliþki mevcuttur. "Bu neye yarar?" diye sorulabilir; Cevap olarak,
mesela cebirsel olarak türete türete bir moment'e geldiðimiz
düþünelim, ve tekrar baþka bir taraftan, baþka bir formülden gene
türete türete tekrar ayný moment iþlevine geliyorsak, bu demektir ki,
iki taraftan gelen rasgele deðiþkenler (ve tekabül eden daðýlýmlarý)
birbirine eþittir. Bazý þartlarda moment üreten iþlevler ile cebir
yapmak, daðýlým fonksiyonlarýndan daha rahat olmaktadýr.

Her rasgele deðiþken için, moment üreten iþlev þöyle bulunur.

$X$ rasgele degiskenin moment ureten operasyonu

$M(t)=E(e^{tX})$ olarak gösterilir

Ayrýksal operasyonlar için

$$ M(t) = \sum_x e^{tx}p(x) $$

Sürekli iþlevler için

$$ M(t) = \int_{-\infty}^{\infty} e^{tx}f(x) \ud x   $$

Kuram

Gelelim yazýmýzýn esas konusu olan kuramýmýza.

Eðer $X_1, X_2...X_n$ baðýmsýz rasgele deðiþken ise, ve her deðiþkenin
$M_i(t)$ $i=1,2,3,...n$ olarak, öz olarak ayný olan birer moment üreten
iþlevi var ise, o zaman,

$$ Y = \sum_{i=1}^n  aX_i $$

açýlýmý

$$ M_y(t) = \prod_{i=1}^n M(a_i t) $$

olacaktýr. 

Ýspat

$$ M_y(t) = E(e^{tY}=E(e^{t(a_1X_1+a_2X_2+..+a_nX_n)} $$

$$ = E[\exp(ta_1 X_1 ta_2X_2...+ta_nX_n)] $$

$$ = E[\exp(ta_1X_1)+\exp(ta_2X_2)+ ... + \exp(ta_nX_n)] $$

$$ = E[\exp(ta_1X_1)]+E[\exp(ta_2X_2)]+ ... + E[\exp(ta_nX_n)]$$

Daha önce belirttiðimiz gibi

$$ M_i(t) = E[\exp(tX_i)] $$

olduðuna göre ve $t$ yerine $ta_i$ koyulduðunu düþünelim

$$ M_y(t) = \prod_{i=1}^n M_y(a_it) $$

olacaktýr. 

Bunu $M_y(t)= (M_i(a_it))^n$ þeklinde de gösterebiliriz. 

\newpage

Çebiþev Eþitsizliði

Olasýlýk matematiðinde, büyük sayýlar kuramý adýnda anýlan ve olasýlýk
matematiðinin belkemiðini oluþturan kuramý ispatlamak için, diðer bir kuram
olan Çebiþev eþitsizliðini de anlamamýz gerekiyor. Çebiþev eþitsizliði bir
rasgele deðiþken, onun ortalamasý (beklentisi) ve herhangi bir sabit sayý
arasýndaki üçlü arasýnda bir 'eþitsizlik' baðlantýsý kurar, ve bu baðlantý
diðer olasýlýk iþlemlerimizde ispat verisi olarak iþimize yarar.

Teori: Herhangi bir $t$ deðeri için, 

$$ P(|X-\mu| > t) \le \frac{\sigma^2}{t^2} $$

Ýspata baþlayalým. Entegral ile olasýlýk hesabý yapmak için bize bir $x$
uzayý lazým.

$$ \mathbb{R} = {x: |x-\mu| > t} $$

Yani $\mathbb{R}$ uzayý, $x$ ile ortalamasýnýn farkýnýn, $t$'den büyük olduðu bütün
sayýlarýn kümesidir.

O zaman, 

$$ P(|X-\mu| > t) = \int_R f(x) \ud x $$

Dikkat edelim $P(..)$ içindeki formül, küme tanýmý ile ayný. O yüzden $P()$
hesabý ortada daha olmayan, ama varolduðu kesin bir daðýlým fonksiyonu
tanýmlamýþ da oluyor. Buna $f(x)$ deriz. $P()$'in, $f(x)$ fonksiyonunun $R$
üzerinden entegral olduðunu olasýlýða giriþ dersinden bilmemiz lazým. 

Eger $x \in R$ dersek o zaman

$$ \frac{|x-\mu|^2}{t^2} \ge 1 $$

t'nin denkleme bu þekilde nereden geldiði þaþkýnlýk yaratabilir. Daha önce
tanýmlanan þu ibareye dikkat edelim, $x: |x-u| > t$ diye belirtmiþtik. Bu
ifadeyi deðiþtirerek, yukarýdaki denkleme gelebiliriz.

Devam edersek, elimizdeki 1'den büyük bir deðer var. Bu deðeri kullanarak,
aþaðýdaki tanýmý yapmamýz doðru olacaktýr.

$$
\int_R f(x) \ud x \le \int_R \frac{(x-\mu)^2}{t^2}f(x) \ud x \le
\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x) \ud x 
$$

Ortadaki entegral niye birinci entegralden büyük? Çünkü orta entegraldeki
$f(x)dx$ ibaresinden önce gelen kýsmýn, her zaman 1'den büyük olacaðýný
belirttiðimize göre, ikinci entegralin birinciden büyük olmasý normaldir,
çünkü birinci entegral $f(x)$ olasýlýk daðýlýmýna baðlý, entegral ise bir
alan hesabýdýr ve olasýlýk daðýlýmlarýnýn sonsuzlar arasýndaki entegrali
her zaman 1 çýkar, kaldý ki üstteki $x$'in uzayýný daha da daralttýk.

Evet...Üçüncü entegral ispata oldukça yaklaþtý aslýnda. Standart sapma
iþaretini hala ortada göremiyoruz, fakat son entegraldeki ibare standart
sapma deðerini zaten içeriyor. Önce daha önceki olasýlýk natematiði
bilgimize dayanarak, standart sapmanýn tanýmýný yazýyoruz. Dikkat edelim,
bu ibare þu anki ispatýmýz dahilinden deðil, haricinden önceki bilgimize
dayanarak geldi. Standart sapmanýn tanýmý þöyledir.

$$ \sigma^2 = \int_{-\infty}^{\infty} (x-\mu)^2f(x) \ud x $$

O zaman

$$
\frac{\sigma^2}{t^2}
= \int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)\ud x
$$

yani

$$
\int_R f(x) \ud x \le \frac{\sigma^2}{t^2} = 
\int_{-\infty}^{\infty} \frac{(x-\mu)^2}{t^2}f(x) \ud x
$$

ki $\int_R f(x) \ud x$ zaten $P(|X-\mu| > t)$ olarak tanimlanmisti. 

$\square$

\newpage

Çok Deðiþkenli Normal Numaralarý  (Multivariate Normal Tricks)

Çok deðiþkenli normal daðýlýmlarla iþ yaparken, mesela Gaussian karýþýmlarý
kullanýrken, bazý numaralarý bilmek faydalý olabiliyor. Bunlardan birincisi
$(x-\mu)^T\Sigma^{-1}(x-\mu)$ hesabýný yapmaktýr, diðer log-toplam-exp
numarasý (logsumexp trick) diye bilinen hesaptýr.

Birinciden baþlayalým, daha kýsalaþtýrmak için $y=x-\mu$ diyelim, yani
$y^T\Sigma^{-1}y$ olsun. Þimdi bu formülde bir ters alma (inversion)
iþleminin olduðunu görüyoruz. Fakat bu iþlem oldukça pahalý bir iþlem
olarak bilinir, hele hele boyutlarýn yükseldiði durumlardan (binler,
onbinler), kovaryansý temsil eden $\Sigma$, $n \times n$ olacaktýr. Acaba
tersini almayý baþka bir þekilde gerçekleþtiremez miyiz?

$\Sigma$ matrisi bir kovaryans matrisi olduðu için simetrik, pozitif yarý
kesin bir matristir. Bu tür matrislerin Cholesky ayrýþtýrmasýnýn olduðunu
biliyoruz ve bu iþlem çok hýzlý yapýlabiliyor. O zaman 

$$ \Sigma = LL^T $$

ki $L$ matrisi alt-üçgensel (lower triangular) bir matristir,

$$ \Sigma^{-1} = (LL^T)^{-1} $$

$$ = L^{-T}L^{-1} $$

Bunu temel alarak iki taraftan $y$'leri geri koyalým,

$$ y^T\Sigma^{-1}y= y^TL^{-T}L^{-1}y $$

Bilindiði gibi lineer cebirde istediðimiz yere parantez koyabiliriz,

$$ = (y^TL^{-T})L^{-1}y $$

Parantezden bir þeyin devriði gibi temsil edersek, parantez içindekilerin
sýrasý deðiþir ve tek tek devriði alýnýr,

$$ = (L^{-1}y)^TL^{-1}y $$

$$  = |L^{-1}y|^2 $$

Üstteki ifadede $|\cdot|$ içindeki kýsým $Ax=b$ durumundaki $x$'in en az
kareler çözümü olan $A^{-1}b$'ye benzemiyor mu? Evet. Gerçi $n \times n$
boyutunda bir matris olduðu için elimizde ``bilinmeyenden fazla denklem''
yok, yani bu sistem artýk belirtilmiþ (overdetermined) deðil, yani en az
kareler deðil direk lineer sistem çözümü yapýyoruz. Bu durumda her standart
lineer cebir kütüphanesinde mevcut bir çaðrý kullanacaðýz, mesela
\verb!solve_triangular! (ve lower -alt- doðru seçeneðini kullanacaðýz), ki
bu çaðrý özellikle alt üçgensel matris üzerinden çözüm yapmaktadýr, çünkü
$L$ alt-üçgensel olduðu için çözüm geriye deðer koymak (back substitution)
ile anýnda bulunabilir. Geriye deðer koymayý hatýrlarsak, mesela

$$ 
\left[\begin{array}{cc}
2 & 0 \\
3 & 4
\end{array}\right]
\left[\begin{array}{c}
x_1\\
x_2
\end{array}\right]
= 
\left[\begin{array}{c}
6\\
8
\end{array}\right]
 $$

En üst satýrda her zaman tek bir bilinmeyen olacak, çünkü matris alt üçgensel,
en üst satýr her zaman en boþ satýrdýr. Bu tek bir eþitlik
demektir, yani $2x_1 = 6$, ki $x_1 = 3$. Bunu alýp bir sonraki satýra gideriz,
artýk $x_1$'i biliyoruz, sonraki satýrda sadece $x_2$ bilinmeyen
kalýyor, $3\cdot x_1 + 4 \cdot x_2 = 8$, yani $x_2 = -1/4$. Sonuca
ulaþtýk. Daha fazla boyut olsaydý durum deðiþmezdi, ayný iþlem daha fazla
tekrarlanýrdý. Bu arada bu türden bir çözümün ne kadar hýzlý olacaðýný
belirtmemize gerek yok herhalde.

Demek ki $y^T\Sigma^{-1}y$ hesabý için önce $\Sigma$ üzerinde Cholesky
alýyoruz, sonra $L^{-1}y$ çözdürüyoruz. Elde edilen deðerin noktasal
çarpýmýný alýnca $\Sigma$'nin tersini elde etmiþ olacaðýz. 

Örnek (önce uzun yoldan),

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
Sigma = np.array([[10., 2.],[2., 5.]])
y = np.array([[1.],[2.]])
print np.dot(np.dot(y.T,lin.inv(Sigma)),y)
\end{minted}

\begin{verbatim}
[[ 0.80434783]]
\end{verbatim}

Þimdi Cholesky ve \verb!solve_triangular! üzerinden

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as slin
L = lin.cholesky(Sigma)
x = slin.solve_triangular(L,y,lower=True)
print np.dot(x.T,x)
\end{minted}

\begin{verbatim}
[[ 0.80434783]]
\end{verbatim}

Ayný sonuca eriþtik.

\newpage

log-toplam-exp (log-sum-exp trick)

Bu numaranýn ilk kýsmý nisbeten basit. Bazý yapay öðrenim algoritmalarý için
olasýlýk deðerlerinin birbiriyle çarpýlmasý gerekiyor, mesela 

$$ r = p_1 \cdot p_2 \dots p_n $$

Olasýlýklar 1'den küçük olduðu için 1'den küçük deðerlerin çarpýmý aþýrý
küçülebilir, ve küçüklüðün taþmasý (underflow) ortaya çýkabilir. Eðer
çarpým yerine $\log$ alýrsak, çarpýmlar toplama dönüþür, sonra sonucu
$\exp$ ile tersine çeviririz, ve $\log$'u alýnan deðerler çok küçülmez,
çarpma yernie toplama iþlemi kullanýldýðý için de nihai deðer de küçüklüðe
doðru taþmaz.

$$ \log r = \log p_1 + \log p_2 + \dots + \log p_n $$

$$ r = \exp(\log p_1 + \log p_2 + \dots + \log p_n )$$

Bir diðer durum içinde $exp$ ifadesi taþýyan bir olasýlýk deðerinin çok
küçük deðerler taþýyabilmesidir. Mesela çok deðiþkenli Gaussian karýþýmlarý
için alttaki gibi bir hesap sürekli yapýlýr, 

$$ = \sum_i w_i
\frac{ 1}{(2\pi)^{k/2} \det(\Sigma)^{1/2}} \exp 
\bigg\{ 
-\frac{ 1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
\bigg\}
 $$

ki $0 \le w_i \le 1$ þeklinde bir aðýrlýk deðeridir. Üstteki formülün
çoðunlukla $\log$'u alýnýr, ve, mesela bir örnek üzerinde görürsek (ve
aðýrlýklarý bir kenara býrakýrsak), 

$$ \log(e^{-1000} + e^{-1001}) $$ 

gibi hesaplar olabilir. Üstteki deðerler tamamen uyduruk denemez,
uygulamalarda pek çok kez karþýmýza çýkan deðerler bunlar. Her neyse, eðer
üstteki ifadeyi kodla hesaplarsak, 

\begin{minted}[fontsize=\footnotesize]{python}
print np.log(np.exp(-1000) + np.exp(-1001))
\end{minted}

\begin{verbatim}
-inf
\end{verbatim}

Bu durumdan kurtulmak için bir numara þudur; $\exp$ ifadeleri arasýnda en
büyük olanýný dýþarý çekeriz, ve $\log$'lar çarpýmý toplam yapar,

$$ \log(e^{-1000}(e^{0} + e^{-1} ))$$

$$ -1000 + \log(1 + e^{-1})$$

Bunu hesaplarsak, 

\begin{minted}[fontsize=\footnotesize]{python}
print -1000 + np.log(1+np.exp(-1))
\end{minted}

\begin{verbatim}
-999.686738312
\end{verbatim}

Bu numaranýn yaptýðý nedir? Maksimumu dýþarý çekerek en az bir deðerin
küçüklüðü taþmamasýný garantilemiþ oluyoruz. Ayrýca, bu þekilde, geri kalan
terimlerde de aþýrý ufalanlar terimler kalma þansý azalýyor. 

[1] Flannery, {\em Numerical Recipes, 3rd Edition}

[2] Tapaswi, {\em Log-Sum-Exp Trick}, \url{http://makarandtapaswi.wordpress.com/2012/07/18/log-sum-exp-trick/}

\newpage

z-Tablosu

Nasýl okunur? Z-deðeri -0.8994 için z kolonundan aþaðý inilir, ve -0.8
bulunur, x.x9xx yani 9 için .09 kolonuna gidilir ve bu kesiþmedeki deðer
okunur, .1867, yuvarlanarak .19 da kabul edilebilir. 

\includegraphics[height=4cm]{stat_appendix_02.png}

\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09

-3.4 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0002

-3.3 .0005 .0005 .0005 .0004 .0004 .0004 .0004 .0004 .0004 .0003

-3.2 .0007 .0007 .0006 .0006 .0006 .0006 .0006 .0005 .0005 .0005

-3.1 .0010 .0009 .0009 .0009 .0008 .0008 .0008 .0008 .0007 .0007

-3.0 .0013 .0013 .0013 .0012 .0012 .0011 .0011 .0011 .0010 .0010

-2.9 .0019 .0018 .0018 .0017 .0016 .0016 .0015 .0015 .0014 .0014

-2.8 .0026 .0025 .0024 .0023 .0023 .0022 .0021 .0021 .0020 .0019

-2.7 .0035 .0034 .0033 .0032 .0031 .0030 .0029 .0028 .0027 .0026

-2.6 .0047 .0045 .0044 .0043 .0041 .0040 .0039 .0038 .0037 .0036

-2.5 .0062 .0060 .0059 .0057 .0055 .0054 .0052 .0051 .0049 .0048

-2.4 .0082 .0080 .0078 .0075 .0073 .0071 .0069 .0068 .0066 .0064

-2.3 .0107 .0104 .0102 .0099 .0096 .0094 .0091 .0089 .0087 .0084

-2.2 .0139 .0136 .0132 .0129 .0125 .0122 .0119 .0116 .0113 .0110

-2.1 .0179 .0174 .0170 .0166 .0162 .0158 .0154 .0150 .0146 .0143

-2.0 .0228 .0222 .0217 .0212 .0207 .0202 .0197 .0192 .0188 .0183

-1.9 .0287 .0281 .0274 .0268 .0262 .0256 .0250 .0244 .0239 .0233

-1.8 .0359 .0351 .0344 .0336 .0329 .0322 .0314 .0307 .0301 .0294

-1.7 .0446 .0436 .0427 .0418 .0409 .0401 .0392 .0384 .0375 .0367

-1.6 .0548 .0537 .0526 .0516 .0505 .0495 .0485 .0475 .0465 .0455

-1.5 .0668 .0655 .0643 .0630 .0618 .0606 .0594 .0582 .0571 .0559

-1.4 .0808 .0793 .0778 .0764 .0749 .0735 .0721 .0708 .0694 .0681

-1.3 .0968 .0951 .0934 .0918 .0901 .0885 .0869 .0853 .0838 .0823

-1.2 .1151 .1131 .1112 .1093 .1075 .1056 .1038 .1020 .1003 .0985

-1.1 .1357 .1335 .1314 .1292 .1271 .1251 .1230 .1210 .1190 .1170

-1.0 .1587 .1562 .1539 .1515 .1492 .1469 .1446 .1423 .1401 .1379

-0.9 .1841 .1814 .1788 .1762 .1736 .1711 .1685 .1660 .1635 .1611

-0.8 .2119 .2090 .2061 .2033 .2005 .1977 .1949 .1922 .1894 .1867

-0.7 .2420 .2389 .2358 .2327 .2296 .2266 .2236 .2206 .2177 .2148

-0.6 .2743 .2709 .2676 .2643 .2611 .2578 .2546 .2514 .2483 .2451

-0.5 .3085 .3050 .3015 .2981 .2946 .2912 .2877 .2843 .2810 .2776

-0.4 .3446 .3409 .3372 .3336 .3300 .3264 .3228 .3192 .3156 .3121

-0.3 .3821 .3783 .3745 .3707 .3669 .3632 .3594 .3557 .3520 .3483

-0.2 .4207 .4168 .4129 .4090 .4052 .4013 .3974 .3936 .3897 .3859

-0.1 .4602 .4562 .4522 .4483 .4443 .4404 .4364 .4325 .4286 .4247

0.0 .5000 .4960 .4920 .4880 .4840 .4801 .4761 .4721 .4681 .4641

\newpage

\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09

0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359

0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753

0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141

0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517

0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879

0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224

0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549

0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852

0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133

0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389

1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621

1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830

1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015

1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177

1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319

1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441

1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545

1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633

1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706

1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767

2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817

2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857

2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890

2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916

2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936

2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952

2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964

2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974

2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981

2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986

3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990

3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993

3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995

3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997

3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998

Kaynaklar

[1] Gullickson, {\em Sociology G4075: Introduction to Social Data Analysis
  II}, \url{https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html}


\newpage

Yunan Harfleri

\newcommand{\X}[1]{$#1$ & \texttt{\string#1}}

\begin{tabular}{cccccccc}
 \X{\alpha}     & \X{\theta}     & \X{o}          & \X{\upsilon}  \\
 \X{\beta}      & \X{\vartheta}  & \X{\pi}        & \X{\phi}      \\
 \X{\gamma}     & \X{\iota}      & \X{\varpi}     & \X{\varphi}   \\
 \X{\delta}     & \X{\kappa}     & \X{\rho}       & \X{\chi}      \\
 \X{\epsilon}   & \X{\lambda}    & \X{\varrho}    & \X{\psi}      \\
 \X{\varepsilon}& \X{\mu}        & \X{\sigma}     & \X{\omega}    \\
 \X{\zeta}      & \X{\nu}        & \X{\varsigma}  &               \\
 \X{\eta}       & \X{\xi}        & \X{\tau} & \\
 \X{\Gamma}     & \X{\Lambda}    & \X{\Sigma}     & \X{\Psi}      \\
 \X{\Delta}     & \X{\Xi}        & \X{\Upsilon}   & \X{\Omega}    \\
 \X{\Theta}     & \X{\Pi}        & \X{\Phi} 
\end{tabular}

\end{document}



