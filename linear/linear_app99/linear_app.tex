\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Ekler

Matris Türevleri

Aksi belirtilmedikçe altta $a,x$ gibi vektörler kolon vektörleri olacaktýr,
yani $m \times 1$, ya da $n \times 1$ gibi boyutlara sahip olacaklardýr. 

$m$ boyutlu vektör $x$'i alan ve geriye tek sayý sonucu döndüren bir $f(x)$
fonksiyonunun $x$'e göre türevini nasýl alýrýz? Yani $x \in \mathbb{R}^m$
ve bir vektör,

$$ x = 
\left[\begin{array}{ccc}
x_1 \\ \vdots \\ x_m
\end{array}\right]
 $$

Bu durumda $x$'in her hücresine / öðesine göre kýsmi türevler (partial
derivatýves) alýnýr, sonuçta tek boyutlu / tek sayýlý fonksiyon, türev 
sonrasý $m$ boyutlu bir sonuç vektörünü yaratýr, yani

$$
\frac{\partial f}{\partial x}  =
\left[\begin{array}{c}
\frac{\partial f}{\partial x_1} \\
\\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_m} 
\end{array}\right]
$$

Bu sonuç tanýdýk gelmiþ olabilir, bu ifade gradyan olarak ta bilinir. 

$$ \frac{\partial f}{\partial x}  = \nabla f = grad \ f(x) $$

Türev bir kolon vektörü olarak çýktý çünkü $x$ de bir kolon
vektörüydü. Elde edilen vektör sürpriz deðil çünkü tek, skalar bir deðer
veren bir fonksiyonun $x$ içindeki {\em her öðesinin} nasýl deðiþtiðine
göre bunun fonksiyon üzerindeki etkilerini merak ediyorduk, üstteki vektör
öðe bazýnda bize aynen bunu gösteriyor. Yani tek skalar sonuç $m$ tane
türev sonucuna ayrýlýyor, çünkü tek sonucun $m$ tane seçeneðe göre
deðiþimini görmek istedik. 

Not olarak belirtelim, gradyan vektörü matematiksel bir kýsayoldur, yani
matematikte kuramsal olarak türetilerek ulaþýlan ana kurallardan biri
denemez. Fakat çok iþe yaradýðýna þüphe yok.

Eðer bir $A$ matrisinin tüm öðeleri tek sayý $\theta$ gibi bir deðiþkene
baðlý ise, o matrisin $\theta$'ya göre türevi, tüm elemanlarýnýn teker
teker $\theta$'ya göre türevidir,

$$ 
\frac{\partial A}{\partial \theta} = 
\left[\begin{array}{cccc}
\frac{\partial a_{11}}{\partial \theta} & 
\frac{\partial a_{12}}{\partial \theta} & \dots & 
\frac{\partial a_{1n}}{\partial \theta} \\

\frac{\partial a_{21}}{\partial \theta} & 
\frac{\partial a_{22}}{\partial \theta} &  \dots & 
\frac{\partial a_{2n}}{\partial \theta}  \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial a_{m1}}{\partial \theta} & 
\frac{\partial a_{m2}}{\partial \theta} &  \dots & 
\frac{\partial a_{mn}}{\partial \theta}  

\end{array}\right]
$$

Þimdi ilginç bir durum; diyelim ki hem fonksiyon $f(x)$'e verilen $x$
çok boyutlu, hem de fonksiyonun sonucu çok boyutlu! Bu gayet mümkün bir
durum. Bu durumda ne olurdu? 

Eðer $f$'in türevinin her türlü deðiþimi temsil etmesini istiyorsak, o
zaman hem her girdi hücresi, hem de her çýktý hücresi kombinasyonu için bu
deðiþimi saptamalýyýz. Jacobian matrisleri tam da bunu yapar. Eðer $m$
boyutlu girdi ve $n$ boyutlu çýktý tanýmlayan $f$'in türevini almak
istersek, bu bize $m \times n$ boyutunda bir matris verecektir!
Hatýrlarsak daha önce gradyan sadece $m$ boyutunda bir {\em vektör}
vermiþti. Þimdi sonuç bir matris. 

$$ 
J(x) = \frac{\partial f(x)}{\partial x} =
\left[\begin{array}{ccc}
\frac{\partial f_{1}(x)}{\partial \theta} & \dots & 
\frac{\partial f_{1}(x)}{\partial \theta} \\

\vdots & \ddots & \vdots \\

\frac{\partial f_{n}(x)}{\partial \theta} & \dots & 
\frac{\partial f_{n}(x)}{\partial \theta}  
\end{array}\right]
 $$

Vektörlere gelelim. $a,x \in \mathbb{R}^n$ ise, $a^Tx$'in $x$'e göre türevi nedir? 

$a^Tx$ bir noktasal çarpým olduðuna göre sonucu bir tek sayýdýr
(scalar). Bu noktasal çarpýmý bir fonksiyon gibi düþünebiliriz, bu durumda
demektir ki tek sayýlý bir fonksiyonun çok boyutlu $x$'e göre türevini
alýyoruz, yani gradyan durumu tekrar vuku buldu,

$$ 
\frac{\partial (a^Tx)}{\partial x} = 
\left[\begin{array}{c}
\frac{\partial (a^Tx)}{\partial x_1} \\ 
\vdots \\ 
\frac{\partial (a^Tx)}{\partial x_n} 
\end{array}\right] 
 $$

$$  =
\left[\begin{array}{c}
\frac{\partial (a_1x_1 + ... + a_nx_n)}{\partial x_1} \\ 
\vdots \\ 
\frac{\partial (a_1x_1 + ... + a_nx_n)}{\partial x_n} 
\end{array}\right] 
= 
\left[\begin{array}{c}
a_1 \\
\vdots  \\
a_n
\end{array}\right] =
a
 $$

Niye her satýrda $a_1,a_2$  gibi deðerler elde ettiðimizin sebebi bariz
herhalde, çünkü mesela ilk satýrda $x_1$'e göre türev alýndýðý durumda
$a_1x_1$ haricindeki tüm terimler yokolacaktýr, çünkü o terimler içinde
$x_1$ yoktur ve Calculus'a göre sabit sayý sayýlýrlar.

Peki $a^Tx$'in $x^T$'ye göre türevi nedir? 

$x^T$'nin yatay bir vektör olduðuna dikkat, yani satýr vektörü, o zaman
sonuç yatay bir vektör olacaktýr (kýyasla gradyan dikeydi).

$$ 
\frac{\partial (a^Tx)}{\partial x^T} = 
\left[\begin{array}{ccc}
\frac{\partial (a^Tx)}{\partial x_1} &
\dots 
&
\frac{\partial (a^Tx)}{\partial x_n} 
\end{array}\right] 
\mlabel{1}
 $$

$$ =
\left[\begin{array}{ccc}
a_1 & \dots & a_n
\end{array}\right] = 
a^T $$

Eðer bir $x \in \mathbb{R}^m$ vektöründen $A$ matrisi $x$ ile çarpýlýyor
ise, bu çarpýmýn $x$'e göre türevi nedir? 

$$ \frac{\partial(Ax)}{\partial x^T} = A
$$

Ýspat: Eðer $a_i \in \mathbb{R}^n$ ise (ki devriði alýnýnca bu vektör yatay hale
gelir, yani altta bu yatay vektörleri üst üste istiflediðimizi düþünüyoruz),

$$ A = \left[\begin{array}{c}
a_1^T \\ \vdots \\ a_m^T
\end{array}\right] $$

O zaman $Ax$ ne olur? {\em Matris Çarpýmý} yazýsýndaki ``satýr bakýþ açýsý''
düþünülürse, $A$'in her satýrý, ayrý ayrý $x$'in tüm satýrlarýný kombine
ederek tekabül eden sonuç satýrýný oluþturur (tabii bu örnekte $x$'in
kendisi bir vektör o yüzden ``satýrlarý'' tek bir sayýdan ibaret), 

$$ Ax = \left[\begin{array}{c}
a_1^Tx \\ \vdots \\ a_m^Tx
\end{array}\right] 
$$

Üstteki bir vektör, her öðesi tek sayý. Türevi alýrsak (dikkat yatay
vektöre göre türev alýyoruz), ve (1)'i dikkate alýrsak, 

$$ 
\frac{\partial Ax}{\partial x^T}  =
\left[\begin{array}{c}
\frac{\partial(a_1^Tx) }{\partial x^T} 
\\ 
\vdots \\ 
\frac{\partial(a_m^Tx) }{\partial }
\end{array}\right] = \left[\begin{array}{c}
a_1^T \\ \vdots \\ a_m^T
\end{array}\right]  = A
 $$

Þu türev nasýl hesaplanýr? 

$$ 
\frac{\partial (x^TA^T)}{\partial x}
$$

Çarpýmý açalým, $x$ devriði yatay bir vektör, $A$'nin satýrlarý $a_i$'ler
ise devrik sonrasý kolonlar haline gelirler, yani

$$ 
\left[\begin{array}{ccc}
x_1 & \dots & x_n
\end{array}\right]
\left[\begin{array}{ccc}
\uparrow & & \uparrow \\
a_1 & \dots & a_m \\
\downarrow & & \downarrow 
\end{array}\right]
 $$

Þimdi matris çarpýmý satýr bakýþýný kullanalým, çarpan $x$ satýrý bir
tane, o zaman sonuç tek satýr olacak. Bu tek $x$ satýrýnýn öðeleri, $A^T$
satýrlarýný teker teker çarpýp toplayacak ve o tek sonuç satýrýný meydana
getirecek. Yani $x_1,x_2,...$ sýrayla $a_1$'in tüm öðelerini çarpýyor,
ayný þekilde $a_2$ için aynýsý oluyor, vs. Bu durumu daha temiz bir
þekilde alttaki gibi belirtebiliriz,

$$ 
= 
\left(\begin{array}{ccc}
x^Ta_1 & \dots x^Ta_m 
\end{array}\right)
 $$

Bu ifadenin türevini almak çok kolay, 

$$ 
= 
\left(\begin{array}{ccc}
\frac{\partial (x^Ta_1)}{\partial x} &
\dots
\frac{\partial (x^Ta_m)}{\partial x} 
\end{array}\right)
 $$

$$ 
= 
\left(\begin{array}{ccc}
a_1 & \dots a_m 
\end{array}\right)
 $$

$$ \frac{\partial (x^TA^T)}{\partial x} = A^T \mlabel{2}$$

Baþka bir türev: Diyelim ki $x \in \mathbb{R}^n, z \in \mathbb{R}^m$. 
Yeni bir vektör $c = A^Tz$ tanýmlayalým ki vektör $n$ boyutunda. O zaman 


$$ 
\frac{\partial (z^TAx)}{\partial x} =  
\frac{\partial (c^Tx)}{\partial x} = c = 
A^Tz
$$

Diðer bir türev

$$ 
\frac{\partial (z^TAx)}{\partial z} =  
Ax
$$

Üstteki (2)'nin bir sonucu olarak görülebilir.

Eðer $x=x(\alpha),z=z(\alpha)$ olursa, türev almada Zincir Kuralýný kullanalým,

$$ 
\frac{\partial (z^TAx)}{\partial \alpha} =  
\frac{\partial (z^TAx)}{\partial x} \frac{\partial x}{\partial \alpha}  + 
\frac{\partial (z^TAx)}{\partial z} \frac{\partial z}{\partial \alpha}  
$$

$$ 
\frac{\partial (z^TAx)}{\partial \alpha} =  
A^Tz \frac{\partial x}{\partial \alpha}  + 
Ax \frac{\partial z}{\partial \alpha}  
$$

Eðer $z = x = \alpha$ dersek,

$$ 
\frac{\partial (x^TAx)}{\partial \alpha} =  
\frac{\partial (x^TAx)}{\partial x} =  
A^Tx \frac{\partial x}{\partial \alpha}  + 
Ax \frac{\partial x}{\partial \alpha}  
$$

$$ 
= Ax + A^Tx  = (A^T+A) x
$$

Diyelim ki $A$ simetrik bir matris, o zaman $A^T=A$, ve

$$ 
= (A^T+A)x = 2Ax
$$

$x^Tx$ ifadesinin $x$ vektörüne göre türevi nedir? En basit yol, $x^TAx$
kalýbýný kullanmak, ve $A = I$ yani birim matrisi koymak. Bu durumda 

$$ 
= \frac{\partial (x^TIx)}{\partial \alpha}  = 2Ix = 2x
 $$

Daha zor yoldan, bu bir noktasal çarpým olacaktýr, $x_1x_1 + x_2x_2 + .. +
 x_nx_n$  yani $x_1^2 + x_2^2 + .. + x_n^2$. Bu tek bir skalar sonuç, o sonucun her
$x$ öðesine göre türevinin alýnmasý,

$$ 
\left[\begin{array}{c}
\frac{\partial (x_1^2 + x_2^2 + .. + x_n^2)}{\partial x_1} \\
\dots \\
\frac{\partial (x_1^2 + x_2^2 + .. + x_n^2)}{\partial x_n} 
\end{array}\right]
 $$

$$ 
= \left[\begin{array}{c}
2x_1 \\
\dots \\
2x_n
\end{array}\right] = 
2x
 $$

Ýzler (Trace)

$$ \frac{\partial }{\partial A} Tr(A) = I $$

$$ \frac{\partial }{\partial A} Tr(AB) = B^T $$

$$ \frac{\partial }{\partial A} Tr(A^TB) = B $$


$$ \frac{\partial }{\partial A} Tr(ABA^T) = A(B + B^T) $$

Eðer $B$ simetrik ise üstteki þu hale getirilebilir

$$  = 2AB $$

\newpage

Uzaklýklar, Norm, Benzerlik

Literatürdeki anlatým norm ve uzaklýk konusu etrafýnda biraz kafa karýþýklýðý
yaratabiliyor, bu yazýda biraz açýklýk getirmeye çalýþalým. Norm bir büyüklük
ölçüsüdür. Vektör uzaylarý ile olan alakasýný görmek için {\em Fonksiyonel
  Analiz} notlarýna bakýlabilir. Büyüklük derken bir $x$ vektörünün
büyüklüðünden bahsediyoruz, ki bu çoðunlukla $||x||$ gibi bir kullanýmda
görülür, eðer altsimge yok ise, o zaman 2 kabul edilir, yani $||x||_2$. Bu ifade
bir L2 norm'unu ifade eder. $||x||_1$ varsa L1 norm'ü olurdu.

L1,L2 normalarý, ya da genel olarak $p$ üzerinden $L_p$ normlarý þöyle gösterilir,

$$ ||x||_p = (\sum_i |x_i|^p)^{1/p} $$

ki $x_i$, $x$ vektörü içindeki öðelerdir. Eðer $p=2$ ise, L2 norm

$$ ||x||_2 = \bigg(\sum_i |x_i|^2 \bigg)^{1/2} $$

Üstel olarak $1/2$'nin karekök demek olduðunu hatýrlayalým, yani 

$$ ||x||_2 = \sqrt{\sum |x_i|^2} $$

Bu norm ayrýca Öklitsel (Euclidian) norm olarak ta bilinir, tabii ki bunun
Öklitsel uzaklýk ile yakýn baðlantýsý var (iki vektörü birbirinden çýkartýp
Öklit normunu alýrsak Öklit uzaklýðýný hesaplamýþ oluruz).

Eðer $p=1$ olsaydý, yani L1 norm, o zaman üstel olarak $1/1$ olur, yani hiçbir
üstel / köksel iþlem yapýlmasýna gerek yoktur, iptal olurlar,

$$ ||x||_1 = \sum |x_i|^2 $$

Örnek

$$ 
a = \left[\begin{array}{r}
3 \\ -2 \\ 1
\end{array}\right]
 $$

$$ ||a|| = \sqrt{3^2+(-2)^2+1^2} = 3.742 $$

Örnekte altsimge yok, demek ki L2 norm. 

Ek Notasyon, Ýþlemler

L1 normu için yapýlan iþlemi düþünelim, vektör öðeleri kendileri ile
çarpýlýyor ve sonuçlar toplanýyor. Bu iþlem

$||x||_1 = x^Tx$

olarak ta gösterilemez mi? Ya da $x \cdot x$ olarak ki bu noktasal çarpýmdýr.

Bazen de yapay öðrenim literatüründe $||x||^2$ þekilde bir kullaným
görebiliyorsunuz. Burada neler oluyor? Altsimge yok, demek ki L2
norm. Sonra L2 normun karesi alýnmýþ, fakat L2 normu tanýmýna göre bir
karekök almýyor muydu? Evet, fakat o zaman kare iþlemi karekökü iptal eder,
demek ki L2 normunun karesini almak bizi L1 normuna döndürür! Eh bu normu
da $x^Tx$ olarak hesaplayabildiðimize göre hemen o notasyona geçebiliriz,
demek ki $||x||^2 = x^Tx = x \cdot x$. 

Ýkisel Vektörlerde Benzerlik

Diðer ilginç bir kullaným ikisel deðerler içeren iki vektör arasýnda
çakýþan 1 deðerlerinin toplamýný bulmak. Mesela 

\begin{minted}[fontsize=\footnotesize]{python}
a = np.array([1,0,0,1,0,0,1,1])
b = np.array([0,0,1,1,0,1,1,0])
\end{minted}

Bu iki vektör arasýndaki 1 uyusumunu bulmak için noktasal çarpým yeterli,
çünkü 1 ve 0, 0 ve 1, 0 ve 0 çarpýmý sýfýr verir, ama 1 çarpý 1 = 1
sonucunu verir. O zaman L1 norm bize ikisel iki vektör arasýnda kabaca bir
benzerlik fikri verebilir.

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(a,b)
\end{minted}

\begin{verbatim}
2
\end{verbatim}

Matris Normlarý

Vektörlerin norm'ü hesaplanabildiði gibi matris norm'ü da hesaplanabilir. Bir
$A$ matrisi için matris norm'ü

%
$$ || A || = \sup \{ ||Ax|| : x \in \mathbb{R}^n, ||x||=1 \textrm{ olacak þekilde } \} $$
%{{x.png}}

Bazen þöyle de gösterilir,

%
$$  || A || = \sup_{||x||=1} \{ ||Ax|| \} $$
%{{x.png}}

ya da

%
$$ || A || = \sup \{ \frac{||Ax||}{||x||} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde } \} $$
%{{x.png}}

Daha genel formda p-norm'u

%
$$ || A || = \sup
\bigg\{
\frac{||Ax||_p}{||x||_p} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde }
\bigg\} $$
%{{x.png}}

Özel durum $p=2$ için ki bu yine, vektörler için olduðu gibi, Öklitsel norm
olarak biliniyor. Bu durumda $A$'nýn normu $A$'nýn en büyük eþsiz
deðeridir. Yaklaþýk olarak hesaplama açýsýndan þunu da verelim,

%
$$ ||A||_1 = \max_{1 \le j \le n} \sum _{i=1}^{m} |a_{ij}| $$
%{{x.png}}

Yani tüm matris kolonlarýnýn hücrelerinin mutlak deðerleri toplanýyor, bu
toplamlar arasýnda en büyük sayýyý veren kolonun toplamý normun yaklaþýk
deðeridir.

\newpage

%
$$ (x-v)^TA(x-v) < 1 $$
%{{x.png}}

Üstteki formülde $x$ yerine $Px$ geçirirsek, ki $P$ herhangi bir matris,
eþitsizliðin sol tarafýna ne olur?

%
$$ (P(x-v))^T A (P(x-v))$$
%{{x.png}}

%
$$ (x-v)^T P^T A P (x-v)  $$
%{{x.png}}

Bu formüle bir þekilde ulaþmamýz lazým. Ama nasýl? Basitleþtirme amaçlý olarak
$w = x-v$ tanýmlayalým, ki $x \ne v$ olacak þekilde. $X = \frac{1}{||w||^2} I$
tanýmlayalým, bu bir köþegen matris, köþegeninde $1/||w||^2$ deðerleri var. Bu
sayede

%
$$ w^T A W < 1  \Rightarrow w^T A W < w^T X w  $$
%{{x.png}}

1 yerine üstteki en saðdaki terimi kullanmýþ olduk. Herhangi bir $x$ için
üstteki eþitsizlik her $w$ için doðru olacaktýr. Bu da $A - X$ negatif kesin
demektir (pozitif kesinliðin tersi), o zaman þunu da söyleyebiliriz,

%
$$ A - X < 0 \Rightarrow P^T(A-x)P < 0 \Rightarrow P^T AP < P^TXP $$
%{{x.png}}

Soldan ve saðdan $w^T,w$ ile çarparsak,

%
$$ w^T P^T AP w < w^T P^TXP w = \frac{1}{||w||^2} w^T P^T P w = (Pu)^T Pu$$
%{{x.png}}

ki $u = \frac{w}{||w||}$ $x-v$ yönünü gösteren birim vektördür. 

Þimdi matris normunun ne olduðunu hatýrlayalým,

%
$$ ||P|| = \sup_{||u||=1} || Pu || $$
%{{x.png}}

O zaman emin bir þekilde diyebiliriz ki 

%
$$ (x-v)^TA(x-v) < 1 \Rightarrow (x-v)^T P^T A P (x-v) < ||P||^2 $$
%{{x.png}}


\newpage

Kaynaklar

[1] Marmer, {\em Economics 627 Econometric Theory II, Vector and Matrix Differentiation}, \url{http://faculty.arts.ubc.ca/vmarmer/econ627/}
        
[2] Duda, Hart, {\em Pattern Classification}

[4] Bishop, {\em Pattern Recognition and Machine Learning}

[5] Wikipedia, {\em Matrix norm}, \url{https://en.wikipedia.org/wiki/Matrix_norm}

\newpage 

Yunan Harfleri

\newcommand{\X}[1]{$#1$ & \texttt{\string#1}}

\begin{tabular}{cccccccc}
 \X{\alpha}     & \X{\theta}     & \X{o}          & \X{\upsilon}  \\
 \X{\beta}      & \X{\vartheta}  & \X{\pi}        & \X{\phi}      \\
 \X{\gamma}     & \X{\iota}      & \X{\varpi}     & \X{\varphi}   \\
 \X{\delta}     & \X{\kappa}     & \X{\rho}       & \X{\chi}      \\
 \X{\epsilon}   & \X{\lambda}    & \X{\varrho}    & \X{\psi}      \\
 \X{\varepsilon}& \X{\mu}        & \X{\sigma}     & \X{\omega}    \\
 \X{\zeta}      & \X{\nu}        & \X{\varsigma}  &               \\
 \X{\eta}       & \X{\xi}        & \X{\tau} & \\
 \X{\Gamma}     & \X{\Lambda}    & \X{\Sigma}     & \X{\Psi}      \\
 \X{\Delta}     & \X{\Xi}        & \X{\Upsilon}   & \X{\Omega}    \\
 \X{\Theta}     & \X{\Pi}        & \X{\Phi} 
\end{tabular}


\end{document}
