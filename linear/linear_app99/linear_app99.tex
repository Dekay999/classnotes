\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Uzaklýklar, Norm, Benzerlik

Literatürdeki anlatým norm ve uzaklýk konusu etrafýnda biraz kafa karýþýklýðý
yaratabiliyor, bu yazýda biraz açýklýk getirmeye çalýþalým. Norm bir büyüklük
ölçüsüdür. Vektör uzaylarý ile olan alakasýný görmek için {\em Fonksiyonel
  Analiz} notlarýna bakýlabilir. Büyüklük derken bir $x$ vektörünün
büyüklüðünden bahsediyoruz, ki bu çoðunlukla $||x||$ gibi bir kullanýmda
görülür, eðer altsimge yok ise, o zaman 2 kabul edilir, yani $||x||_2$. Bu ifade
bir L2 norm'unu ifade eder. $||x||_1$ varsa L1 norm'ü olurdu.

L1,L2 normalarý, ya da genel olarak $p$ üzerinden $L_p$ normlarý þöyle gösterilir,

$$ ||x||_p = (\sum_i |x_i|^p)^{1/p} $$

ki $x_i$, $x$ vektörü içindeki öðelerdir. Eðer $p=2$ ise, L2 norm

$$ ||x||_2 = \bigg(\sum_i |x_i|^2 \bigg)^{1/2} $$

Üstel olarak $1/2$'nin karekök demek olduðunu hatýrlayalým, yani 

$$ ||x||_2 = \sqrt{\sum |x_i|^2} $$

Bu norm ayrýca Öklitsel (Euclidian) norm olarak ta bilinir, tabii ki bunun
Öklitsel uzaklýk ile yakýn baðlantýsý var (iki vektörü birbirinden çýkartýp
Öklit normunu alýrsak Öklit uzaklýðýný hesaplamýþ oluruz).

Eðer $p=1$ olsaydý, yani L1 norm, o zaman üstel olarak $1/1$ olur, yani hiçbir
üstel / köksel iþlem yapýlmasýna gerek yoktur, iptal olurlar,

$$ ||x||_1 = \sum |x_i|^2 $$

Örnek

$$ 
a = \left[\begin{array}{r}
3 \\ -2 \\ 1
\end{array}\right]
 $$

$$ ||a|| = \sqrt{3^2+(-2)^2+1^2} = 3.742 $$

Örnekte altsimge yok, demek ki L2 norm. 

Ek Notasyon, Ýþlemler

L1 normu için yapýlan iþlemi düþünelim, vektör öðeleri kendileri ile
çarpýlýyor ve sonuçlar toplanýyor. Bu iþlem

$||x||_1 = x^Tx$

olarak ta gösterilemez mi? Ya da $x \cdot x$ olarak ki bu noktasal çarpýmdýr.

Bazen de yapay öðrenim literatüründe $||x||^2$ þekilde bir kullaným
görebiliyorsunuz. Burada neler oluyor? Altsimge yok, demek ki L2
norm. Sonra L2 normun karesi alýnmýþ, fakat L2 normu tanýmýna göre bir
karekök almýyor muydu? Evet, fakat o zaman kare iþlemi karekökü iptal eder,
demek ki L2 normunun karesini almak bizi L1 normuna döndürür! Eh bu normu
da $x^Tx$ olarak hesaplayabildiðimize göre hemen o notasyona geçebiliriz,
demek ki $||x||^2 = x^Tx = x \cdot x$. 

Ýkisel Vektörlerde Benzerlik

Diðer ilginç bir kullaným ikisel deðerler içeren iki vektör arasýnda
çakýþan 1 deðerlerinin toplamýný bulmak. Mesela 

\begin{minted}[fontsize=\footnotesize]{python}
a = np.array([1,0,0,1,0,0,1,1])
b = np.array([0,0,1,1,0,1,1,0])
\end{minted}

Bu iki vektör arasýndaki 1 uyusumunu bulmak için noktasal çarpým yeterli,
çünkü 1 ve 0, 0 ve 1, 0 ve 0 çarpýmý sýfýr verir, ama 1 çarpý 1 = 1
sonucunu verir. O zaman L1 norm bize ikisel iki vektör arasýnda kabaca bir
benzerlik fikri verebilir.

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(a,b)
\end{minted}

\begin{verbatim}
2
\end{verbatim}

Matris Normlarý

Vektörlerin norm'ü hesaplanabildiði gibi matris norm'ü da hesaplanabilir. Bir
$A$ matrisi için matris norm'ü

%
$$ || A || = \sup \{ ||Ax|| : x \in \mathbb{R}^n, ||x||=1 \textrm{ olacak þekilde } \} $$
%{{x.png}}

Bazen þöyle de gösterilir,

%
$$  || A || = \sup_{||x||=1} \{ ||Ax|| \} $$
%{{x.png}}

ya da

%
$$ || A || = \sup \{ \frac{||Ax||}{||x||} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde } \} $$
%{{x.png}}

Daha genel formda p-norm'u

%
$$ || A || = \sup
\bigg\{
\frac{||Ax||_p}{||x||_p} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde }
\bigg\} $$
%{{x.png}}

Özel durum $p=2$ için ki bu yine, vektörler için olduðu gibi, Öklitsel norm
olarak biliniyor. Bu durumda $A$'nýn normu $A$'nýn en büyük eþsiz
deðeridir. Yaklaþýk olarak hesaplama açýsýndan þunu da verelim,

%
$$ ||A||_1 = \max_{1 \le j \le n} \sum _{i=1}^{m} |a_{ij}| $$
%{{x.png}}

Yani tüm matris kolonlarýnýn hücrelerinin mutlak deðerleri toplanýyor, bu
toplamlar arasýnda en büyük sayýyý veren kolonun toplamý normun yaklaþýk
deðeridir.

\newpage

%
$$ (x-v)^TA(x-v) < 1 $$
%{{x.png}}

Üstteki formülde $x$ yerine $Px$ geçirirsek, ki $P$ herhangi bir matris,
eþitsizliðin sol tarafýna ne olur?

%
$$ (P(x-v))^T A (P(x-v))$$
%{{x.png}}

%
$$ (x-v)^T P^T A P (x-v)  $$
%{{x.png}}

Bu formüle bir þekilde ulaþmamýz lazým. Ama nasýl? Basitleþtirme amaçlý olarak
$w = x-v$ tanýmlayalým, ki $x \ne v$ olacak þekilde. $X = \frac{1}{||w||^2} I$
tanýmlayalým, bu bir köþegen matris, köþegeninde $1/||w||^2$ deðerleri var. Bu
sayede

%
$$ w^T A W < 1  \Rightarrow w^T A W < w^T X w  $$
%{{x.png}}

1 yerine üstteki en saðdaki terimi kullanmýþ olduk. Herhangi bir $x$ için
üstteki eþitsizlik her $w$ için doðru olacaktýr. Bu da $A - X$ negatif kesin
demektir (pozitif kesinliðin tersi), o zaman þunu da söyleyebiliriz,

%
$$ A - X < 0 \Rightarrow P^T(A-x)P < 0 \Rightarrow P^T AP < P^TXP $$
%{{x.png}}

Soldan ve saðdan $w^T,w$ ile çarparsak,

%
$$ w^T P^T AP w < w^T P^TXP w = \frac{1}{||w||^2} w^T P^T P w = (Pu)^T Pu$$
%{{x.png}}

ki $u = \frac{w}{||w||}$ $x-v$ yönünü gösteren birim vektördür. 

Þimdi matris normunun ne olduðunu hatýrlayalým,

%
$$ ||P|| = \sup_{||u||=1} || Pu || $$
%{{x.png}}

O zaman emin bir þekilde diyebiliriz ki 

%
$$ (x-v)^TA(x-v) < 1 \Rightarrow (x-v)^T P^T A P (x-v) < ||P||^2 $$
%{{x.png}}


\newpage

Kaynaklar

[1] Marmer, {\em Economics 627 Econometric Theory II, Vector and Matrix Differentiation}, \url{http://faculty.arts.ubc.ca/vmarmer/econ627/}
        
[2] Duda, Hart, {\em Pattern Classification}

[4] Bishop, {\em Pattern Recognition and Machine Learning}

[5] Wikipedia, {\em Matrix norm}, \url{https://en.wikipedia.org/wiki/Matrix_norm}

\newpage 

Yunan Harfleri

\newcommand{\X}[1]{$#1$ & \texttt{\string#1}}

\begin{tabular}{cccccccc}
 \X{\alpha}     & \X{\theta}     & \X{o}          & \X{\upsilon}  \\
 \X{\beta}      & \X{\vartheta}  & \X{\pi}        & \X{\phi}      \\
 \X{\gamma}     & \X{\iota}      & \X{\varpi}     & \X{\varphi}   \\
 \X{\delta}     & \X{\kappa}     & \X{\rho}       & \X{\chi}      \\
 \X{\epsilon}   & \X{\lambda}    & \X{\varrho}    & \X{\psi}      \\
 \X{\varepsilon}& \X{\mu}        & \X{\sigma}     & \X{\omega}    \\
 \X{\zeta}      & \X{\nu}        & \X{\varsigma}  &               \\
 \X{\eta}       & \X{\xi}        & \X{\tau} & \\
 \X{\Gamma}     & \X{\Lambda}    & \X{\Sigma}     & \X{\Psi}      \\
 \X{\Delta}     & \X{\Xi}        & \X{\Upsilon}   & \X{\Omega}    \\
 \X{\Theta}     & \X{\Pi}        & \X{\Phi} 
\end{tabular}


\end{document}
