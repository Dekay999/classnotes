\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Ders 21

Bu özdeðer/vektörler hakkýndaki ilk dersimiz. Bu deðerler özel
büyüklüklerdir, özel sayýlardýr, ve onlarý niye istediðimizi, niye
hesapladýðýmýzý göreceðiz.

Özvektör nedir? 

Elimde bir $A$ matrisi var. Bir matris ne yapar? Vektörler üzerinde, mesela
$x$ vektörü, etkide bulunabilir, onlarý deðiþtirebilir. Sanki $A$'yi bir
fonksiyon gibi de görebiliriz, $x$ vektörü $A$'ya ``giriyor'' ve
``dýþarýya'' bir $Ax$ çýkýyor. Calculus'ta olduðu gibi $f()$'e bir tek sayý
$x$ veriliyor, $f(x)$ geri dýþarý çýkýyor [hakikaten de $x$ ve $Ax$ ayný
boyutta yani giriþ çýkýþ analojisi çok uygun]. Lineer Cebirde daha çok
boyut var, giren ve çýkan vektörler.

Bu derste özellikle ilgilendiðim vektörler ise dýþarý çýktýðý zaman girdiði
haliyle {\em ayný yönü gösteren} vektörler. Dikkat, ``ayný'' olan vektörler
deðil çýkýnca ayný ``yönü'' gösteren vektörler. Bu tipik bir durum olmazdý
deðil mi? Çoðunlukla $A$'yi bir uyguladýk mý dýþarý çýkan vektör tamamen
baþka bir yönü gösterir. Bizim ilgilendiðimiz durumda öyle olmayacak, bu
durumda $Ax$, $x$'e paralel olacak. Ýþte bu vektörler özvektörler olacak. 

Paralel ne demektir? Formülle daha rahat belirtilir,

$$ Ax = \lambda x $$

$\lambda$, yani özdeðer, bir skalardýr. Ýki tarafta $x$'in olmasý
paralelliðe iþaret ediyor, sadece büyüklük ($\lambda$ üzerinden) deðiþik
olabiliyor. Tabii büyüklük derken $\lambda$ eksi deðerde olabileceði için
vektörün ters yönde olmasýna da izin vermiþ oluyoruz. $\lambda$ sýfýr da
olabilir, hatta hayali sayý bile olabilir.

Özdeðer sýfýr üzerinde biraz daha duralým. Bu durumda $Ax = 0 \cdot x$ elde
ederiz yani $Ax = 0$. Bu ne demektir? $x$'lerin $A$'nin sýfýr uzayýnda
(nullspace) olmasý... Eðer $A$ eþsiz (singular) ise, ki $Ax = 0$ bu demek
zaten demek ki öyle bir $x$ olabiliyor ki $Ax = 0$ olabiliyor, o zaman $x$
sýfýr olmayan bir vektördür, ve $\lambda = 0$ bir özdeðer olmalýdýr.

Bir yansýtma matrisine bakalým, mesela $P$. Elimizde bir düzlem (plane)
var, ve bu sathýn üzerinde yansýtma yapan bir $P$ var. 

\includegraphics[height=4cm]{21_1.png}

$b$, $P$'nin bir özvektörü müdür? Deðildir. Çünkü $b$ ve $Pb$ ayný yönü
göstermiyorlar. 

Peki, bu resme göre, yansýtma sonrasý ayný yönde olacak bir vektör var
mýdýr? Varsa nerededir? Cevap, eðer $x$ üstteki düzlemin tam üzerinde ise
$P$ yansýtmasý sonrasý ayný yönde kalýr. Tabii yansýtma tekrar kendisini
verir, yani vektör hiç deðiþmemiþ olur. $Px = x$, ki $\lambda = 1$.

Baþka bir özvektör var mý? Olmasýný umuyorum çünkü 3 boyuttayým ve bu
demektir ki 2 tane daha birbirinden baðýmsýz özvektör bulabilmeliyim, ki
nihayetinde özvektörlerin ikisi düzlem üzerinde [düzlem iki boyutlu bir þey
olduðuna göre], o zaman üçüncüsü düzlem dýþýnda olacak. Düzlem dýþýnda olan
özvektör dik olan özvektör olmalý.

\includegraphics[height=4cm]{21_2.png}

Bu durumda $Px = 0$, ve $\lambda = 0$, çünkü dikliðin bir diðer tanýmý
çarpým sonrasý sonucun sýfýr olmasý. 

Bir diðer örnek. Þu permutasyon matrisine bakalým. 

$$ 
A = \left[\begin{array}{cc}
0 & 1 \\
1 & 0
\end{array}\right]
 $$

Bu matrisi hangi vektör ile çarparsam ayný yönde bir vektör elde ederim?
Permutasyon matrisi taným itibariyle permutasyon yapar, yani öðelerin
yerini deðiþtirir. Ýki boyut baðlamýnda bir vektörün iki öðesinin yerini
deðiþtirecektir. Peki hangi vektörün öðeleri yer deðiþtirirse yine kendisi
olur? Cevap basit, $x = [1 \ 1]$. 

$$ 
x = \left[\begin{array}{c} 1 \\ 1 \end{array}\right], \quad
Ax = \left[\begin{array}{c} 1 \\ 1 \end{array}\right],  \quad
\lambda = 1,  \quad
Ax = x
$$

Bir tane daha özdeðer/vektör lazým. Bu diðer özdeðer $\lambda = -1$
olmalý. Peki nasýl bir vektör olmalý ki öðeleri yer deðiþtirince ters yönü
göstersin? 

$$ 
x = \left[\begin{array}{r} -1 \\ 1 \end{array}\right],  \quad
Ax = \left[\begin{array}{r} 1 \\ -1 \end{array}\right],  \quad
\lambda = -1,  \quad
Ax = -x
 $$

Özvektör/deðerler hakkýnda ufak bir þey daha söylemek istiyorum. $N \times
N$ matrisinin $N$ tane özdeðeri vardýr. Bu deðerleri bulmak kolay
deðildir. 1., 2., hatta $N$'inci seviye bir denklemden çýkar bu
deðerler. Fakat bize yardým eden bir numara vardýr, tüm özdeðerlerin
toplamý matrisin köþegen deðerlerinin toplamýna eþittir, ki bu toplama
``iz'' (trace) ismi verilir. 

Bu numarayý üstteki örnekte kullanýrsak, $\lambda = 1$ bulduðum anda diðer
özdeðerin -1 olduðunu hemen bilirim, çünkü ana matrisin izi sýfýr, 
$0 - 1 = -1$. 

Artýk özdeðer/vektör hesabýna gelelim. $Ax = \lambda x$ denklemi var, bu
denklemde iki bilinmeyen var. Bu denklemi nasýl çözerim? Bir numara, her
þeyi tek tarafa gönderirim,

$$ (A-\lambda I )x = 0 $$

Þimdi bu denklem bana bir þeyler söylüyor. Eðer bir $x$ ``var'' ise, bu
varlýk, $A-\lambda I$'nin eþsiz olduðu anlamýna gelir. Peki eþsiz matrisler
hakkýnda ne biliyorum? Determinantlarýnýn sýfýr olduðunu biliyorum. Yani,

$$ \det (A -\lambda I) = 0 $$

Bu denklem özdeðer denklemi, ya da karakteristik denklem (characteristic
equation) olarak bilinir. 

Ýlk önce $\lambda$ bulmakla iþe baþlarým. Tabii $N$ tane $\lambda$
olacaktýr, bunlarýn hepsini bulmakla iþe baþlarým. Not: $N$ $\lambda$
olmasý demek $N$ deðiþik $\lambda$ olmasý anlamýna gelmiyor, bazý $\lambda$
deðerleri kendini tekrar edebilir. Tabii tekrarlanan $\lambda$ bizim
dersteki her türlü derdin kaynaðýdýr [biraz þaka biraz gerçek havasýyla
söylüyor hoca].

$\lambda$'yi bulunca ne yaparým? Matrisin sýfýr uzayýný hesaplarým, ki artýk bu
iþlemde usta olduk, eliminasyona baþlarým, pivot bulurum, vs.

Örnek

$$ 
A = \left[\begin{array}{ccc}
3 & 1 \\ 1 & 3
\end{array}\right]
 $$

Bu matris simetrik. Bu tür özel þartlar matrisin özdeðerlerinin de özel olmasý
anlamýna gelir. Mesela simetrik matrislerin tüm özdeðerlerinin reel sayý
olduðunu hemen bilirim. Peki özvektörler? Birbirlerine dik olurlar. Bir önceki
örneði hatýrlarsak, $\left[\begin{array}{cc}1&-1\end{array}\right]$ ve
$\left[\begin{array}{cc}-1&1\end{array}\right]$.

$$
\det (A -\lambda I) = \left[\begin{array}{ccc} 3-\lambda & 1 \\ 1 & 3-\lambda
\end{array}\right] = 
(3-\lambda)^2 -1 = 0
$$

$$ \lambda^2 - 6\lambda + 8 = 0 $$

Bu formüldeki $6$ nedir? Matris izinin eksi hali. Peki 8? O da
determinant. Yani 2x2 durumunda sayýlar çok basit þekilde ortaya
çýkýyorlar. Neyse, üstteki formülü faktorize edelim, $(\lambda - 4)(\lambda
- 2)$, yani $\lambda_1 = 4,\lambda_2 = 2$. 

Þimdi özvektörler: Bu vektörler köþegenden 2 ya da 4 çýkartýldýðý zaman
ortaya çýkan matrislerin sýfýr uzayýdýr. 

$$ 
\left[\begin{array}{ccc}
3-4 & 1 \\ 1 & 3-4
\end{array}\right] = 
\left[\begin{array}{ccc}
-1 & 1 \\ 1 & -1
\end{array}\right]
 $$

Bu matris eþsiz mi? Öyle. Bu matrisi $x_1 = [1 \ 1]$ ile çarparsam sýfýr elde
ederim. Diðeri?

$$ 
\left[\begin{array}{ccc}
3-2 & 1 \\ 1 & 3-2
\end{array}\right] = 
\left[\begin{array}{ccc}
1 & 1 \\ 1 & 1
\end{array}\right]
 $$

$x_2 = [1 \ -1]$. Bu da ikinci özvektör.

Ýlginç bir durum, bu sonuç permutasyon matrisinden gelen sonuca çok
benziyor. Fark nerede? Ana matrisin köþegenine 3 eklenmiþ, ya da $A + 3I$
yapýlmýþ. Bunu yapýnca özdeðerlere 3 eklemiþ oldum. Ama özvektörler hiç
deðiþmeden kaldý.

Daha çetrefil bir örnek görelim. Diyelim ki iki matris $A,B$'nin
özdeðerlerini biliyorum. $A+B$'nin özdeðerleri $\lambda,\alpha$ nedir?  Ýlk
akla gelen cevap ``özdeðerleri toplanýr'' doðru deðil. Çünkü

$$ (A+B)x = (\lambda+\alpha)x $$

demiþ oluruz, ama bunu derken özvektörler ayný demiþ oluruz. Bu doðru
deðildir. $A \cdot B$ ayný þekilde. Bunlar kötü örnekler. 

Bir örnek daha yapalým, bu sefer döndürme / rotasyon (rotation) matrisi. 90
derece döndürme matrisi olsun.

Örnek

$$ Q =  
\left[\begin{array}{ccc}
\cos 90 & -\sin 90 \\
\sin 90 & \cos 90
\end{array}\right]
= \left[\begin{array}{ccc}
0 & -1\\
1  & 0
\end{array}\right]
$$

Bu bir dik, dikgen (orthogonal) bir matris. Özdeðerlerin toplamý sýfýr
olacak, çünkü iz öyle. Determinant özdeðerlerin çarpýmýna eþit, yani bu
çarpým 1 olacak.

Fakat bu örnekte bazý þeyler yanlýþ gidecek. Niye? Düþünelim, hangi vektör
bu matrise verilince, döndürüldükten sonra dýþarý ayný yönde olarak çýkar?
Özellikle 90 derece döndürüldükten sonra. Gördüðünüz gibi problem
çýkabilir.  Özdeðerler ile de problem var, toplamý 0 olan ama çarpýmý 1
olan nasýl sayýlar bulabileceðiz ki?

Fakat bir çare var. Hesaplayalým, 

$$ \det(Q - \lambda I) = 
\left[\begin{array}{ccc}
-\lambda & -1 \\
1 & -\lambda
\end{array}\right] = \lambda^2 + 1 = 0
 $$

Özdeðerler nedir? $\lambda_1 = i, \lambda_2 = -i$. Bu sayýlar kompleks / hayali
sayýlardýr, reel deðillerdir. Bu olabilir. Bir matris, üstte olduðu gibi,
tamamen reel sayýlardan oluþabilir, ama özdeðerleri hayali olabilir. 

Bu iki hayali sayý birbirinin kopmleks eþleniði (complex
conjugate). Kompleks eþleniðin ne olduðunu hatýrlýyoruz herhalde, bir
sayýnýn hayali kýsmýnýn iþaretini deðiþtirince onun kompleks eþleniðini
elde etmiþ olurum. Üstteki sayýlar zaten tamamen hayali bölümden oluþuyor,
hiç reel kýsýmlarý yok, o tek kýsmýn da iþaretini deðiþtirince eþleniði
elde ediyorum. 

Eðer matrisim simetrik, ya da ``simetriðe yakýn'' olsaydý, üstteki durum
kesinlikle ortaya çýkmazdý. Çünkü kural odur ki, simetrik matrislerin
özdeðerleri reeldir. 

Bol bol örnek veriyorum ki tüm olasýlýklarý görebilelim. 

Bir kötü ihtimal daha.

$$ A = 
\left[\begin{array}{ccc}
3 & 1 \\ 0 & 3
\end{array}\right]
 $$

Bu matris üçgensel (triangular). Bu tür matrislerde özdeðerler köþegenin
üzerindedir! Bunu bilmek oldukça faydalýdýr. Ama kontrol edelim,

$$ \det(Q - \lambda I) = 
\left[\begin{array}{ccc}
3-\lambda & 1 \\
0 & 3-\lambda
\end{array}\right] = (3-\lambda)(3-\lambda) = 0, 
\lambda_1 = 3, \lambda_2 = 3
 $$

Problem nerede? Problem özvektörlerde. Hatýrlayalým, 

$$ (A-\lambda I)x = 0$$

denklemini çözüyoruz, yani

$$ (A-\lambda I)x = 
\left[\begin{array}{cc}
0 & 1 \\ 
0 & 0
\end{array}\right]
\vec{x}
=
\left[\begin{array}{c}
0 \\ 0
\end{array}\right]
$$

Burada $\vec{x}$ sonucunu arýyorum, bir vektör. Kafadan hemen birinci
özvektörü bulabilirim, 

$$ x_1 = \left[\begin{array}{c}
1 \\ 0
\end{array}\right] $$

Peki ikinci özvektör nedir? Bu vektörün birinciden baðýmsýz olmasý gerekir,
unutmayalým. Böyle bir vektör bulabilir miyiz? Bulamayýz. Mümkün deðil. Bu
sebeple bu örneðe dejenere durum (deðenerate case) denir. Ýkinci bir
baðýmsýz özvektör yoktur. 

Ekler 

Alttaki anlatým alternatif bir kaynaktan alýnmýþtýr

Özvektörler, Özdeðerleri Elle Hesaplamak (Eigenvectors, Eigenvalues)

Özdeðerler ve özvektörler her matrise göre özel vektörlerdir, ki matris bu
özel vektörleri transform ettiðinde / iþlediðinde sonuç yine özvektörün
kendisidir, daha doðrusu onun bir sabit (özdeðer) ile çarpýlmýþ
halidir. Yani

$$ Ax = \lambda x $$

Tek tarafa geçirelim

$$ Ax - \lambda x = 0 $$

Bu noktada $x$'leri dýþarý çekmek isterdik, fakat bunu yapamayýz, çünkü o
zaman içeride $A - \lambda$ kalýr ve bu olmaz, çünkü $A$ bir matris,
$\lambda$ bir tek sayý. Ama $Ix = x$'ten hareketle

$$ Ax - \lambda I x = 0 $$

diyebiliriz. Þimdi dýþarý çekersek

$$ (A - \lambda I) x = 0 $$

Bu ifadenin doðru olmasý için parantez içindeki ifade / matris eþsiz
(singular) olmalýdýr. Bunun için ise parantez içinin determinantý sýfýr
olmalýdýr. Yani

$$ |A - \lambda I| = 0 $$

Örnek 

$$ 
A = 
\left[\begin{array}{rr}
1 & 4 \\ 3 & 5
\end{array}\right]
 $$

$$ 
A - \lambda I = 
\left[\begin{array}{rr}
1 & 4 \\ 3 & 5
\end{array}\right] - 
\lambda
\left[\begin{array}{rr}
1 & 0 \\ 0 & 1
\end{array}\right] 
 $$

$$ 
= 
\left[\begin{array}{rr}
1 - \lambda & 4 \\ 3 & 5-\lambda
\end{array}\right]
 $$

$$ det(A - \lambda I) = (1-\lambda)(5-\lambda) - 4 \cdot 3 $$

Üstteki denkleme karakteristik denklem (characteristic equation) denir. 

$$ = -7 -6\lambda + \lambda^2 $$

Kökleri $\lambda_1 = 7$, $\lambda_2 = -1$.

Her özdeðere tekabül eden özvektörü bulmak istiyorsak, çýkartma iþlemini
yapalým

$$ 
A - \lambda I = 
\left[\begin{array}{rr}
1-7 & 4 \\ 3 & 5-7
\end{array}\right] = 
\left[\begin{array}{rr}
-6 & 4 \\ 3 & -2
\end{array}\right]
 $$

Þu formüle dönersek

$$ (A - \lambda I) x = 0 $$

Çýkartma sonrasý elimize geçen matrisi çarpacak öyle bir $x$ vektörü
arýyoruz ki bu vektörle çarpýnca elimize sýfýr (vektörü) geçsin. Yani bu
aradýðýmýz $x$ vektörü $(A - \lambda I)$'nin sýfýr uzayýnda (nullspace). 

2 x 2 boyutundaki böyle ufak bir örnek için $x$'i aslýnda tahmin
edebiliriz. Öyle iki sayý bulalým ki, 1. ve 2. kolonu onlarla ayrý ayrý
çarpýp topladýðýmýzda sonuç sýfýr olsun. Her iki kolonun tepesinde -6 ve 4
görüyorum, sadece bu iki sayýnýn sýfýra toplanmasý için acaba -6'yý 2 ile
4'ü 3 ile çarpýp toplasam olur mu? Kolondaki diðer sayýlara bakýyoruz, 3 ve
-2 için de bu iþe yarýyor. Demek ki özvektörlerden biri 

$$ x_1 = 
\left[\begin{array}{r}
2 \\ 3
\end{array}\right]
 $$

Diðeri ise, ayný tekniði kullanarak,

$$ x_2 = 
\left[\begin{array}{r}
-2 \\ 1
\end{array}\right]
 $$

Ekler

Özdeðer/Vektör Hesabýnda Üst Metot (Power Method)

Diyelim ki bir $A$ matrisinin, ki $A \in \mathbb{R}^{n \times n}$, özdeðerleri
$\lambda_1,...,\lambda_n$ ve özvektörleri $v_1,..,v_n$ olarak verilmiþ. Bu
demektir ki her $i=1,..,n$ için $Av_i = \lambda_i v_i$. Farzedelim ki bu
matrisin tüm özvektörleri bir ``özbaz (eigenbasis)'' oluþturuyor ve bu baz ile
$\mathbb{R}^n$'deki herhangi bir vektörü temsil edebiliyoruz. Hatýrlayalým eðer
tekrar eden özdeðer yok ise (hepsi deðiþik), normalize edilmiþ $v_i$ vektörleri
birimdik (orthonormal) bir set oluþtururlar, $||v||=1,v_i^Tv_i=1,v_j^Tv_i=0$
demektir.

Diyelim ki $|\lambda_1| > |\lambda_2| > .. > |\lambda_n| $, bu yazýda
$\lambda_1$'e baskýn (dominant) özdeðer diyeceðiz. Þimdi Herhangi bir $v_0 \in
\mathbb{R}^n$'i alalým.  Üsttekiler ýþýðýnda $\mu_1,..,\mu_n$ olarak öyle
katsayýlar olmalýdýr ki

$$ v_o = \mu_1v_1 + .. + \mu_nv_n 
$$

çünkü özvektörler bir baz oluþturuyorlar. Þimdi her iki tarafý soldan $A$ ile
çarpalým, ayrýca $Av_i = \lambda_iv_i$ eþitliðinden hareketle üstteki eþitliðin
sað tarafýný alýp üçüncü bir eþitlik olarak en saðda yazalým,

$$ A v_o = \mu_1 A v_1 + .. + \mu_n A v_n =
\mu_1\lambda_1v_1 + ... + \mu_n\lambda_nv_n
$$

Þimdi üstteki ifadeyi $A$ ile bir daha, hatta birkaç defa çarpalým, diyelim
toplam $m$ kere çarpmýþ olalým,


$$
A^m v_o = \mu_1 A^m v_1 + .. + \mu_n A^m v_n =
\mu_1\lambda_1^mv_1 + ... + \mu_n\lambda_n^mv_n
\mlabel{1}
$$

En saðda niye $\lambda_i^m$ ifadeleri elde ettik? Mesela $\mu_1\lambda_1v_1$
ifadesi, $A$ ile bir kere çarpýlýnca,

$$
\mu_1\lambda_1\underbrace{Av_1}_{\lambda_1v_1} =
\mu_1\lambda_1\lambda_1v_1 = 
\mu_1\lambda_1^2v_1 
$$

olacaktýr. Bunu $m$ kere yapýnca (1)'in en saðýndaki sonucu elde ederiz.

Þimdi (1)'in en saðýndaki eþitliðin içinden $\lambda_1^m$'i çýkartalým (1)'ýn en
solundaki eþitlik ile yanyana getirelim,

$$
A^m v_o = 
\lambda_1^m \bigg(  
\mu_1v_1 
+ \mu_n \bigg(\frac{\lambda_2}{\lambda_1}\bigg)^m v_2
+ ... 
+ \mu_n \bigg(\frac{\lambda_n}{\lambda_1}\bigg)^m v_n
\bigg)
$$

Ýspatýn baþýnda baskýn özdeðerin $\lambda_1$ olduðunu söylemiþtik. O zaman 

$$ 
\bigg| \frac{\lambda_2}{\lambda_1} \bigg| < 1, ..., 
\bigg| \frac{\lambda_n}{\lambda_1} \bigg| < 1
 $$

Bu demektir ki limit koþulu $m \to \infty$ durumunda

$$
A^m v_o = 
\lambda_1^m \bigg(  
\mu_1v_1 
\cancelto{0}
{
+ \mu_n \bigg(\frac{\lambda_2}{\lambda_1}\bigg)^m v_2
+ ... 
+ \mu_n \bigg(\frac{\lambda_n}{\lambda_1}\bigg)^m v_n
}
\bigg)
$$

Yani,

$$
A^m v_o = \lambda_1^m  \mu_1v_1 
$$

çünkü 1'den küçük olan tüm bölümler, katlarý alýndýkça ve o kat ($m$) çok
büyüdüðünde sýfýra giderler. Bu bölümleri içeren tüm terimler yokolur ve geriye
üstteki ifade kalýr.

Böylece üst metodunu türetmiþ olduk. En son ifade þunu söylüyor, herhangi bir
vektör $v_0$'i alalým, ve onu $A$ ile $m$ kere çarpalým, ve bu durumda elimize
gerçek özvektör $v_1$'e paralel bir vektör $\lambda_1^m \mu_1v_1$ geçecektir
(paralel çünkü $v_1$ deðeri tek sayý / skalar $\lambda^m\mu_1$ ile
çarpýlmakta). Bu vektörü normalize ederek bir özvektör sonucu elde
edebiliriz. Unutmayalým, özvektörlerin sadece yönü ile ilgileniyoruz,
büyüklükleri ile ilgilenmiyoruz, eðer $v_1$ bir özvektör ise, herhangi bir
skalar $k$ için $kv_1$ de bir özvektördür. Ders baþýnda özvektörlerin ``A ile
çarpýlýp {\em yönünün} deðiþmemesi'' baðlamýnda tanýmlandýðýný aklýmýzda tutalým
[2].

Örnek olarak alttaki $A$'yi alalým, baþlangýç olarak $v_0 = [1 \ 1]$

\begin{minted}[fontsize=\footnotesize]{python}
v0 = np.array([1.,1.])
A = np.array([[13., 5.], [2., 4.]])
for i in range(20): 
    v0 = np.dot(A,v0)
print 'v0 =',v0
\end{minted}

\begin{verbatim}
v0 = [  1.14093076e+23   2.28186151e+22]
\end{verbatim}

Sonsuzluk norm'u (infinity norm) ile normalize edersek (sonsuzluk normu bir
vektör içindeki en büyük öðenin alýnýp bölümde kullanýlmasýyla yapýlan
normalizasyondur),

\begin{minted}[fontsize=\footnotesize]{python}
v1 = v0 / np.max(v0)
v1 = v1.reshape((2,1))
print 'v1 ='
print v1
\end{minted}

\begin{verbatim}
v1 =
[[ 1. ]
 [ 0.2]]
\end{verbatim}

Kontrol edelim,

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
U,D = lin.eig(A)
print U
print D
\end{minted}

\begin{verbatim}
[ 14.   3.]
[[ 0.98058068 -0.4472136 ]
 [ 0.19611614  0.89442719]]
\end{verbatim}

Birinci kolona oldukça yakýn bir deðer elde ettik, ki en büyük özdeðere
tekabül eden özvektör orada.

Peki özdeðerin kendisini nasýl buluruz? Rayleigh Bölümü (Rayleigh Quotient) 
formülünü kullanabiliriz. Bu formül, eðer $x$ bir özvektör ise

$$ \lambda = \frac{x^TAx}{x^Tx} $$

Türetelim,

$$ Ax = \lambda x  $$

$$ x^TAx = x^T\lambda x $$

$\lambda$ bir tek sayý olduðuna göre sað taraftan $x^Tx$'i alýp, solda bölüm
yapabiliriz, ve Rayleigh formülüne eriþiriz.

$$ x^TAx / x^Tx = \lambda $$

1. özdeðeri hesaplayalým o zaman,

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(np.dot(v1.T,A),v1) / np.dot(v1.T,v1)
\end{minted}

\begin{verbatim}
[[ 14.]]
\end{verbatim}

Tüm Özvektörler

En büyük özdeðeri bulmanýn yolunu gördük. Diðer özvektörleri nasýl buluruz?
Bunun yöntemlerinden birisi en büyük özdeðer $\lambda_1$'i bulduktan sonra
onu bir iþlem sonrasý en küçük haline getirmek, ve Üst Metotu tekrar
kullanarak (artýk en büyük olan) $\lambda_2$'yi bulmak. $\lambda_1$'i en
küçük haline getirmek, özdeðer/vektörü ana matris $A$'dan çýkartmak, onu
``deflasyon iþlemine tabi tutmak'' (deflation) olarak
biliniyor. Çýkartacaðýmýz deðer  $\lambda_1u_1u_1^T$ olacak. Ýspatý
þöyle gösterelim (normalize edilmiþ $v_i$ özvektörleri þimdi $u_i$ olarak
gösteriyoruz), 

$$ (A-\lambda_1u_1u_1^T)u_j = Au_j-\lambda_1u_1u_1^Tu_1u_j = 
\lambda_ju_j - \lambda_1u_1(u_1^Tu_j)
$$

Eðer $j=1$ ise

$$  (A-\lambda_1u_1u_1^T)u_1 = \lambda_1u_1 - \lambda_1u_1(u_1^Tu_1) = 0u_j$$

Eðer $j \ne 1$ ise

$$ (A-\lambda_1u_1u_1^T)u_j = \lambda_ju_j - \lambda_1u_1(u_1^Tu_j) = \lambda_ju_j  $$

Yani $(A-\lambda_1u_1u_1^T)$'nin özvektörleri $A$ ile aynýdýr, tek farký en
büyük olaný 0 haline gelmiþtir, ama $j \ne 1$ için, yani diðer tüm
özvektörleri aynýdýr. O zaman deflasyon iþleminden sonra ele geçen yeni
matris üzerinde Üst Metotu tekrar kullanýrsak artýk en büyük olan $u_2$'yi
buluruz, sonra deflasyonu tekrarlarýz, bir daha Üst Metot iþletiriz, bu
böyle devam eder. 

\begin{minted}[fontsize=\footnotesize]{python}
lam1 = 14.
v0 = np.array([1.,1.])
B = A - np.dot(v1,v1.T)*lam1
print 'B'
print B
for i in range(20): 
    v0 = np.dot(B,v0)
v2 = v0 / np.max(v0)
v2 = v2.reshape((2,1))
print 'v2'
print v2
\end{minted}

\begin{verbatim}
B
[[-1.    2.2 ]
 [-0.8   3.44]]
v2
[[ 0.55]
 [ 1.  ]]
\end{verbatim}

Rayleigh Bölümü ile ikinci özdeðeri bulalým,

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(np.dot(v2.T,B),v2) / np.dot(v2.T,v2)
\end{minted}

\begin{verbatim}
[[ 3.]]
\end{verbatim}

Google Nasýl Ýþler? 

Lineer Cebir hocalarý Google'a müteþekkir olmalý, çünkü bu ünlü arama
motorunun kullandýðý PageRank tekniðinin özü aslýnda lineer cebirin
temelini oluþturan kavramlardan özdeðer / özvektör hesabý. Öðrenciler
``niye bu kavramlarý öðreniyoruz hocam?''  diye sorunca artýk cevap
kolay: ``bu yöntemi Google da kullanýyor!''.

Þimdi arama motorunun yapmasý gerekenleri düþünelim: Google'a bir kelime
yazdýðýmýzda geri gelen sonuçlar nasýl kararlaþtýrýlýr? Ýlk akla gelen
yöntem tabii ki Web'deki tüm sayfalarýn (milyarlarca sayfa) sayfalar
üzerindeki kelimelerin o sayfa ile iliþkilendirilmesi ve arama yapýlýnca
kelimeye göre sayfa geri getirilmesi. Mesela alttaki örnekte ``book
(kitap)'' yazýnca geriye 1., 2. ve 5. sayfalar geri gelecek. Fakat hangi
sýrada? Bu sayfalardan hangisi diðerlerinden daha önemli?

\includegraphics[height=9cm]{pg2.png}

PageRank'in temelinde daha fazla referans edilen sayfalarýn daha üstte
çýkmasý yatar. Hatta o referans eden sayfalarýn kendilerine daha fazla
referans var ise bu etki ta en sondaki sayfaya kadar yansýtýlýr, hatta bu
zincir baþtan sona her seviyede hesaplanabilir. Peki bu nasýl
gerçekleþtirilir?

PageRank Web sayfalarýný bir Markov Zinciri olarak görür. Markov Zincirleri
seri halindeki $X_n, n=0,1,2,..$ rasgele deðiþkenini modeller ve bu
deðiþkenler belli sayýdaki konumlarýn birinde olabilirler. Mesela konumlarý
bir doðal sayý ile ilintilendirirsek $X_n = i$ olabilir ki $i=\{0,1,..\}$
diye kabul edelim.

Markov Zincirlerinde (MZ) $i$ konumundan $j$ konumuna geçiþ olasýlýðýný,
$P_{ij}$, biliriz ve bu $P(X_{n+1} = j | X_{n} = i)$ olarak açýlabilir. Açýlýmdan  
görüleceði üzere bir MZ sonraki adýma geçiþ olasýlýðý için sadece
bir önceki adýma bakar. Bu tür önce/sonra yapýsýndaki iki boyutlu hal, 
çok rahat bir þekilde matrisine çevirilebilir / gösterilebilir. Önceki konum 
satýrlar, sonraki konum kolonlar olarak betimlenir mesela. 

Örnek

Bir sonraki günde yaðmur yaðmayacaðýný bir MZ olarak tasarlayalým. Bir
sonraki günde yaðmur yaðmayacaðýný sadece bugün etkiliyor olsun. Eðer bugün
yaðmur yaðýyorsa yarýn yaðmur yaðmasý 0.7, eðer bugün yaðmýyor ise yarýn
yaðmasý 0.4. MZ þöyle

$$ 
P =
\left[\begin{array}{cc}
0.7 & 0.3 \\
0.4 & 0.6
\end{array}\right]
 $$

Geçiþ olasýlýklarýndan bahsettiðimize göre ve elimizde sýnýrlý / belli
sayýda konum var ise, bir MZ'nin her satýrýndaki olasýlýklarýn toplamý
tabii ki 1'e eþit olmalýdýr. 

MZ'lerin ilginç bir özelliði $n$ adým sonra $i,j$ geçiþinin $P^n$ hesabýyla
yapýlabilmesidir. Yani $P$'yi $n$ defa kendisiyle çarpýp $i,j$ kordinatýna 
bakarsak $n$ adým sonrasýný rahatça görebiliriz. Bunun ispatýný burada
vermeyeceðiz. 

Mesela üstteki örnekte, eðer bugün yaðmur yaðýyorsa 4 gün sonra yaðmur
yaðma olasýlýðý nedir? 

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
P = np.array([[0.7,0.3],[0.4,0.6]])
P4 = lin.matrix_power(P,4)
print P4
\end{minted}

\begin{verbatim}
[[ 0.5749  0.4251]
 [ 0.5668  0.4332]]
\end{verbatim}

Aradýðýmýz geçiþ için kordinat 0,0'a bakýyoruz ve sonuç 0.5749. Numpy
\verb!matrix_power! bir matrisi istediðimiz kadar kendisiyle çarpmamýzý
saðlýyor. 

Duraðan Daðýlým (Stationary Distribution)

Eðer yaðmur örneðindeki matrisi çarpmaya devam edersek, mesela 8 kere
kendisiyle çarpsak sonuç ne olurdu? 

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
P = np.array([[0.7,0.3],[0.4,0.6]])
P8 = lin.matrix_power(P,8)
print P8
\end{minted}

\begin{verbatim}
[[ 0.57145669  0.42854331]
 [ 0.57139108  0.42860892]]
\end{verbatim}

Dikkat edilirse, her satýr bir deðere yaklaþmaya baþladý. Bu deðer MZ'nin
duraðan daðýlýmýdýr, belli koþullara uyan her MZ'nin böyle bir duraðan
daðýlýmý vardýr. Bu koþullar MZ'nin periyodik olmayan (aperiodic) ve tekrar
eden (recurrent) olmasýdýr. Bu þartlar çok ``özel'' þartlar deðildir
aslýnda, daha çok ``normal'' bir MZ'yi tarif ediyor diyebiliriz. Tüm
konumlarý tekrar eden yapmak kolaydýr, MZ tek baðlý (singly connected) hale
getirilir, yani her konumdan her diðer konuma bir geçiþ olur, ve periyodik
olmamasý için ise MZ'ye olmadýðý zamanlarda bir konumdan kendisine geçiþ
saðlanýr (az bir gürültü üzerinden). 

Nihayet duraðanlýk þu denklemi ortaya çýkartýr,

$$ \pi = \pi P $$

Burada duraðan daðýlým $\pi$'dir. Bu denklem tanýdýk geliyor mu?  Devriðini
alarak þöyle gösterelim, belki daha iyi tanýnýr, 

$$ P^T\pi^T = \pi^T $$

Bir þey daha ekleyelim, 

$$ P^T\pi^T = 1 \cdot \pi^T $$

Bu özdeðer/vektör formuna benzemiyor mu? Evet! Bu form 

$$ Ax = \lambda x $$

MZ denklemi þunu söylüyor, 1 deðerindeki özdeðere ait özvektör bir MZ'nin
duraðan daðýlýmýdýr! Bu arada, MZ geçiþ matrisi $P$'nin en büyük
özdeðerinin her zaman 1 olduðunu biliyoruz (çünkü üstteki tarif ettiðimiz
özel þartlara sahip olan türden matrisler böyle özdeðerlere sahip
olmalý). Bu durumda en büyük özdeðere ait özvektörü hesaplamak yeterli
olacaktýr. Bunu yapmayý zaten {\em Lineer Cebir Ders 21}'de öðrenmiþtik,
üst metot (power method) sayesinde bu hesap kolayca yapýlabiliyor.

Þimdi en baþtaki Web sayfalarýna ait geçiþleri yazalým,

\begin{minted}[fontsize=\footnotesize]{python}
P = [[1./4, 2./4, 0, 0, 1./4],
     [1./6, 0, 2./6, 1./6, 2./6],
     [0, 0, 0, 2./4, 2./4],
     [1./8, 0, 0, 4./8, 3./8],
     [0, 1./2, 0, 1./2, 0]]

P = np.array(P)
print P
\end{minted}

\begin{verbatim}
[[ 0.25        0.5         0.          0.          0.25      ]
 [ 0.16666667  0.          0.33333333  0.16666667  0.33333333]
 [ 0.          0.          0.          0.5         0.5       ]
 [ 0.125       0.          0.          0.5         0.375     ]
 [ 0.          0.5         0.          0.5         0.        ]]
\end{verbatim}

Þimdi üst metotu kullanarak duraðan daðýlýmý hesaplayalým. Herhangi bir
baþlangýç vektörünü $P$ ile 20 kere  çarpmak yeterli olur.

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
x=np.array([.5, .3, .1, .1, 0]) # herhangi bir vektor
for i in range(20): 
    x = np.dot(x,P)
print 'pi = ', x
\end{minted}

\begin{verbatim}
pi =  [ 0.10526316  0.18421053  0.06140351  0.38596491  0.2631579 ]
\end{verbatim}

Not: Aslýnda cebirsel olarak $P$'yi 20 kere kendisiyle çarpmak ve sonucu
baþlangýç vektörü ile bir kere çarpmak ta düþünülebilirdi. Fakat 20 kere
vektör / matris çarpýmlarý yapmak, 20 kere matris / matris çarpýmý
yapmaktan daha verimli olacaktýr. Büyük Veri ortamý için de bu söylenebilir.

Neyse, eðer özvektör hesabýný kendimiz elle yapmak yerine direk kütüphane
çaðrýsý kullansaydýk,

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
evals,evec = lin.eig(P.T)
pi =  evec[:,0] / np.sum(evec[:,0])
print np.abs(pi)
\end{minted}

\begin{verbatim}
[ 0.10526316  0.18421053  0.06140351  0.38596491  0.26315789]
\end{verbatim}

Ayný sonuca ulaþtýk. Bu sonuç gösteriyor ki ``book'' yazdýðýzda Google bize
5. sayfayý en baþta olacak þekilde sonuç döndürmeli, çünkü onun duraðan
daðýlýmý 1,2,5 sayfalarýnýn arasýnda en yüksek.

Duraðan Daðýlýma Bakýþ

MZ ve duraðan daðýlýmýn PageRank'le alakasýný bir daha düþünelim. MZ ile
$n$ adým sonrasýný hesaplayabiliyoruz, duraðan daðýlým ise sonsuz adým
sonrasýný ifade ediyor. Ve bu daðýlým, bir anlamda, sonsuz yapýlan adýmlar
sýrasýnda {\em en fazla hangi konumlarda} zaman geçirileceðini
gösteriyor. Konum yerine sayfa dersek duraðan daðýlýmýn niye en önemli
sayfalarý belirlemek için önemli olduðunu anlarýz. 

Kullanýcý herhangi bir sayfada iken hangi diðer sayfalara gideceði o sayfa
üzerinde baðlantýlar üzerinden anlaþýlýr, PageRank bu baðlantýnýn
mevcudiyetine bakar sadece, o mevcudiyet üzerinden bir geçiþ olasýlýðý
hesaplar, ve bu olasýlýða göre (raslantýsal þekilde) baðlantýnýn takip
edileceði düþünülür. Bu arada çoðunlukla sayfalar arasýndaki baðlantýlarýn
aðýrlýðý 1 olacaktýr, fakat örnek amaçlý 2,3 gibi sayýlar da kullanýlýyor. 

Rasgele Sayfa Geçiþi

Google veri temsili üzerinde bazý ekler yapmaktadýr, mesela kullanýcýnýn
hiçbir baðlantý takip etmeyip tarayýcýya direk URL girerek baþka bir
sayfaya zýplamasý (teleporting) bir þekilde temsil edilmelidir. Ayrýca hiç
dýþa baðlantý vermeyen sayfalar (ölü noktalar) hesaba katýlmalýdýr. Þimdi
$\pi^T$ yerine $p$, $P$ yerine $N$ kullanalým, PageRank özyineli
algoritmasý

$$ p = N^Tp $$ 

olarak gösterilebilir. 

Bu her iki durum için formül þu þekilde ikiye ayýrýlýr,

$$ p = (1-d)N^Tp + dN_f^Tp $$

$$ = ((1-d)N^T + dN_f^T) p $$

$$ = M^Tp $$

ki,

$$M = (1-d)N^T + dN_f^T$$ 

olacaktýr. $N_f$ bir normalize edilmiþ ``zýplama'' matrisidir, yani her
sayfadan her diðer sayfaya bir baðlantý ``varmýþ gibi'' yapar, mesela 5x5
boyutunda tüm öðeleri 0.20 olacaktýr. $d$ bir aðýrlýk sabitidir, Google'ýn
bunu 0.85 olarak tanýmladýðý duyulmuþtur, ve gerçek baðlantý matrisi ve
rasgele zýplama matrisi arasýnda bir aðýrlýk tanýmlar, her ikisinde de
birazcýk alarak (daha çok ana $N$'den tabii) niahi matrisi oluþturur. Örnek
olarak þu grafiðe bakalým, 

\includegraphics[height=4cm]{pg3.png}

\begin{minted}[fontsize=\footnotesize]{python}
N = [[0, 0, 0, 1., 0],
     [0, 0, 1./2, 0, 1./2],
     [1, 0, 0, 0, 0],
     [0, 1./3, 1./3, 0, 1./3],
     [0, 1, 0, 0, 0]]

N = np.array(N)

Nf = 0.20 * np.ones((5,5))
d = 0.85
M = d*N + (1-d)*Nf
x=np.array([.5, .3, .1, .1, 0]) # herhangi bir vektor
for i in range(20): 
    x = np.dot(x,M)
print 'result = ', x 
\end{minted}

\begin{verbatim}
result =  [ 0.18959094  0.24375097  0.18775335  0.19115138  0.18775335]
\end{verbatim}

Sonuca göre $v_2$ en yüksek PageRank deðerine sahip. 

Mimari

Google tabii ki arama sonuçlarýný iyileþtirmek için yýllar içinde diðer ek
fonksiyonlarý motoruna ekledi. Duyumlarýmýza göre artýk PageRank gibi
onlarca ek kriter kullanýlmaktadýr; fakat PageRank hala çok önemli ve
þirketin kuruluþu baðlamýnda Google'ý Google yapan algoritmaydý, onun diðer
motorlara nazaran elindeki avantajý, en büyük ilerlemesiydi.

Sistem kodlamasý açýsýndan PageRank'e tüm Web sayfalarý ve onlarýn
arasýndaki iliþkiler verilmelidir, bu milyarlarca sayfa ve onlarýn
arasýndaki baðlantýlar demektir. Google bunu yapabilmek için Web ``aðýný''
örümcek (spider) programlarý ile sürekli geziyor, ve bu veriyi alýp,
PageRank'e hesap için aktarýyor.

Kaynaklar

[1] Feys, {\em MATH317 EIGHTEENTH TUTORIAL}, \url{http://www.math.mcgill.ca/feys/documents/tutnotesR18.pdf}

[2] Woolgar, {\em Lab 15-Power Method and Dominant Eigenvalues}, \url{http://www.math.ualberta.ca/~ewoolgar/labs/linalg/Lab15.pdf}

[3] Roberts, {\em Computation of matrix eigenvalues and eigenvectors }, \url{http://www.robots.ox.ac.uk/~sjrob/Teaching/EngComp/ecl4.pdf}

[4] Murphy, K., {\em CS340: Machine Learning Lecture Notes}, \url{www.ugrad.cs.ubc.ca/~cs340}

[5] Ross, S., {\em Introduction to Probability Models, 8th Edition}

[6] Zaki, Maira, {\em Fundamentals of Data Mining Algorithms}

\end{document}
